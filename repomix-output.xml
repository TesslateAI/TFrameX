This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
.env copy
.gitignore
.repomixignore
builder/backend/.repomixignore
builder/backend/agent_definitions.py
builder/backend/agents/basic.py
builder/backend/agents/chain.py
builder/backend/agents/context.py
builder/backend/agents/distributor_agent.py
builder/backend/agents/file_generator_agent.py
builder/backend/agents/flow_builder.py
builder/backend/agents/multi_call.py
builder/backend/agents/planner_agent.py
builder/backend/agents/utils.py
builder/backend/app.py
builder/backend/context.txt
builder/backend/flow_executor.py
builder/backend/longtext.txt
builder/backend/requirements.txt
builder/frontend/.gitignore
builder/frontend/components.json
builder/frontend/eslint.config.js
builder/frontend/index.html
builder/frontend/jsconfig.json
builder/frontend/package.json
builder/frontend/public/Tesslate.svg
builder/frontend/README.md
builder/frontend/src/App.jsx
builder/frontend/src/assets/react.svg
builder/frontend/src/components/ChatbotPanel.jsx
builder/frontend/src/components/NodesPanel.jsx
builder/frontend/src/components/OutputPanel.jsx
builder/frontend/src/components/Sidebar.jsx
builder/frontend/src/components/TopBar.jsx
builder/frontend/src/components/ui/alert.jsx
builder/frontend/src/components/ui/button.jsx
builder/frontend/src/components/ui/card.jsx
builder/frontend/src/components/ui/input.jsx
builder/frontend/src/components/ui/label.jsx
builder/frontend/src/components/ui/scroll-area.jsx
builder/frontend/src/components/ui/select.jsx
builder/frontend/src/components/ui/tabs.jsx
builder/frontend/src/components/ui/textarea.jsx
builder/frontend/src/index.css
builder/frontend/src/lib/utils.js
builder/frontend/src/main.jsx
builder/frontend/src/nodes/BaseNode.jsx
builder/frontend/src/nodes/BasicAgentNode.jsx
builder/frontend/src/nodes/ChainOfAgentsNode.jsx
builder/frontend/src/nodes/ContextAgentNode.jsx
builder/frontend/src/nodes/DistributorAgentNode.jsx
builder/frontend/src/nodes/FileGeneratorAgentNode.jsx
builder/frontend/src/nodes/MultiCallSystemNode.jsx
builder/frontend/src/nodes/PlannerAgentNode.jsx
builder/frontend/src/nodes/SoftwareBuilderNode.jsx
builder/frontend/src/store.js
builder/frontend/vite.config.js
pyproject.toml
README.md
repomix.config.json
requirements.txt
tframex/agents/__init__.py
tframex/agents/agent_logic.py
tframex/agents/agents.py
tframex/model/__init__.py
tframex/model/model_logic.py
tframex/repomix-output.xml
tframex/systems/__init__.py
tframex/systems/systems.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="builder/backend/.repomixignore">
generated/
example_outputs/
</file>

<file path="tframex/repomix-output.xml">
This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
agents/__init__.py
agents/agent_logic.py
agents/agents.py
model/__init__.py
model/model_logic.py
systems/__init__.py
systems/systems.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="agents/__init__.py">
# TAF/tframex/agents/__init__.py

# Import from the specific files within the 'agents' sub-package
from .agent_logic import BaseAgent
from .agents import BasicAgent, ContextAgent

# Optional: Define __all__
__all__ = ['BaseAgent', 'BasicAgent', 'ContextAgent']
</file>

<file path="agents/agent_logic.py">
# agent_logic.py
import logging
from abc import ABC, abstractmethod
from tframex.model.model_logic import BaseModel # NEW
from typing import Any, List, Dict
# --- END MODIFICATION ---

logger = logging.getLogger(__name__)

class BaseAgent(ABC):
    """Abstract base class for all agents."""
    def __init__(self, agent_id: str, model: BaseModel):
        """
        Initializes the BaseAgent.

        Args:
            agent_id (str): A unique identifier for the agent instance.
            model (BaseModel): The language model instance the agent will use.
        """
        self.agent_id = agent_id
        self.model = model
        logger.info(f"Agent '{self.agent_id}' initialized using model '{self.model.model_id}'")

    @abstractmethod
    async def run(self, *args, **kwargs) -> Any:
        """
        The main execution method for the agent.
        Must be implemented by subclasses.
        Returns:
            Any: The result of the agent's operation.
        """
        raise NotImplementedError

    async def _stream_and_aggregate(self, prompt: str, **kwargs) -> str:
        """
        Helper to call model's stream (using chat format) and collect the full response.
        """
        # --- MODIFICATION: Convert prompt string to messages list ---
        messages: List[Dict[str, str]] = [{"role": "user", "content": prompt}]
        full_response = ""
        # --- MODIFICATION: Pass messages list to model ---
        async for chunk in self.model.call_stream(messages, **kwargs):
            full_response += chunk
        return full_response
        # --- END MODIFICATION ---

# Potentially add shared utility functions here in the future
# e.g., parse_xml_tags, format_common_prompts etc.
</file>

<file path="agents/agents.py">
# agents.py
import logging
from tframex.agents.agent_logic import BaseAgent # NEW
from tframex.model.model_logic import BaseModel # NEWBaseModel

logger = logging.getLogger(__name__)

class BasicAgent(BaseAgent):
    """
    A simple agent that takes a prompt, calls the LLM, and returns the full response.
    """
    def __init__(self, agent_id: str, model: BaseModel):
        super().__init__(agent_id, model)

    async def run(self, prompt: str, **kwargs) -> str:
        """
        Sends the prompt to the LLM and returns the aggregated streamed response.

        Args:
            prompt (str): The input prompt.
            **kwargs: Additional parameters for the model call (e.g., max_tokens).

        Returns:
            str: The complete response from the LLM.
        """
        logger.info(f"Agent '{self.agent_id}' running with prompt: '{prompt[:50]}...'")
        full_response = await self._stream_and_aggregate(prompt, **kwargs)
        logger.info(f"Agent '{self.agent_id}' finished.")
        return full_response

class ContextAgent(BaseAgent):
    """
    An agent that combines a given context with the prompt before calling the LLM.
    """
    def __init__(self, agent_id: str, model: BaseModel, context: str):
        """
        Initializes the ContextAgent.

        Args:
            agent_id (str): Unique identifier for the agent.
            model (BaseModel): The language model instance.
            context (str): The predefined context to use.
        """
        super().__init__(agent_id, model)
        self.context = context
        logger.info(f"Agent '{self.agent_id}' initialized with context: '{self.context[:100]}...'")

    async def run(self, prompt: str, **kwargs) -> str:
        """
        Combines context and prompt, sends to LLM, and returns the full response.

        Args:
            prompt (str): The input prompt.
            **kwargs: Additional parameters for the model call (e.g., max_tokens).

        Returns:
            str: The complete response from the LLM.
        """
        combined_prompt = f"Context:\n{self.context}\n\n---\n\nPrompt:\n{prompt}"
        logger.info(f"Agent '{self.agent_id}' running with combined prompt.")
        logger.debug(f"Agent '{self.agent_id}' combined prompt: {combined_prompt}") # Log full prompt if needed
        full_response = await self._stream_and_aggregate(combined_prompt, **kwargs)
        logger.info(f"Agent '{self.agent_id}' finished.")
        return full_response
</file>

<file path="model/__init__.py">
# TAF/tframex/model/__init__.py

# Import the classes you want to expose directly from the 'model' package
from .model_logic import BaseModel, VLLMModel

# Optional: Define __all__ to control 'from tframex.model import *' behaviour
__all__ = ['BaseModel', 'VLLMModel']
</file>

<file path="model/model_logic.py">
# model_logic.py
import httpx
import json
import asyncio
import logging
from abc import ABC, abstractmethod
# --- MODIFICATION: Added List for messages type hint ---
from typing import AsyncGenerator, Optional, Dict, Any, List

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class BaseModel(ABC):
    """Abstract base class for language models."""
    def __init__(self, model_id: str):
        self.model_id = model_id
        logger.info(f"Initializing base model structure for ID: {model_id}")

    # --- MODIFICATION: Changed signature and docstring for 'messages' ---
    @abstractmethod
    async def call_stream(self, messages: List[Dict[str, str]], **kwargs) -> AsyncGenerator[str, None]:
        """
        Calls the language model (now expecting chat format) and streams response chunks.
        Must be implemented by subclasses.

        Args:
            messages (List[Dict[str, str]]): A list of message dictionaries,
                                             e.g., [{"role": "user", "content": "Hello"}].
        Yields:
            str: Chunks of the generated text content.
        """
        raise NotImplementedError
        yield "" # Required for async generator typing
    # --- END MODIFICATION ---

    @abstractmethod
    async def close_client(self):
        """Closes any underlying network clients."""
        raise NotImplementedError

class VLLMModel(BaseModel):
    """
    Represents a connection to a VLLM OpenAI-compatible endpoint.
    MODIFIED TO USE CHAT COMPLETIONS ENDPOINT AND FORMAT.
    """
    def __init__(self,
                 model_name: str,
                 api_url: str,
                 api_key: str,
                 default_max_tokens: int = 1024,
                 default_temperature: float = 0.7):
        super().__init__(model_id=f"vllm_{model_name.replace('/', '_')}")
        self.model_name = model_name
        base_url = api_url.replace('/v1', '').rstrip('/')
        # --- MODIFICATION: Changed URL name and target endpoint ---
        self.chat_completions_url = f"{base_url}/v1/chat/completions"
        # --- END MODIFICATION ---
        self.api_key = api_key
        self.default_max_tokens = default_max_tokens
        self.default_temperature = default_temperature
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }
        timeouts = httpx.Timeout(None, connect=100.0)
        self._client = httpx.AsyncClient(headers=self.headers, timeout=timeouts)
        # --- MODIFICATION: Updated log message ---
        logger.info(f"VLLMModel '{self.model_id}' initialized for CHAT endpoint {self.chat_completions_url}")
        # --- END MODIFICATION ---

    # --- MODIFICATION: Method signature now expects 'messages' list ---
    async def call_stream(self, messages: List[Dict[str, str]], max_retries: int = 2, **kwargs) -> AsyncGenerator[str, None]:
        """
        Calls the VLLM CHAT completions endpoint with messages and streams the response.
        Includes basic retry logic for specific network errors.

        Args:
            messages (List[Dict[str, str]]): The conversation history/prompt.
            max_retries (int): Maximum number of retries on specific errors.
            **kwargs: Override default parameters like 'max_tokens', 'temperature'.

        Yields:
            str: Chunks of the generated text content, including tags like <think>.
        """
        payload = {
            "model": self.model_name,
            # --- MODIFICATION: Use 'messages' instead of 'prompt' ---
            "messages": messages,
            "max_tokens": kwargs.get('max_tokens', self.default_max_tokens),
            "temperature": kwargs.get('temperature', self.default_temperature),
            "stream": True,
            **{k: v for k, v in kwargs.items() if k not in ['max_tokens', 'temperature', 'max_retries']}
        }
        # --- END MODIFICATION ---

        last_exception = None
        for attempt in range(max_retries + 1):
            try:
                # --- MODIFICATION: Updated log message and URL variable ---
                logger.debug(f"[{self.model_id}] Attempt {attempt+1}/{max_retries+1}: Sending request to {self.chat_completions_url}")
                async with self._client.stream("POST", self.chat_completions_url, json=payload) as response:
                # --- END MODIFICATION ---
                    if response.status_code == 429: # Specific handling for rate limits
                         retry_after = int(response.headers.get("Retry-After", "5")) # Default to 5s
                         logger.warning(f"[{self.model_id}] Rate limit hit (429). Retrying after {retry_after} seconds.")
                         await asyncio.sleep(retry_after)
                         last_exception = httpx.HTTPStatusError("Rate limit hit", request=response.request, response=response)
                         continue # Go to next attempt

                    if response.status_code != 200:
                        error_content = await response.aread()
                        error_msg = f"API Error: Status {response.status_code}, Response: {error_content.decode()}"
                        logger.error(f"[{self.model_id}] {error_msg}")
                        yield f"ERROR: {error_msg}" # Yield non-retryable API errors
                        return # Stop processing this request

                    # --- Stream Processing ---
                    async for line in response.aiter_lines():
                        line = line.strip()
                        if line.startswith("data:"):
                            data_content = line[len("data:"):].strip()
                            if data_content == "[DONE]":
                                logger.debug(f"[{self.model_id}] DONE signal received.")
                                return # Successful completion of the stream

                            try:
                                json_data = json.loads(data_content)
                                text_chunk = "" # Initialize empty
                                # --- MODIFICATION: Parse chat completions stream format ---
                                if 'choices' in json_data and len(json_data['choices']) > 0:
                                    choice = json_data['choices'][0]
                                    if 'delta' in choice and 'content' in choice['delta']:
                                         # Check if content is not None before assigning
                                         content = choice['delta']['content']
                                         if content is not None:
                                             text_chunk = content
                                # --- END MODIFICATION ---

                                # Yield chunk even if empty/whitespace
                                yield text_chunk

                            except json.JSONDecodeError:
                                logger.warning(f"[{self.model_id}] Could not decode JSON chunk: {data_content}")
                            except Exception as e:
                                logger.warning(f"[{self.model_id}] Error processing chunk data {data_content}: {e}")

                    logger.debug(f"[{self.model_id}] Stream finished without explicit [DONE] after loop.")
                    return # Successfully finished processing stream

            except httpx.ReadError as e:
                 last_exception = e
                 logger.warning(f"[{self.model_id}] Attempt {attempt+1} failed with ReadError: {e}. Retrying...")
                 await asyncio.sleep(2 ** attempt)
            except httpx.ConnectError as e:
                 last_exception = e
                 logger.warning(f"[{self.model_id}] Attempt {attempt+1} failed with ConnectError: {e}. Retrying...")
                 await asyncio.sleep(2 ** attempt)
            except httpx.PoolTimeout as e:
                 last_exception = e
                 logger.warning(f"[{self.model_id}] Attempt {attempt+1} failed with PoolTimeout: {e}. Retrying...")
                 await asyncio.sleep(2 ** attempt)
            # --- MODIFICATION: Catch RemoteProtocolError specifically for logging/retry ---
            except httpx.RemoteProtocolError as e:
                 last_exception = e
                 logger.warning(f"[{self.model_id}] Attempt {attempt+1} failed with RemoteProtocolError: {e}. Retrying...")
                 await asyncio.sleep(2 ** attempt)
            # --- END MODIFICATION ---
            except Exception as e:
                 logger.error(f"[{self.model_id}] An unexpected error occurred during streaming attempt {attempt+1}: {e}", exc_info=True)
                 yield f"ERROR: Unexpected error - {e}"
                 return

        logger.error(f"[{self.model_id}] All {max_retries + 1} attempts failed. Last error: {last_exception}")
        yield f"ERROR: Request failed after multiple retries - {last_exception}"


    async def close_client(self):
        """Closes the underlying HTTP client."""
        await self._client.aclose()
        logger.info(f"[{self.model_id}] VLLM HTTP client closed.")
</file>

<file path="systems/__init__.py">
# TAF/tframex/systems/__init__.py

# Import from the specific files within the 'systems' sub-package
from .systems import ChainOfAgents, MultiCallSystem, FrontendAgentSystem

# Optional: Define __all__
__all__ = ['ChainOfAgents', 'MultiCallSystem', 'FrontendAgentSystem']
</file>

<file path="systems/systems.py">
# systems.py
import asyncio
import logging
import os
import math
import re # Added
import time # Added
from tframex.model import BaseModel
from tframex.agents import BasicAgent # Using BasicAgent for summarization/final answer
# --- MODIFICATION: Added more specific types ---
from typing import List, Dict, Any, Tuple, Optional
# --- END MODIFICATION ---


logger = logging.getLogger(__name__)

# --- Text Chunking Helper (Existing) ---
def chunk_text(text: str, chunk_size: int, chunk_overlap: int = 50) -> List[str]:
    """Splits text into overlapping chunks."""
    if chunk_overlap >= chunk_size:
        raise ValueError("Overlap must be smaller than chunk size")
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunks.append(text[start:end])
        start += chunk_size - chunk_overlap
        if end >= len(text):
             break
    if len(chunks) > 1 and chunks[-1] == chunks[-2][chunk_overlap:]:
         pass
    final_chunks = [c for c in chunks if c]
    logger.info(f"Chunked text into {len(final_chunks)} chunks (size={chunk_size}, overlap={chunk_overlap})")
    return final_chunks

# --- ChainOfAgents (Existing - No Changes Needed) ---
class ChainOfAgents:
    """
    A system that processes long text by summarizing chunks sequentially.
    """
    def __init__(self, system_id: str, model: BaseModel, chunk_size: int = 1000, chunk_overlap: int = 100):
        self.system_id = system_id
        self.model = model
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.processing_agent = BasicAgent(agent_id=f"{system_id}_processor", model=model)
        logger.info(f"System '{self.system_id}' initialized (Chain of Agents).")

    async def run(self, initial_prompt: str, long_text: str, **kwargs) -> str:
        """Processes the long text based on the initial prompt using a chain of summaries."""
        logger.info(f"System '{self.system_id}' starting run for prompt: '{initial_prompt[:50]}...'")
        chunks = chunk_text(long_text, self.chunk_size, self.chunk_overlap)
        if not chunks:
            logger.warning(f"System '{self.system_id}': No text chunks generated from input.")
            return "Error: Input text was empty or too short to chunk."

        current_summary = ""
        num_chunks = len(chunks)

        for i, chunk in enumerate(chunks):
            chunk_prompt = (
                f"Overall Goal: {initial_prompt}\n\n"
                f"Previous Summary (if any):\n{current_summary}\n\n"
                f"---\n\n"
                f"Current Text Chunk ({i+1}/{num_chunks}):\n{chunk}\n\n"
                f"---\n\n"
                f"Task: Summarize the 'Current Text Chunk' focusing on information relevant to the 'Overall Goal'. "
                f"Integrate relevant details from the 'Previous Summary' if applicable, but keep the summary concise. "
                f"Output *only* the refined summary."
            )
            logger.info(f"System '{self.system_id}': Processing chunk {i+1}/{num_chunks}...")
            current_summary = await self.processing_agent.run(chunk_prompt, **kwargs)
            logger.debug(f"System '{self.system_id}': Intermediate summary after chunk {i+1}: '{current_summary[:100]}...'")

        final_prompt = (
            f"Context (summary derived from the full text):\n{current_summary}\n\n"
            f"---\n\n"
            f"Prompt:\n{initial_prompt}\n\n"
            f"---\n\n"
            f"Task: Using the provided context (summary), answer the prompt accurately and completely."
        )
        logger.info(f"System '{self.system_id}': Generating final answer...")
        final_answer = await self.processing_agent.run(final_prompt, **kwargs)

        logger.info(f"System '{self.system_id}' finished run.")
        return final_answer

# --- MultiCallSystem (Existing - No Changes Needed) ---
class MultiCallSystem:
    """
    A system that makes multiple simultaneous calls to the LLM with the same prompt.
    """
    def __init__(self, system_id: str, model: BaseModel):
        self.system_id = system_id
        self.model = model
        logger.info(f"System '{self.system_id}' initialized (Multi Call).")

    async def _call_and_save_task(self, prompt: str, output_filename: str, **kwargs) -> str:
        """Internal task to call LLM stream (using chat format) and save to a file."""
        full_response = ""
        messages: List[Dict[str, str]] = [{"role": "user", "content": prompt}]
        try:
            with open(output_filename, 'w', encoding='utf-8') as f:
                async for chunk in self.model.call_stream(messages, **kwargs):
                    f.write(chunk)
                    full_response += chunk
                    f.flush()
            logger.info(f"System '{self.system_id}': Saved response to {output_filename}")
            return output_filename
        except Exception as e:
            logger.error(f"System '{self.system_id}': Error saving to {output_filename}: {e}")
            try:
                 with open(output_filename, 'w', encoding='utf-8') as f:
                      f.write(f"ERROR processing/saving response: {e}\n\nPartial response if any:\n{full_response}")
            except Exception:
                 pass
            return f"ERROR: Failed to write to {output_filename}"

    async def run(self, prompt: str, num_calls: int, output_dir: str = "multi_call_outputs", base_filename: str = "output", **kwargs) -> Dict[str, str]:
        """Makes `num_calls` simultaneous requests to the model with the given prompt."""
        logger.info(f"System '{self.system_id}' starting run for {num_calls} simultaneous calls.")
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
            logger.info(f"Created output directory: {output_dir}")

        tasks = []
        output_files = {}

        for i in range(1, num_calls + 1):
            output_filename = os.path.join(output_dir, f"{base_filename}_{i}.txt")
            task_id = f"call_{i}"
            task = self._call_and_save_task(prompt, output_filename, **kwargs)
            tasks.append(task)
            output_files[task_id] = output_filename

        logger.info(f"System '{self.system_id}': Launching {num_calls} concurrent tasks...")
        results = await asyncio.gather(*tasks, return_exceptions=True)

        final_results = {}
        success_count = 0
        error_count = 0
        for i, result in enumerate(results):
            task_id = f"call_{i+1}"
            original_filename = output_files[task_id]
            if isinstance(result, Exception):
                logger.error(f"System '{self.system_id}': Task {task_id} raised an exception: {result}")
                final_results[task_id] = f"ERROR: Task Exception - {result}"
                error_count += 1
                try:
                    with open(original_filename, 'w', encoding='utf-8') as f:
                         f.write(f"ERROR: Task Exception - {result}")
                except Exception:
                    pass
            elif isinstance(result, str) and result.startswith("ERROR:"):
                 logger.error(f"System '{self.system_id}': Task {task_id} failed: {result}")
                 final_results[task_id] = result
                 error_count += 1
            else:
                 final_results[task_id] = result
                 success_count +=1

        logger.info(f"System '{self.system_id}' finished run. Success: {success_count}, Errors: {error_count}")
        return final_results

# --- NEW: FrontendAgentSystem Class ---
class FrontendAgentSystem:
    """
    A system that orchestrates the generation of a multi-page frontend website
    based on a user request, using planning, task distribution, and parallel
    file generation agents.
    """
    def __init__(self,
                 system_id: str,
                 model: BaseModel,
                 artifacts_dir: str = "build_artifacts",
                 website_output_dir: str = "generated_website",
                 max_tokens_plan: int = 4096,
                 max_tokens_file_gen: int = 34000,
                 temperature: float = 0.5):
        """
        Initializes the FrontendAgentSystem.

        Args:
            system_id (str): Identifier for this system instance.
            model (BaseModel): The language model instance to use.
            artifacts_dir (str): Directory to save intermediate build artifacts (plan, etc.).
            website_output_dir (str): Directory to save the final generated website files.
            max_tokens_plan (int): Max tokens for planning/distribution LLM calls.
            max_tokens_file_gen (int): Max tokens for file generation LLM calls.
            temperature (float): Default temperature for LLM calls.
        """
        self.system_id = system_id
        self.model = model
        self.artifacts_dir = artifacts_dir
        self.website_output_dir = website_output_dir
        self.max_tokens_plan = max_tokens_plan
        self.max_tokens_file_gen = max_tokens_file_gen
        self.temperature = temperature
        self._logger = logging.getLogger(f"FrontendAgentSystem.{system_id}") # Specific logger instance

        # Ensure output directories exist
        os.makedirs(self.artifacts_dir, exist_ok=True)
        os.makedirs(self.website_output_dir, exist_ok=True)
        self._logger.info(f"Initialized. Artifacts Dir: '{self.artifacts_dir}', Website Output Dir: '{self.website_output_dir}'")

    # --- Helper Methods as Private Class Methods ---

    def _save_file(self, filename: str, content: str, is_artifact: bool = False) -> bool:
        """Saves content to a file in the appropriate directory (artifact or website)."""
        base_dir = self.artifacts_dir if is_artifact else self.website_output_dir
        # Basic security check
        if os.path.isabs(filename) or ".." in filename:
            self._logger.error(f"Invalid filepath detected (absolute or traversal): {filename}")
            return False

        full_path = os.path.join(base_dir, filename)
        try:
            os.makedirs(os.path.dirname(full_path), exist_ok=True)
            with open(full_path, 'w', encoding='utf-8') as f:
                f.write(content)
            self._logger.info(f"Successfully saved file: {full_path}")
            return True
        except OSError as e:
            self._logger.error(f"Failed to save file {full_path}: {e}")
            return False
        except Exception as e:
            self._logger.error(f"An unexpected error occurred saving file {full_path}: {e}")
            return False

    def _extract_code(self, llm_output: str) -> Optional[str]:
        """Extracts the first code block (```...```) from LLM output."""
        match = re.search(r"```(?:[a-zA-Z0-9]*\n)?(.*?)```", llm_output, re.DOTALL | re.IGNORECASE)
        if match:
            code = match.group(1).strip()
            if code:
                self._logger.debug(f"Extracted code block (length: {len(code)}).")
                return code
            else:
                self._logger.warning("Found code block delimiters but content inside was empty.")
                return None
        else:
            self._logger.warning("No markdown code block found in the LLM output.")
            if llm_output.strip().startswith('<') or llm_output.strip().startswith(('function', 'const', 'let', 'var', 'import', 'public class', '@', '.', '#')):
                 self._logger.warning("No code block found, but output resembles code. Returning full output.")
                 return llm_output.strip()
            return None

    def _parse_task_distribution(self, dist_output: str) -> Tuple[Optional[str], List[Dict[str, str]]]:
        """Parses the Task Distributor output to extract memory and file prompts."""
        memory = None
        prompts = []
        memory_match = re.search(r"<memory>(.*?)</memory>", dist_output, re.DOTALL | re.IGNORECASE)
        if memory_match:
            memory = memory_match.group(1).strip()
            self._logger.info("Extracted memory block.")
        else:
            self._logger.warning("Could not find <memory> block in Task Distributor output.")

        prompt_pattern = re.compile(
            r'<prompt\s+filename="(?P<filename>[^"]+)"(?:\s+url="(?P<url>[^"]+)?"\s*)?>(?P<prompt_content>.*?)</prompt>',
            re.DOTALL | re.IGNORECASE
        )
        for match in prompt_pattern.finditer(dist_output):
            data = match.groupdict()
            filename = data['filename'].strip()
            prompt_content = data['prompt_content'].strip()
            url = data.get('url', '').strip()

            if filename and prompt_content:
                prompts.append({
                    "filename": filename,
                    "url": url if url else filename,
                    "prompt": prompt_content
                })
                self._logger.debug(f"Extracted prompt for file: {filename}")
            else:
                self._logger.warning(f"Found prompt block but filename or content was empty: {match.group(0)}")

        if not prompts:
             self._logger.warning("Could not find any valid <prompt ...> blocks in Task Distributor output.")
        return memory, prompts

    def _strip_think_tags(self, text: str) -> str:
        """Removes content up to and including the first </think> tag if present."""
        think_end_tag = "</think>"
        tag_pos = text.find(think_end_tag)
        if tag_pos != -1:
            self._logger.debug("Found </think> tag, stripping preceding content.")
            return text[tag_pos + len(think_end_tag):].strip()
        else:
            return text

    # --- Main Run Method ---
    async def run(self, user_request: str):
        """
        Orchestrates the frontend generation process for the given user request.
        """
        start_time_build = time.time()
        self._logger.info(f"--- Starting Frontend Build Process for Request: '{user_request}' ---")

        # Create agent instances needed for this run
        # Using separate IDs for clarity within this system run
        orchestration_agent = BasicAgent(agent_id=f"{self.system_id}_orchestrator", model=self.model)
        generation_agent = BasicAgent(agent_id=f"{self.system_id}_generator", model=self.model)

        # === STEP 1: Planner Agent ===
        self._logger.info("--- Step 1: Planning ---")
        plan = None
        planner_prompt = f"""
You are an expert software architect and planner. Your goal is to create a comprehensive plan to build the software requested by the user.

User Request: "{user_request}"

Instructions:
1.  Analyze the user request thoroughly.
2.  Think through the project structure: Define a clear and logical directory and file structure. List all necessary files (HTML, CSS, JavaScript, images, etc.).
3.  Think through styling and UI/UX: Describe the desired look and feel, color palette, typography, and any key UI components. Consider responsiveness.
4.  Think through images and media: Identify the types of images or media needed and suggest placeholders or sources if applicable.
5.  Think through formatting and content: Outline the content required for each page or component.
6.  Think through frameworks and libraries: Recommend appropriate technologies (e.g., Tailwind CSS was mentioned in context, stick to basic HTML/CSS/JS if not specified, but plan for it if requested). If using libraries, specify how they should be included (CDN, local).
7.  Think through caveats and best practices: Mention any potential challenges, limitations, or important development practices (like accessibility, SEO basics for web).
8.  Output *only* the detailed plan in a clear, structured format (e.g., using markdown). Do not include any conversational text before or after the plan itself. Ensure the plan is detailed enough for another agent to break it down into specific file-generation tasks.
"""
        try:
            self._logger.info("Calling Planner Agent...")
            raw_plan_response = await orchestration_agent.run(planner_prompt, max_tokens=self.max_tokens_plan, temperature=self.temperature)
            plan = self._strip_think_tags(raw_plan_response) # Strip think tags
            if not plan or plan.startswith("ERROR:") :
                 self._logger.error(f"Planner Agent failed or returned an error: {plan}")
                 raise ValueError("Planner Agent failed.")
            self._logger.info("Planner Agent finished. Plan received (first 500 chars):\n" + plan[:500] + "...")
            # Save the plan artifact
            self._save_file("plan.md", f"User Request:\n{user_request}\n\n---\n\nGenerated Plan:\n{plan}", is_artifact=True)

        except Exception as e:
            self._logger.error(f"Error during Planning step: {e}")
            # Note: We don't close the client here, caller is responsible
            return # Stop the process

        # === STEP 2: Task Distribution Agent ===
        self._logger.info("--- Step 2: Task Distribution ---")
        memory = None
        file_prompts = []
        distributor_prompt = f"""
You are a task distribution agent. Your input is a software development plan. Your goal is to break down this plan into:
1.  A shared `<memory>` block containing essential context, design guidelines, framework choices, and overall architecture described in the plan that *all* subsequent file-generation agents need to know to ensure consistency.
2.  Individual `<prompt>` blocks, one for *each file* identified in the plan's file structure. Each prompt block must specify the target `filename` and contain a highly detailed and specific prompt instructing another agent on *exactly* what code to generate for that single file, referencing the shared memory/plan as needed.

Development Plan:
--- START PLAN ---
{plan}
--- END PLAN ---

Instructions:
1.  Carefully read the entire Development Plan.
2.  Extract the core principles, design language, chosen frameworks/libraries, color schemes, typography, file structure overview, and any global requirements into a concise `<memory>` block. This memory block should *not* contain specific file contents but rather the shared context.
3.  For *each* file mentioned in the plan's file structure (e.g., index.html, style.css, script.js, about.html, assets/logo.png):
    *   If it's a code file (HTML, CSS, JS, etc.): Create a `<prompt filename="path/to/file.ext">` block. Inside this block, write a very specific prompt detailing exactly what code needs to be in this file. Include details about structure, content (referencing the plan), functionality, required HTML elements, CSS classes (mentioning framework if used), JS functions, links to other files (using relative paths based on the plan's structure), and references to the shared `<memory>` context if necessary. Instruct the agent receiving this prompt to output *only* the raw code for the file, enclosed in appropriate markdown ``` code blocks.
    *   If it's a non-code asset (like an image placeholder path mentioned in the plan): You can optionally create a prompt block instructing to note this placeholder or skip creating a prompt block for it. Focus on generating code files.
4.  Ensure filenames and relative paths in the prompts are consistent with the file structure defined in the plan.
5.  Output *only* the `<memory>` block followed immediately by all the `<prompt>` blocks. Do not include any other conversational text, introductions, or summaries.
"""
        try:
            self._logger.info("Calling Task Distributor Agent...")
            raw_dist_response = await orchestration_agent.run(distributor_prompt, max_tokens=self.max_tokens_plan, temperature=self.temperature)
            dist_output = self._strip_think_tags(raw_dist_response) # Strip think tags
            if not dist_output or dist_output.startswith("ERROR:") :
                 self._logger.error(f"Task Distributor Agent failed or returned an error: {dist_output}")
                 raise ValueError("Task Distributor Agent failed.")

            self._logger.info("Task Distributor Agent finished. Parsing output...")
            memory, file_prompts = self._parse_task_distribution(dist_output)

            if not memory or not file_prompts:
                self._logger.error("Failed to parse memory or file prompts from Task Distributor output. Check the output format.")
                self._save_file("task_distribution_raw_output.txt", f"Distributor Prompt:\n{distributor_prompt}\n\n---\n\nRaw Response:\n{raw_dist_response}", is_artifact=True)
                raise ValueError("Failed to parse Task Distributor output.")

            self._logger.info(f"Successfully parsed memory and {len(file_prompts)} file prompts.")
            # Save parsed data artifact
            parsed_prompts_log = f"<memory>\n{memory}\n</memory>\n\n" + "\n\n".join([f"<prompt filename=\"{p['filename']}\">\n{p['prompt']}\n</prompt>" for p in file_prompts])
            self._save_file("task_distribution_parsed.txt", parsed_prompts_log, is_artifact=True)

        except Exception as e:
            self._logger.error(f"Error during Task Distribution step: {e}")
            return # Stop the process

        # === STEP 3: File Generation Agents (Parallel Execution) ===
        self._logger.info("--- Step 3: Generating Files in Parallel ---")
        if not file_prompts:
            self._logger.warning("No file prompts were generated. Skipping file generation step.")
        else:
            tasks = []
            file_mapping = {}

            self._logger.info(f"Preparing {len(file_prompts)} file generation tasks...")
            for i, task_info in enumerate(file_prompts):
                filename = task_info["filename"]
                specific_prompt = task_info["prompt"]
                generation_prompt = f"""
<memory>
{memory}
</memory>

<prompt filename="{filename}">
{specific_prompt}
</prompt>

Based *only* on the specific prompt for `{filename}` above and the shared `<memory>` context, generate the complete, raw code content for the file `{filename}`.
Output *only* the raw code content for the file, enclosed in the appropriate markdown code block (e.g., ```html ... ```, ```css ... ```, ```javascript ... ```).
Do not include any other text, explanations, introductions, or summaries outside the code block.
"""
                task = generation_agent.run(generation_prompt, max_tokens=self.max_tokens_file_gen, temperature=self.temperature)
                tasks.append(task)
                file_mapping[i] = filename
                self._logger.debug(f"Created generation task for: {filename}")

            self._logger.info(f"Launching {len(tasks)} file generation tasks concurrently...")
            start_time_gen = time.time()
            results = await asyncio.gather(*tasks, return_exceptions=True)
            end_time_gen = time.time()
            self._logger.info(f"File generation tasks completed in {end_time_gen - start_time_gen:.2f} seconds.")

            self._logger.info("Processing generation results and saving files...")
            files_saved = 0
            files_failed = 0
            for i, result in enumerate(results):
                filename = file_mapping[i]
                self._logger.debug(f"Processing result for: {filename}")

                if isinstance(result, Exception):
                    self._logger.error(f"Task for {filename} failed with exception: {result}")
                    files_failed += 1
                    self._save_file(f"{filename}.error.txt", f"Task failed with exception:\n{result}", is_artifact=False) # Save error in website dir
                elif isinstance(result, str) and result.startswith("ERROR:"):
                     self._logger.error(f"Task for {filename} returned an error: {result}")
                     files_failed += 1
                     self._save_file(f"{filename}.error.txt", f"Task returned error:\n{result}", is_artifact=False)
                elif isinstance(result, str):
                    generation_output = self._strip_think_tags(result) # Strip think tags
                    code_content = self._extract_code(generation_output)
                    if code_content:
                        if self._save_file(filename, code_content, is_artifact=False): # Save to website dir
                            files_saved += 1
                        else:
                            files_failed += 1
                    else:
                        self._logger.error(f"Failed to extract code block for {filename}. Saving raw output.")
                        files_failed += 1
                        self._save_file(f"{filename}.raw_output.txt", generation_output, is_artifact=False) # Save raw output to website dir
                else:
                     self._logger.error(f"Task for {filename} returned unexpected result type: {type(result)}")
                     files_failed += 1
                     self._save_file(f"{filename}.error.txt", f"Task returned unexpected result type: {type(result)}\n{result}", is_artifact=False)

            self._logger.info(f"File generation finished. Saved: {files_saved}, Failed/Skipped: {files_failed}")

        end_time_build = time.time()
        self._logger.info(f"--- Frontend build process finished in {end_time_build - start_time_build:.2f} seconds ---")
        self._logger.info(f"Generated website files should be in '{self.website_output_dir}'")
        self._logger.info(f"Build artifacts (plan, etc.) are in '{self.artifacts_dir}'")
</file>

</files>
</file>

<file path=".env copy">
API_URL=
API_KEY=
MODEL_NAME=Qwen/Qwen3-30B-A3B-FP8
MAX_TOKENS=32000
TEMPERATURE=0.7
</file>

<file path=".repomixignore">
# Add patterns to ignore here, one per line
# Example:
# *.log
# tmp/
venv/
__pycache__/
examples/
node_modules/
</file>

<file path="builder/backend/agents/basic.py">
# backend/agents/basic.py
import logging
from tframex.agents import BasicAgent # Assuming tframex is importable

logger = logging.getLogger(__name__)

async def execute_basic_agent(agent_instance: BasicAgent, input_data: dict) -> dict:
    """
    Executes the BasicAgent logic.
    Input data expected: {'prompt': str, 'max_tokens': Optional[int]}
    Output data: {'output': str}
    """
    prompt = input_data.get('prompt')
    max_tokens = input_data.get('max_tokens') # Can be None

    if not prompt:
        logger.error("BasicAgent execution failed: 'prompt' input is missing.")
        # You might want to raise an exception or return an error structure
        return {"output": "Error: Prompt input is missing."}

    logger.info(f"Running BasicAgent with prompt: '{prompt[:50]}...', max_tokens: {max_tokens}")
    try:
        response = await agent_instance.run(prompt=prompt, max_tokens=max_tokens)
        return {"output": response}
    except Exception as e:
        logger.error(f"BasicAgent execution encountered an error: {e}", exc_info=True)
        return {"output": f"Error during execution: {e}"}
</file>

<file path="builder/backend/agents/chain.py">
# backend/agents/chain.py
import logging
from tframex.systems import ChainOfAgents # Assuming tframex is importable

logger = logging.getLogger(__name__)

async def execute_chain_system(system_instance: ChainOfAgents, input_data: dict) -> dict:
    """
    Executes the ChainOfAgents system logic.
    Input data expected: {'initial_prompt': str, 'long_text': str, 'max_tokens': Optional[int]}
                         (chunk_size, chunk_overlap are part of system config, not runtime input)
    Output data: {'output': str}
    """
    initial_prompt = input_data.get('initial_prompt')
    long_text = input_data.get('long_text')
    max_tokens = input_data.get('max_tokens') # Can be None

    if not initial_prompt:
        return {"output": "Error: 'initial_prompt' input is missing."}
    if not long_text:
        return {"output": "Error: 'long_text' input is missing."}

    logger.info(f"Running ChainOfAgents with initial_prompt: '{initial_prompt[:50]}...', long_text: '{long_text[:50]}...', max_tokens: {max_tokens}")
    try:
        # Pass kwargs like max_tokens down
        response = await system_instance.run(
            initial_prompt=initial_prompt,
            long_text=long_text,
            max_tokens=max_tokens
        )
        return {"output": response}
    except Exception as e:
        logger.error(f"ChainOfAgents execution encountered an error: {e}", exc_info=True)
        return {"output": f"Error during execution: {e}"}
</file>

<file path="builder/backend/agents/context.py">
# backend/agents/context.py
import logging
from tframex.agents import ContextAgent # Assuming tframex is importable

logger = logging.getLogger(__name__)

async def execute_context_agent(agent_instance: ContextAgent, input_data: dict) -> dict:
    """
    Executes the ContextAgent logic.
    Input data expected: {'prompt': str, 'context': str, 'max_tokens': Optional[int]}
    Output data: {'output': str}
    """
    prompt = input_data.get('prompt')
    context = input_data.get('context', '') # Default to empty string if missing
    max_tokens = input_data.get('max_tokens') # Can be None

    if not prompt:
        logger.error("ContextAgent execution failed: 'prompt' input is missing.")
        return {"output": "Error: Prompt input is missing."}

    # Update the agent's context dynamically IF context is provided in input_data
    # Otherwise, it uses the context it was initialized with (if any, though unlikely here)
    # A better approach might be to *always* expect context via input_data for stateless execution
    if 'context' in input_data:
         agent_instance.context = context # Update context before running

    logger.info(f"Running ContextAgent with prompt: '{prompt[:50]}...', context: '{context[:50]}...', max_tokens: {max_tokens}")
    try:
        response = await agent_instance.run(prompt=prompt, max_tokens=max_tokens)
        return {"output": response}
    except Exception as e:
        logger.error(f"ContextAgent execution encountered an error: {e}", exc_info=True)
        return {"output": f"Error during execution: {e}"}
</file>

<file path="builder/backend/agents/distributor_agent.py">
# Filename: builder/backend/agents/distributor_agent.py

import logging
import re
import json
from typing import List, Dict, Tuple, Optional
from tframex.agents import BaseAgent # Use BaseAgent
from .utils import strip_think_tags # Import the shared utility

logger = logging.getLogger(__name__)

# Agent Configuration
DEFAULT_MAX_TOKENS_PLAN = 34000
DEFAULT_TEMPERATURE = 0.5

# --- Helper Function (Moved from original script) ---
def _parse_task_distribution(dist_output: str) -> Tuple[Optional[str], List[Dict[str, str]]]:
    """
    Parses the Task Distributor output to extract memory and file prompts.
    Expected format:
    <memory>...</memory>
    <prompt filename="file1.html" url="...">...</prompt>
    <prompt filename="style.css" url="...">...</prompt>
    """
    memory = None
    prompts = []

    # Extract memory
    memory_match = re.search(r"<memory>(.*?)</memory>", dist_output, re.DOTALL | re.IGNORECASE)
    if memory_match:
        memory = memory_match.group(1).strip()
        logger.info("Extracted memory block.")
    else:
        logger.warning("Could not find <memory> block in Task Distributor output.")

    # Extract prompts
    prompt_pattern = re.compile(
        r'<prompt\s+filename="(?P<filename>[^"]+)"(?:\s+url="(?P<url>[^"]+)?"\s*)?>(?P<prompt_content>.*?)</prompt>',
        re.DOTALL | re.IGNORECASE
    )
    for match in prompt_pattern.finditer(dist_output):
        data = match.groupdict()
        filename = data['filename'].strip()
        prompt_content = data['prompt_content'].strip()
        url = data.get('url', '').strip() # Handle optional url

        if filename and prompt_content:
            prompts.append({
                "filename": filename,
                "url": url if url else filename, # Default url to filename if not provided
                "prompt": prompt_content
            })
            logger.debug(f"Extracted prompt for file: {filename}")
        else:
            logger.warning(f"Found prompt block but filename or content was empty: {match.group(0)}")

    if not prompts:
         logger.warning("Could not find any valid <prompt ...> blocks in Task Distributor output.")

    return memory, prompts
# --- End Helper Function ---


async def execute_distributor_agent(agent_instance: BaseAgent, input_data: dict) -> dict:
    """
    Executes the Task Distributor Agent logic.
    Input: {'plan': str}
    Output: {'memory': str, 'file_prompts_json': str} # Pass prompts as JSON string
    """
    plan = input_data.get('plan')
    max_tokens = input_data.get('max_tokens', DEFAULT_MAX_TOKENS_PLAN)

    if not plan:
        logger.error("DistributorAgent execution failed: 'plan' input is missing.")
        return {"memory": None, "file_prompts_json": "[]", "error": "Error: Plan input is missing."} # Return error in output dict too

    distributor_prompt = f"""
You are a task distribution agent. Your input is a software development plan. Your goal is to break down this plan into:
1.  A shared `<memory>` block containing essential context, design guidelines, framework choices, and overall architecture described in the plan that *all* subsequent file-generation agents need to know to ensure consistency. This should be lengthy.
2.  Individual `<prompt>` blocks, one for *each file* identified in the plan's file structure. Each prompt block must specify the target `filename` and contain a highly detailed and specific prompt instructing another agent on *exactly* what code to generate for that single file, referencing the shared memory/plan as needed.
Any information the prompt that is coupled with another file, the entire integration must be properly and fully explained. Make the most solid pseudocode possible and talk about every single tiny detail in the prompt.
Development Plan:
--- START PLAN ---
{plan}
--- END PLAN ---

Instructions:
1.  Carefully read the entire Development Plan.
2.  Extract the core principles, design language, chosen frameworks/libraries, color schemes, typography, file structure overview, and any global requirements into a concise `<memory>` block. This memory block should *not* contain specific file contents but rather the shared context.
3.  For *each* file mentioned in the plan's file structure (e.g., index.html, style.css, script.js, about.html, assets/logo.png):
    *   If it's a code file (HTML, CSS, JS, etc.): Create a `<prompt filename="path/to/file.ext">` block. Inside this block, write a very specific prompt detailing exactly what code needs to be in this file. Include details about structure, content (referencing the plan), functionality, required HTML elements, CSS classes (mentioning framework if used), JS functions, links to other files (using relative paths based on the plan's structure), and references to the shared `<memory>` context if necessary. Instruct the agent receiving this prompt to output *only* the raw code for the file, enclosed in appropriate markdown ``` code blocks.
    *   If it's a non-code asset (like an image placeholder path mentioned in the plan): You can optionally create a prompt block instructing to note this placeholder or skip creating a prompt block for it. Focus on generating code files.
4.  Ensure filenames and relative paths in the prompts are consistent with the file structure defined in the plan.
5.  Output *only* the `<memory>` block followed immediately by all the `<prompt>` blocks. Do not include any other conversational text, introductions, or summaries.

Example Output Structure:
<memory>
Shared context, design guidelines, framework info...
</memory>
<prompt filename="index.html" url="index.html">
Generate the complete HTML structure for the main landing page (index.html). Use semantic HTML5 tags. Include a header, navigation (linking to about.html), main content area, and footer based on the plan and shared memory design. Use Tailwind CSS classes defined in the memory block for styling. Content should be based on the 'Home Page Content' section of the plan. Output only the complete HTML code in a ```html ... ``` block.
</prompt>
<prompt filename="css/style.css" url="css/style.css">
Generate the CSS rules for the website based on the plan and memory. Define styles for base elements, utility classes (if not using a framework like Tailwind mentioned in memory), and specific component styles outlined in the plan. Reference the color palette and typography from memory. Output only the CSS code in a ```css ... ``` block.
</prompt>
<prompt filename="js/script.js" url="js/script.js">
Generate the JavaScript code for interactive elements mentioned in the plan, like a mobile menu toggle or an image carousel. Ensure code is clean and follows best practices mentioned in memory. Output only the JavaScript code in a ```javascript ... ``` block.
</prompt>
... (more prompt blocks for other files)
"""

    logger.info(f"Running DistributorAgent...")
    try:
        # Use BasicAgent instance passed in
        raw_dist_response = await agent_instance.run(distributor_prompt, max_tokens=max_tokens, temperature=DEFAULT_TEMPERATURE)
        dist_output = strip_think_tags(raw_dist_response)

        if not dist_output or dist_output.startswith("ERROR:"):
            logger.error(f"Task Distributor Agent failed or returned an error: {dist_output}")
            return {"memory": None, "file_prompts_json": "[]", "error": f"Error: Distributor Agent failed. Details: {dist_output}"}

        logger.info("Task Distributor Agent finished. Parsing output...")
        memory, file_prompts = _parse_task_distribution(dist_output)

        if not memory or not file_prompts:
            logger.error("Failed to parse memory or file prompts from Task Distributor output.")
            # Save raw output artifact (optional here)
            # save_file("task_distribution_raw_output.txt", ..., base_dir="build_artifacts")
            return {"memory": memory, "file_prompts_json": "[]", "error": "Error: Failed to parse Task Distributor output. Check logs."}

        logger.info(f"Successfully parsed memory and {len(file_prompts)} file prompts.")
        # Save parsed data artifact (optional here)
        # parsed_prompts_log = f"<memory>\n{memory}\n</memory>\n\n" + ...
        # save_file("task_distribution_parsed.txt", parsed_prompts_log, base_dir="build_artifacts")

        # Serialize file_prompts to JSON string for output
        file_prompts_json = json.dumps(file_prompts)

        return {"memory": memory, "file_prompts_json": file_prompts_json}

    except Exception as e:
        logger.error(f"DistributorAgent execution encountered an error: {e}", exc_info=True)
        return {"memory": None, "file_prompts_json": "[]", "error": f"Error during DistributorAgent execution: {e}"}
</file>

<file path="builder/backend/agents/file_generator_agent.py">
# Filename: builder/backend/agents/file_generator_agent.py

import logging
import re
import json
import os
import asyncio
import time
from typing import List, Dict, Optional, Any, Tuple
from tframex.agents import BaseAgent # Use BaseAgent
from .utils import strip_think_tags # Import the shared utility

logger = logging.getLogger(__name__)

# Agent Configuration
DEFAULT_MAX_TOKENS_FILE_GEN = 34000
DEFAULT_TEMPERATURE = 0.5
GENERATED_BASE_DIR = "generated" # Base directory for all runs

# --- Helper Functions (Moved/Adapted from original script) ---

def _extract_code(llm_output: str) -> Optional[str]:
    """Extracts the first code block (```...```) from LLM output."""
    # Regex to find markdown code blocks, capturing the content
    # It handles optional language identifiers (like ```html)
    match = re.search(r"```(?:[a-zA-Z0-9]*\n)?(.*?)```", llm_output, re.DOTALL | re.IGNORECASE)
    if match:
        code = match.group(1).strip()
        # Basic check if code seems plausible (not empty)
        if code:
            logger.debug(f"Extracted code block (length: {len(code)}).")
            return code
        else:
             logger.warning("Found code block delimiters but content inside was empty.")
             return None # Explicitly return None if block is empty
    else:
        logger.warning("No markdown code block found in the LLM output.")
        # Sometimes the LLM might forget the ```, attempt to return the whole thing if it looks like code
        if llm_output.strip().startswith('<') or llm_output.strip().startswith(('function', 'const', 'let', 'var', 'import', 'public class', '@', '.', '#')):
             logger.warning("No code block found, but output resembles code. Returning full output.")
             return llm_output.strip()
        return None

def _save_file(run_id: str, filename: str, content: str) -> Tuple[bool, str]:
    """Saves content to a file within the specific run's generated folder."""
    if not run_id:
        logger.error("Cannot save file: run_id is missing.")
        return False, ""

    # Construct path relative to the base generated directory
    run_dir = os.path.join(GENERATED_BASE_DIR, run_id)
    # Ensure the path is relative to the run directory (prevent traversal)
    # Basic security check
    if os.path.isabs(filename) or ".." in filename:
        logger.error(f"Invalid filename detected (absolute or traversal): {filename}")
        return False, ""

    full_path = os.path.join(run_dir, filename)
    try:
        # Create directories if they don't exist
        os.makedirs(os.path.dirname(full_path), exist_ok=True)
        with open(full_path, 'w', encoding='utf-8') as f:
            f.write(content)
        logger.info(f"Successfully saved file: {full_path}")
        return True, full_path
    except OSError as e:
        logger.error(f"Failed to save file {full_path}: {e}")
        return False, full_path
    except Exception as e:
        logger.error(f"An unexpected error occurred saving file {full_path}: {e}")
        return False, full_path
# --- End Helper Functions ---

async def _generate_single_file(agent_instance: BaseAgent, run_id: str, filename: str, memory: str, specific_prompt: str, max_tokens: int, temperature: float) -> Dict[str, Any]:
    """Internal task to generate and save a single file."""
    generation_prompt = f"""
<memory>
{memory}
</memory>

<prompt filename="{filename}">
{specific_prompt}
</prompt>

Based *only* on the specific prompt for `{filename}` above and the shared `<memory>` context, generate the complete, raw code content for the file `{filename}`.
Output *only* the raw code content for the file, enclosed in the appropriate markdown code block (e.g., ```html ... ```, ```css ... ```, ```javascript ... ```).
Do not include any other text, explanations, introductions, or summaries outside the code block.
For tailwind, use this: <script src="https://cdn.jsdelivr.net/npm/@tailwindcss/browser@4"></script>
"""
    file_result = {"filename": filename, "status": "failed", "path": None, "error": None}
    try:
        logger.debug(f"Starting generation task for: {filename} (Run ID: {run_id})")
        # Use BasicAgent instance passed in
        raw_response = await agent_instance.run(generation_prompt, max_tokens=max_tokens, temperature=temperature)
        generation_output = strip_think_tags(raw_response)

        if generation_output.startswith("ERROR:"):
             logger.error(f"Generation task for {filename} returned an error: {generation_output}")
             file_result["error"] = f"LLM Error: {generation_output}"
             _save_file(run_id, f"{filename}.error.txt", generation_output)
             return file_result

        code_content = _extract_code(generation_output)
        if code_content:
            saved_ok, saved_path = _save_file(run_id, filename, code_content)
            if saved_ok:
                file_result["status"] = "success"
                file_result["path"] = saved_path
            else:
                file_result["error"] = f"Failed to save file to {saved_path}"
        else:
            logger.error(f"Failed to extract code block for {filename}. Saving raw output.")
            file_result["error"] = "Failed to extract code block from LLM output."
            _save_file(run_id, f"{filename}.raw_output.txt", generation_output)

        return file_result

    except Exception as e:
        logger.error(f"Generation task for {filename} failed with exception: {e}", exc_info=True)
        file_result["error"] = f"Exception: {e}"
        _save_file(run_id, f"{filename}.error.txt", f"Task failed with exception:\n{e}")
        return file_result


async def execute_file_generator_agent(agent_instance: BaseAgent, input_data: dict) -> dict:
    """
    Executes the File Generator Agent logic.
    Input: {'memory': str, 'file_prompts_json': str, 'run_id': str}
    Output: {'generation_summary': str, 'preview_link': Optional[str]}
    """
    memory = input_data.get('memory')
    file_prompts_json = input_data.get('file_prompts_json')
    run_id = input_data.get('run_id') # Crucial input from executor
    max_tokens = input_data.get('max_tokens', DEFAULT_MAX_TOKENS_FILE_GEN)
    temperature = input_data.get('temperature', DEFAULT_TEMPERATURE)

    if not memory or not file_prompts_json or not run_id:
        error_msg = f"FileGeneratorAgent execution failed: Missing input. Got memory: {bool(memory)}, prompts: {bool(file_prompts_json)}, run_id: {bool(run_id)}"
        logger.error(error_msg)
        return {"generation_summary": error_msg, "preview_link": None}

    try:
        file_prompts: List[Dict[str, str]] = json.loads(file_prompts_json)
    except json.JSONDecodeError as e:
        error_msg = f"FileGeneratorAgent failed: Could not decode file_prompts_json: {e}"
        logger.error(error_msg)
        return {"generation_summary": error_msg, "preview_link": None}

    if not file_prompts:
        logger.warning("No file prompts provided to FileGeneratorAgent. Nothing to generate.")
        return {"generation_summary": "No files requested for generation.", "preview_link": None}

    # --- Parallel Execution ---
    logger.info(f"Running FileGeneratorAgent: Preparing {len(file_prompts)} file generation tasks for Run ID: {run_id}...")
    tasks = []
    for task_info in file_prompts:
        filename = task_info["filename"]
        specific_prompt = task_info["prompt"]

        task = _generate_single_file(
            agent_instance=agent_instance,
            run_id=run_id,
            filename=filename,
            memory=memory,
            specific_prompt=specific_prompt,
            max_tokens=max_tokens,
            temperature=temperature
        )
        tasks.append(task)

    logger.info(f"Launching {len(tasks)} file generation tasks concurrently...")
    start_time_gen = time.time()
    # results will be a list of dictionaries like file_result defined in _generate_single_file
    results: List[Dict[str, Any]] = await asyncio.gather(*tasks, return_exceptions=False) # Handle exceptions within task
    end_time_gen = time.time()
    logger.info(f"File generation tasks completed in {end_time_gen - start_time_gen:.2f} seconds.")

    # --- Process Results and Create Summary ---
    files_saved = 0
    files_failed = 0
    summary_lines = [f"File Generation Summary (Run ID: {run_id}):"]
    preview_target_file = None # Find index.html or similar for preview link

    for result in results:
        filename = result.get("filename", "Unknown File")
        status = result.get("status", "failed")
        path = result.get("path")
        error = result.get("error")

        if status == "success" and path:
            files_saved += 1
            summary_lines.append(f"  - [SUCCESS] {filename} -> {path}")
            # Check if this is a potential preview target
            if filename.lower() == 'index.html' or filename.lower().endswith('.html'):
                 if preview_target_file is None or filename.lower() == 'index.html': # Prioritize index.html
                     preview_target_file = filename
        else:
            files_failed += 1
            summary_lines.append(f"  - [FAILED]  {filename} - Error: {error}")

    summary_lines.append(f"\nFinished. Saved: {files_saved}, Failed/Skipped: {files_failed}")
    generation_summary = "\n".join(summary_lines)
    logger.info(f"File generation summary created for Run ID: {run_id}. Saved: {files_saved}, Failed: {files_failed}")

    # --- Generate Preview Link ---
    preview_link = None
    if preview_target_file:
        # Construct the relative URL path for the preview endpoint
        # Flask route will be /api/preview/<run_id>/<filepath>
        preview_link = f"/api/preview/{run_id}/{preview_target_file}"
        logger.info(f"Generated preview link: {preview_link}")

    return {"generation_summary": generation_summary, "preview_link": preview_link}
</file>

<file path="builder/backend/agents/multi_call.py">
# backend/agents/multi_call.py
import logging
import os
from tframex.systems import MultiCallSystem # Assuming tframex is importable

logger = logging.getLogger(__name__)

# Get output dir from env or use a default relative to backend/
DEFAULT_MULTI_CALL_OUTPUT_DIR = os.getenv("MULTI_CALL_OUTPUT_DIR", "example_outputs/ex4_multi_call_outputs")


async def execute_multi_call_system(system_instance: MultiCallSystem, input_data: dict) -> dict:
    """
    Executes the MultiCallSystem logic.
    Input data expected: {'prompt': str, 'num_calls': int, 'base_filename': str, 'max_tokens': Optional[int]}
                         (output_dir is part of system config, not runtime input)
    Output data: {'output': str} # Returns a summary string
    """
    prompt = input_data.get('prompt')
    num_calls = input_data.get('num_calls', 3) # Default if not provided
    base_filename = input_data.get('base_filename', 'multi_output')
    max_tokens = input_data.get('max_tokens')
    # Output dir comes from system config (env var)
    output_dir = system_instance.default_output_dir # Access the configured dir

    if not prompt:
        return {"output": "Error: 'prompt' input is missing."}

    logger.info(f"Running MultiCallSystem with prompt: '{prompt[:50]}...', num_calls: {num_calls}, base_filename: {base_filename}, output_dir: {output_dir}, max_tokens: {max_tokens}")

    try:
        # Ensure output dir exists before running
        os.makedirs(output_dir, exist_ok=True)

        results = await system_instance.run(
            prompt=prompt,
            num_calls=num_calls,
            output_dir=output_dir, # Pass the configured dir
            base_filename=base_filename,
            max_tokens=max_tokens
        )
        # Format results for display
        result_summary = f"MultiCallSystem completed. Results saved in '{output_dir}'.\nSummary:\n"
        for task_id, result_path_or_error in results.items():
                result_summary += f"  - {task_id}: {result_path_or_error}\n"
        return {"output": result_summary} # Return the summary string

    except Exception as e:
        logger.error(f"MultiCallSystem execution encountered an error: {e}", exc_info=True)
        return {"output": f"Error during execution: {e}"}
</file>

<file path="builder/backend/agents/planner_agent.py">
# Filename: builder/backend/agents/planner_agent.py

import logging
from tframex.agents import BaseAgent # Use BaseAgent for potential future state
from tframex.model import VLLMModel
from .utils import strip_think_tags # Import the shared utility

logger = logging.getLogger(__name__)

# Agent Configuration (Could be loaded from env or passed during init)
DEFAULT_MAX_TOKENS_PLAN = 34000
DEFAULT_TEMPERATURE = 0.5

async def execute_planner_agent(agent_instance: BaseAgent, input_data: dict) -> dict:
    """
    Executes the Planner Agent logic.
    Input: {'user_request': str}
    Output: {'plan': str}
    """
    user_request = input_data.get('user_request')
    max_tokens = input_data.get('max_tokens', DEFAULT_MAX_TOKENS_PLAN) # Allow override

    if not user_request:
        logger.error("PlannerAgent execution failed: 'user_request' input is missing.")
        return {"plan": "Error: User Request input is missing."}

    planner_prompt = f"""
You are an expert software architect and planner. Your goal is to create a comprehensive plan to build the software requested by the user.

User Request: "{user_request}"

Instructions:
1.  Analyze the user request thoroughly.
2.  Think through the project structure: Define a clear and logical directory and file structure. List all necessary files (HTML, CSS, JavaScript, images, etc.).
3.  Think through styling and UI/UX: Describe the desired look and feel, color palette, typography, and any key UI components. Consider responsiveness.
4.  Think through images and media: Identify the types of images or media needed and suggest placeholders or sources if applicable.
5.  Think through formatting and content: Outline the content required for each page or component.
6.  Think through frameworks and libraries: Recommend appropriate technologies (e.g., Tailwind CSS was mentioned in context, stick to basic HTML/CSS/JS if not specified, but plan for it if requested). If using libraries, specify how they should be included (CDN, local).
7.  Think through caveats and best practices: Mention any potential challenges, limitations, or important development practices (like accessibility, SEO basics for web).
8.  Output *only* the detailed plan in a clear, structured format (e.g., using markdown). Do not include any conversational text before or after the plan itself. Ensure the plan is detailed enough for another agent to break it down into specific file-generation tasks.
"""
    logger.info(f"Running PlannerAgent for request: '{user_request[:100]}...'")
    try:
        # Use the agent's underlying model call method
        # Note: BaseAgent doesn't directly have .run(), we use the model
        # If PlannerAgent inherited BaseAgent, we could use self._stream_and_aggregate
        # Assuming agent_instance provides access to the model or a run method
        raw_plan_response = await agent_instance.model.call_stream_and_aggregate(
             messages=[{"role": "user", "content": planner_prompt}],
             max_tokens=max_tokens,
             temperature=DEFAULT_TEMPERATURE
             # agent_instance might need a wrapper .run() like BasicAgent
             # For simplicity, assume agent_instance has a model reference and we call directly
             # or it's a BasicAgent instance as passed by constructor in agent_definitions
             # Let's assume it's a BasicAgent instance passed in.
             # raw_plan_response = await agent_instance.run(planner_prompt, max_tokens=max_tokens, temperature=DEFAULT_TEMPERATURE)
        )

        plan = strip_think_tags(raw_plan_response)

        if not plan or plan.startswith("ERROR:"):
             logger.error(f"Planner Agent failed or returned an error: {plan}")
             return {"plan": f"Error: Planner Agent failed. Details: {plan}"}

        logger.info("Planner Agent finished successfully.")
        # Save plan artifact (optional here, could be done by executor if needed)
        # save_file("plan.md", f"User Request:\n{user_request}\n\n---\n\nGenerated Plan:\n{plan}", base_dir="build_artifacts") # Requires run_id context to save properly per run

        return {"plan": plan}
    except Exception as e:
        logger.error(f"PlannerAgent execution encountered an error: {e}", exc_info=True)
        return {"plan": f"Error during PlannerAgent execution: {e}"}

# Helper specific to VLLMModel used in flow_executor's get_model()
# Add a stream aggregation helper if not using BasicAgent directly
async def _stream_and_aggregate_helper(model, messages, **kwargs):
    full_response = ""
    async for chunk in model.call_stream(messages, **kwargs):
        full_response += chunk
    return full_response

# Add the helper to VLLMModel if it doesn't exist or use BasicAgent
VLLMModel.call_stream_and_aggregate = _stream_and_aggregate_helper
</file>

<file path="builder/backend/agents/utils.py">
# builder/backend/utils.py
import logging
import re
from typing import Optional, Dict, Any # Added for find_logical_name

logger = logging.getLogger(__name__) # Use utils logger

def strip_think_tags(text: str) -> str:
    """Removes content up to and including the first </think> tag if present."""
    if not isinstance(text, str): # Ensure input is a string
        return text # Return non-strings as-is

    think_end_tag = "</think>"
    tag_pos = text.find(think_end_tag)
    if tag_pos != -1:
        # Removed debug logging for cleaner output, can be added back if needed
        # logger.debug("Found </think> tag, stripping preceding content.")
        content_after = text[tag_pos + len(think_end_tag):].strip()
        return content_after
    else:
        # logger.debug("No </think> tag found, using full response.")
        return text.strip() # Return original text (stripped) if tag not found

# Optional: Move find_logical_name_for_handle here too, as it's also utility-like
# and helps keep flow_executor focused on execution.
def find_logical_name_for_handle(definition: Dict, handle_id: str, io_type: str) -> Optional[str]:
    """Finds the logical input/output name corresponding to a handle ID."""
    if not definition or io_type not in definition:
        return None
    io_map = definition[io_type] # Either 'inputs' or 'outputs' dict
    for logical_name, details in io_map.items():
        # Ensure details is a dictionary before accessing .get()
        if isinstance(details, dict) and details.get('handle_id') == handle_id:
            return logical_name
    # Fallback: if handle_id itself is a key (less likely with new structure)
    if handle_id in io_map:
        # logger.debug(f"Falling back to using handle_id '{handle_id}' as logical name for {io_type}.")
        return handle_id
    return None
</file>

<file path="builder/backend/context.txt">
The user is interested in Python programming best practices, especially regarding code cleanliness, performance, and concurrency. They are working with potentially large codebases and interacting with external APIs.
</file>

<file path="builder/backend/longtext.txt">
Python is dynamically typed, which offers flexibility but can lead to runtime errors if not carefully managed. This means variables can change type during execution. Static analysis tools like MyPy help mitigate this by adding optional type hints (PEP 484) and checking them before runtime, catching potential type errors early in the development cycle. This improves code reliability and maintainability, especially in larger projects.

Another key aspect is its extensive standard library, often called "batteries included". It covers areas from web protocols (HTTP, email) to GUI development (Tkinter), data processing (CSV, JSON, XML), operating system interfaces, and more. This reduces the need for external packages for common tasks.

The Global Interpreter Lock (GIL) in CPython, the most common Python implementation, is a mutex that protects access to Python objects, preventing multiple threads from executing Python bytecode simultaneously within a single process. While it simplifies memory management (making C extensions easier to write), it can limit the performance of CPU-bound multithreaded programs on multi-core processors, as only one thread runs Python code at a time. However, it doesn't significantly impact I/O-bound tasks, where threads spend most of their time waiting for external operations (like network requests or disk reads).

Asynchronous programming with the `asyncio` library provides concurrency for I/O-bound tasks without needing multiple OS threads. It uses an event loop and `async`/`await` syntax to manage coroutines, allowing the program to switch between tasks when one is waiting for I/O, leading to efficient handling of many concurrent connections or operations. This is particularly useful for web servers, network clients, and applications involving significant waiting time. Libraries like `aiohttp` build upon asyncio for asynchronous web development.
</file>

<file path="builder/frontend/.gitignore">
# Logs
logs
*.log
npm-debug.log*
yarn-debug.log*
yarn-error.log*
pnpm-debug.log*
lerna-debug.log*

node_modules
dist
dist-ssr
*.local

# Editor directories and files
.vscode/*
!.vscode/extensions.json
.idea
.DS_Store
*.suo
*.ntvs*
*.njsproj
*.sln
*.sw?
</file>

<file path="builder/frontend/components.json">
{
  "$schema": "https://ui.shadcn.com/schema.json",
  "style": "new-york",
  "rsc": false,
  "tsx": false,
  "tailwind": {
    "config": "",
    "css": "src/index.css",
    "baseColor": "neutral",
    "cssVariables": true,
    "prefix": ""
  },
  "aliases": {
    "components": "@/components",
    "utils": "@/lib/utils",
    "ui": "@/components/ui",
    "lib": "@/lib",
    "hooks": "@/hooks"
  },
  "iconLibrary": "lucide"
}
</file>

<file path="builder/frontend/eslint.config.js">
import js from '@eslint/js'
import globals from 'globals'
import reactHooks from 'eslint-plugin-react-hooks'
import reactRefresh from 'eslint-plugin-react-refresh'

export default [
  { ignores: ['dist'] },
  {
    files: ['**/*.{js,jsx}'],
    languageOptions: {
      ecmaVersion: 2020,
      globals: globals.browser,
      parserOptions: {
        ecmaVersion: 'latest',
        ecmaFeatures: { jsx: true },
        sourceType: 'module',
      },
    },
    plugins: {
      'react-hooks': reactHooks,
      'react-refresh': reactRefresh,
    },
    rules: {
      ...js.configs.recommended.rules,
      ...reactHooks.configs.recommended.rules,
      'no-unused-vars': ['error', { varsIgnorePattern: '^[A-Z_]' }],
      'react-refresh/only-export-components': [
        'warn',
        { allowConstantExport: true },
      ],
    },
  },
]
</file>

<file path="builder/frontend/jsconfig.json">
{
    "compilerOptions": {
      "baseUrl": ".",
      "paths": {
        "@/*": ["./src/*"]
      }
    }
  }
</file>

<file path="builder/frontend/public/Tesslate.svg">
<?xml version="1.0" encoding="UTF-8"?><svg id="b" xmlns="http://www.w3.org/2000/svg" width="658.14" height="509.11" viewBox="0 0 658.14 509.11"><g id="c"><path d="m328.92.91h311.11c13.77,0,22.48,14.78,15.85,26.84-24.58,44.7-39.62,93.66-71.79,130.68-6.79,7.82-15.04,6.05-22.83,6.11-51.14.38-102.29.12-153.43.26-10.68.03-19.96.63-26.45-11.65-21.76-41.12-44.32-81.83-68.28-125.37-6.64-12.06,2.07-26.86,15.84-26.86Z" fill="#f7f7f6" stroke-width="0"/><path d="m18.11,163.4c-13.77,0-22.48-14.79-15.84-26.86C25.46,94.43,47.81,54.04,69.53,13.32,75.65,1.85,83.89-.09,95.4,0c52.13.4,104.27.27,156.41.1,9.83-.03,18.26.13,23.97,10.76,22.29,41.53,45.02,82.83,68.54,125.71,6.62,12.06-2.1,26.83-15.86,26.83H18.11Z" fill="#f8f7f7" stroke-width="0"/><path d="m344.41,217.54c-52.18,94.98-102.79,187.1-155.03,282.19-6.86,12.48-24.82,12.51-31.73.06-24.42-44.05-47.7-86.03-70.99-128-4.66-8.4,1.77-14.26,5.08-20.37,26.2-48.43,52.89-96.61,79.12-145.03,5.41-9.99,11.2-16.25,24.27-16.06,43.6.64,87.21.45,133.35.36,13.78-.03,22.56,14.77,15.92,26.85Z" fill="#fafaf9" stroke-width="0"/></g></svg>
</file>

<file path="builder/frontend/README.md">
# React + Vite

This template provides a minimal setup to get React working in Vite with HMR and some ESLint rules.

Currently, two official plugins are available:

- [@vitejs/plugin-react](https://github.com/vitejs/vite-plugin-react/blob/main/packages/plugin-react) uses [Babel](https://babeljs.io/) for Fast Refresh
- [@vitejs/plugin-react-swc](https://github.com/vitejs/vite-plugin-react/blob/main/packages/plugin-react-swc) uses [SWC](https://swc.rs/) for Fast Refresh

## Expanding the ESLint configuration

If you are developing a production application, we recommend using TypeScript with type-aware lint rules enabled. Check out the [TS template](https://github.com/vitejs/vite/tree/main/packages/create-vite/template-react-ts) for information on how to integrate TypeScript and [`typescript-eslint`](https://typescript-eslint.io) in your project.
</file>

<file path="builder/frontend/src/assets/react.svg">
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="iconify iconify--logos" width="35.93" height="32" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 228"><path fill="#00D8FF" d="M210.483 73.824a171.49 171.49 0 0 0-8.24-2.597c.465-1.9.893-3.777 1.273-5.621c6.238-30.281 2.16-54.676-11.769-62.708c-13.355-7.7-35.196.329-57.254 19.526a171.23 171.23 0 0 0-6.375 5.848a155.866 155.866 0 0 0-4.241-3.917C100.759 3.829 77.587-4.822 63.673 3.233C50.33 10.957 46.379 33.89 51.995 62.588a170.974 170.974 0 0 0 1.892 8.48c-3.28.932-6.445 1.924-9.474 2.98C17.309 83.498 0 98.307 0 113.668c0 15.865 18.582 31.778 46.812 41.427a145.52 145.52 0 0 0 6.921 2.165a167.467 167.467 0 0 0-2.01 9.138c-5.354 28.2-1.173 50.591 12.134 58.266c13.744 7.926 36.812-.22 59.273-19.855a145.567 145.567 0 0 0 5.342-4.923a168.064 168.064 0 0 0 6.92 6.314c21.758 18.722 43.246 26.282 56.54 18.586c13.731-7.949 18.194-32.003 12.4-61.268a145.016 145.016 0 0 0-1.535-6.842c1.62-.48 3.21-.974 4.76-1.488c29.348-9.723 48.443-25.443 48.443-41.52c0-15.417-17.868-30.326-45.517-39.844Zm-6.365 70.984c-1.4.463-2.836.91-4.3 1.345c-3.24-10.257-7.612-21.163-12.963-32.432c5.106-11 9.31-21.767 12.459-31.957c2.619.758 5.16 1.557 7.61 2.4c23.69 8.156 38.14 20.213 38.14 29.504c0 9.896-15.606 22.743-40.946 31.14Zm-10.514 20.834c2.562 12.94 2.927 24.64 1.23 33.787c-1.524 8.219-4.59 13.698-8.382 15.893c-8.067 4.67-25.32-1.4-43.927-17.412a156.726 156.726 0 0 1-6.437-5.87c7.214-7.889 14.423-17.06 21.459-27.246c12.376-1.098 24.068-2.894 34.671-5.345a134.17 134.17 0 0 1 1.386 6.193ZM87.276 214.515c-7.882 2.783-14.16 2.863-17.955.675c-8.075-4.657-11.432-22.636-6.853-46.752a156.923 156.923 0 0 1 1.869-8.499c10.486 2.32 22.093 3.988 34.498 4.994c7.084 9.967 14.501 19.128 21.976 27.15a134.668 134.668 0 0 1-4.877 4.492c-9.933 8.682-19.886 14.842-28.658 17.94ZM50.35 144.747c-12.483-4.267-22.792-9.812-29.858-15.863c-6.35-5.437-9.555-10.836-9.555-15.216c0-9.322 13.897-21.212 37.076-29.293c2.813-.98 5.757-1.905 8.812-2.773c3.204 10.42 7.406 21.315 12.477 32.332c-5.137 11.18-9.399 22.249-12.634 32.792a134.718 134.718 0 0 1-6.318-1.979Zm12.378-84.26c-4.811-24.587-1.616-43.134 6.425-47.789c8.564-4.958 27.502 2.111 47.463 19.835a144.318 144.318 0 0 1 3.841 3.545c-7.438 7.987-14.787 17.08-21.808 26.988c-12.04 1.116-23.565 2.908-34.161 5.309a160.342 160.342 0 0 1-1.76-7.887Zm110.427 27.268a347.8 347.8 0 0 0-7.785-12.803c8.168 1.033 15.994 2.404 23.343 4.08c-2.206 7.072-4.956 14.465-8.193 22.045a381.151 381.151 0 0 0-7.365-13.322Zm-45.032-43.861c5.044 5.465 10.096 11.566 15.065 18.186a322.04 322.04 0 0 0-30.257-.006c4.974-6.559 10.069-12.652 15.192-18.18ZM82.802 87.83a323.167 323.167 0 0 0-7.227 13.238c-3.184-7.553-5.909-14.98-8.134-22.152c7.304-1.634 15.093-2.97 23.209-3.984a321.524 321.524 0 0 0-7.848 12.897Zm8.081 65.352c-8.385-.936-16.291-2.203-23.593-3.793c2.26-7.3 5.045-14.885 8.298-22.6a321.187 321.187 0 0 0 7.257 13.246c2.594 4.48 5.28 8.868 8.038 13.147Zm37.542 31.03c-5.184-5.592-10.354-11.779-15.403-18.433c4.902.192 9.899.29 14.978.29c5.218 0 10.376-.117 15.453-.343c-4.985 6.774-10.018 12.97-15.028 18.486Zm52.198-57.817c3.422 7.8 6.306 15.345 8.596 22.52c-7.422 1.694-15.436 3.058-23.88 4.071a382.417 382.417 0 0 0 7.859-13.026a347.403 347.403 0 0 0 7.425-13.565Zm-16.898 8.101a358.557 358.557 0 0 1-12.281 19.815a329.4 329.4 0 0 1-23.444.823c-7.967 0-15.716-.248-23.178-.732a310.202 310.202 0 0 1-12.513-19.846h.001a307.41 307.41 0 0 1-10.923-20.627a310.278 310.278 0 0 1 10.89-20.637l-.001.001a307.318 307.318 0 0 1 12.413-19.761c7.613-.576 15.42-.876 23.31-.876H128c7.926 0 15.743.303 23.354.883a329.357 329.357 0 0 1 12.335 19.695a358.489 358.489 0 0 1 11.036 20.54a329.472 329.472 0 0 1-11 20.722Zm22.56-122.124c8.572 4.944 11.906 24.881 6.52 51.026c-.344 1.668-.73 3.367-1.15 5.09c-10.622-2.452-22.155-4.275-34.23-5.408c-7.034-10.017-14.323-19.124-21.64-27.008a160.789 160.789 0 0 1 5.888-5.4c18.9-16.447 36.564-22.941 44.612-18.3ZM128 90.808c12.625 0 22.86 10.235 22.86 22.86s-10.235 22.86-22.86 22.86s-22.86-10.235-22.86-22.86s10.235-22.86 22.86-22.86Z"></path></svg>
</file>

<file path="builder/frontend/src/components/ui/alert.jsx">
import * as React from "react"
import { cva } from "class-variance-authority";

import { cn } from "@/lib/utils"

const alertVariants = cva(
  "relative w-full rounded-lg border px-4 py-3 text-sm grid has-[>svg]:grid-cols-[calc(var(--spacing)*4)_1fr] grid-cols-[0_1fr] has-[>svg]:gap-x-3 gap-y-0.5 items-start [&>svg]:size-4 [&>svg]:translate-y-0.5 [&>svg]:text-current",
  {
    variants: {
      variant: {
        default: "bg-card text-card-foreground",
        destructive:
          "text-destructive bg-card [&>svg]:text-current *:data-[slot=alert-description]:text-destructive/90",
      },
    },
    defaultVariants: {
      variant: "default",
    },
  }
)

function Alert({
  className,
  variant,
  ...props
}) {
  return (
    (<div
      data-slot="alert"
      role="alert"
      className={cn(alertVariants({ variant }), className)}
      {...props} />)
  );
}

function AlertTitle({
  className,
  ...props
}) {
  return (
    (<div
      data-slot="alert-title"
      className={cn("col-start-2 line-clamp-1 min-h-4 font-medium tracking-tight", className)}
      {...props} />)
  );
}

function AlertDescription({
  className,
  ...props
}) {
  return (
    (<div
      data-slot="alert-description"
      className={cn(
        "text-muted-foreground col-start-2 grid justify-items-start gap-1 text-sm [&_p]:leading-relaxed",
        className
      )}
      {...props} />)
  );
}

export { Alert, AlertTitle, AlertDescription }
</file>

<file path="builder/frontend/src/components/ui/button.jsx">
import * as React from "react"
import { Slot } from "@radix-ui/react-slot"
import { cva } from "class-variance-authority";

import { cn } from "@/lib/utils"

const buttonVariants = cva(
  "inline-flex items-center justify-center gap-2 whitespace-nowrap rounded-md text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&_svg]:pointer-events-none [&_svg:not([class*='size-'])]:size-4 shrink-0 [&_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive",
  {
    variants: {
      variant: {
        default:
          "bg-primary text-primary-foreground shadow-xs hover:bg-primary/90",
        destructive:
          "bg-destructive text-white shadow-xs hover:bg-destructive/90 focus-visible:ring-destructive/20 dark:focus-visible:ring-destructive/40 dark:bg-destructive/60",
        outline:
          "border bg-background shadow-xs hover:bg-accent hover:text-accent-foreground dark:bg-input/30 dark:border-input dark:hover:bg-input/50",
        secondary:
          "bg-secondary text-secondary-foreground shadow-xs hover:bg-secondary/80",
        ghost:
          "hover:bg-accent hover:text-accent-foreground dark:hover:bg-accent/50",
        link: "text-primary underline-offset-4 hover:underline",
      },
      size: {
        default: "h-9 px-4 py-2 has-[>svg]:px-3",
        sm: "h-8 rounded-md gap-1.5 px-3 has-[>svg]:px-2.5",
        lg: "h-10 rounded-md px-6 has-[>svg]:px-4",
        icon: "size-9",
      },
    },
    defaultVariants: {
      variant: "default",
      size: "default",
    },
  }
)

function Button({
  className,
  variant,
  size,
  asChild = false,
  ...props
}) {
  const Comp = asChild ? Slot : "button"

  return (
    (<Comp
      data-slot="button"
      className={cn(buttonVariants({ variant, size, className }))}
      {...props} />)
  );
}

export { Button, buttonVariants }
</file>

<file path="builder/frontend/src/components/ui/card.jsx">
import * as React from "react"

import { cn } from "@/lib/utils"

function Card({
  className,
  ...props
}) {
  return (
    (<div
      data-slot="card"
      className={cn(
        "bg-card text-card-foreground flex flex-col gap-6 rounded-xl border py-6 shadow-sm",
        className
      )}
      {...props} />)
  );
}

function CardHeader({
  className,
  ...props
}) {
  return (
    (<div
      data-slot="card-header"
      className={cn(
        "@container/card-header grid auto-rows-min grid-rows-[auto_auto] items-start gap-1.5 px-6 has-data-[slot=card-action]:grid-cols-[1fr_auto] [.border-b]:pb-6",
        className
      )}
      {...props} />)
  );
}

function CardTitle({
  className,
  ...props
}) {
  return (
    (<div
      data-slot="card-title"
      className={cn("leading-none font-semibold", className)}
      {...props} />)
  );
}

function CardDescription({
  className,
  ...props
}) {
  return (
    (<div
      data-slot="card-description"
      className={cn("text-muted-foreground text-sm", className)}
      {...props} />)
  );
}

function CardAction({
  className,
  ...props
}) {
  return (
    (<div
      data-slot="card-action"
      className={cn(
        "col-start-2 row-span-2 row-start-1 self-start justify-self-end",
        className
      )}
      {...props} />)
  );
}

function CardContent({
  className,
  ...props
}) {
  return (<div data-slot="card-content" className={cn("px-6", className)} {...props} />);
}

function CardFooter({
  className,
  ...props
}) {
  return (
    (<div
      data-slot="card-footer"
      className={cn("flex items-center px-6 [.border-t]:pt-6", className)}
      {...props} />)
  );
}

export {
  Card,
  CardHeader,
  CardFooter,
  CardTitle,
  CardAction,
  CardDescription,
  CardContent,
}
</file>

<file path="builder/frontend/src/components/ui/input.jsx">
import * as React from "react"

import { cn } from "@/lib/utils"

function Input({
  className,
  type,
  ...props
}) {
  return (
    (<input
      type={type}
      data-slot="input"
      className={cn(
        "file:text-foreground placeholder:text-muted-foreground selection:bg-primary selection:text-primary-foreground dark:bg-input/30 border-input flex h-9 w-full min-w-0 rounded-md border bg-transparent px-3 py-1 text-base shadow-xs transition-[color,box-shadow] outline-none file:inline-flex file:h-7 file:border-0 file:bg-transparent file:text-sm file:font-medium disabled:pointer-events-none disabled:cursor-not-allowed disabled:opacity-50 md:text-sm",
        "focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px]",
        "aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive",
        className
      )}
      {...props} />)
  );
}

export { Input }
</file>

<file path="builder/frontend/src/components/ui/label.jsx">
import * as React from "react"
import * as LabelPrimitive from "@radix-ui/react-label"

import { cn } from "@/lib/utils"

function Label({
  className,
  ...props
}) {
  return (
    (<LabelPrimitive.Root
      data-slot="label"
      className={cn(
        "flex items-center gap-2 text-sm leading-none font-medium select-none group-data-[disabled=true]:pointer-events-none group-data-[disabled=true]:opacity-50 peer-disabled:cursor-not-allowed peer-disabled:opacity-50",
        className
      )}
      {...props} />)
  );
}

export { Label }
</file>

<file path="builder/frontend/src/components/ui/scroll-area.jsx">
"use client"

import * as React from "react"
import * as ScrollAreaPrimitive from "@radix-ui/react-scroll-area"

import { cn } from "@/lib/utils"

function ScrollArea({
  className,
  children,
  ...props
}) {
  return (
    (<ScrollAreaPrimitive.Root data-slot="scroll-area" className={cn("relative", className)} {...props}>
      <ScrollAreaPrimitive.Viewport
        data-slot="scroll-area-viewport"
        className="focus-visible:ring-ring/50 size-full rounded-[inherit] transition-[color,box-shadow] outline-none focus-visible:ring-[3px] focus-visible:outline-1">
        {children}
      </ScrollAreaPrimitive.Viewport>
      <ScrollBar />
      <ScrollAreaPrimitive.Corner />
    </ScrollAreaPrimitive.Root>)
  );
}

function ScrollBar({
  className,
  orientation = "vertical",
  ...props
}) {
  return (
    (<ScrollAreaPrimitive.ScrollAreaScrollbar
      data-slot="scroll-area-scrollbar"
      orientation={orientation}
      className={cn(
        "flex touch-none p-px transition-colors select-none",
        orientation === "vertical" &&
          "h-full w-2.5 border-l border-l-transparent",
        orientation === "horizontal" &&
          "h-2.5 flex-col border-t border-t-transparent",
        className
      )}
      {...props}>
      <ScrollAreaPrimitive.ScrollAreaThumb
        data-slot="scroll-area-thumb"
        className="bg-border relative flex-1 rounded-full" />
    </ScrollAreaPrimitive.ScrollAreaScrollbar>)
  );
}

export { ScrollArea, ScrollBar }
</file>

<file path="builder/frontend/src/components/ui/select.jsx">
import * as React from "react"
import * as SelectPrimitive from "@radix-ui/react-select"
import { CheckIcon, ChevronDownIcon, ChevronUpIcon } from "lucide-react"

import { cn } from "@/lib/utils"

function Select({
  ...props
}) {
  return <SelectPrimitive.Root data-slot="select" {...props} />;
}

function SelectGroup({
  ...props
}) {
  return <SelectPrimitive.Group data-slot="select-group" {...props} />;
}

function SelectValue({
  ...props
}) {
  return <SelectPrimitive.Value data-slot="select-value" {...props} />;
}

function SelectTrigger({
  className,
  size = "default",
  children,
  ...props
}) {
  return (
    (<SelectPrimitive.Trigger
      data-slot="select-trigger"
      data-size={size}
      className={cn(
        "border-input data-[placeholder]:text-muted-foreground [&_svg:not([class*='text-'])]:text-muted-foreground focus-visible:border-ring focus-visible:ring-ring/50 aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive dark:bg-input/30 dark:hover:bg-input/50 flex w-fit items-center justify-between gap-2 rounded-md border bg-transparent px-3 py-2 text-sm whitespace-nowrap shadow-xs transition-[color,box-shadow] outline-none focus-visible:ring-[3px] disabled:cursor-not-allowed disabled:opacity-50 data-[size=default]:h-9 data-[size=sm]:h-8 *:data-[slot=select-value]:line-clamp-1 *:data-[slot=select-value]:flex *:data-[slot=select-value]:items-center *:data-[slot=select-value]:gap-2 [&_svg]:pointer-events-none [&_svg]:shrink-0 [&_svg:not([class*='size-'])]:size-4",
        className
      )}
      {...props}>
      {children}
      <SelectPrimitive.Icon asChild>
        <ChevronDownIcon className="size-4 opacity-50" />
      </SelectPrimitive.Icon>
    </SelectPrimitive.Trigger>)
  );
}

function SelectContent({
  className,
  children,
  position = "popper",
  ...props
}) {
  return (
    (<SelectPrimitive.Portal>
      <SelectPrimitive.Content
        data-slot="select-content"
        className={cn(
          "bg-popover text-popover-foreground data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 data-[side=bottom]:slide-in-from-top-2 data-[side=left]:slide-in-from-right-2 data-[side=right]:slide-in-from-left-2 data-[side=top]:slide-in-from-bottom-2 relative z-50 max-h-(--radix-select-content-available-height) min-w-[8rem] origin-(--radix-select-content-transform-origin) overflow-x-hidden overflow-y-auto rounded-md border shadow-md",
          position === "popper" &&
            "data-[side=bottom]:translate-y-1 data-[side=left]:-translate-x-1 data-[side=right]:translate-x-1 data-[side=top]:-translate-y-1",
          className
        )}
        position={position}
        {...props}>
        <SelectScrollUpButton />
        <SelectPrimitive.Viewport
          className={cn("p-1", position === "popper" &&
            "h-[var(--radix-select-trigger-height)] w-full min-w-[var(--radix-select-trigger-width)] scroll-my-1")}>
          {children}
        </SelectPrimitive.Viewport>
        <SelectScrollDownButton />
      </SelectPrimitive.Content>
    </SelectPrimitive.Portal>)
  );
}

function SelectLabel({
  className,
  ...props
}) {
  return (
    (<SelectPrimitive.Label
      data-slot="select-label"
      className={cn("text-muted-foreground px-2 py-1.5 text-xs", className)}
      {...props} />)
  );
}

function SelectItem({
  className,
  children,
  ...props
}) {
  return (
    (<SelectPrimitive.Item
      data-slot="select-item"
      className={cn(
        "focus:bg-accent focus:text-accent-foreground [&_svg:not([class*='text-'])]:text-muted-foreground relative flex w-full cursor-default items-center gap-2 rounded-sm py-1.5 pr-8 pl-2 text-sm outline-hidden select-none data-[disabled]:pointer-events-none data-[disabled]:opacity-50 [&_svg]:pointer-events-none [&_svg]:shrink-0 [&_svg:not([class*='size-'])]:size-4 *:[span]:last:flex *:[span]:last:items-center *:[span]:last:gap-2",
        className
      )}
      {...props}>
      <span className="absolute right-2 flex size-3.5 items-center justify-center">
        <SelectPrimitive.ItemIndicator>
          <CheckIcon className="size-4" />
        </SelectPrimitive.ItemIndicator>
      </span>
      <SelectPrimitive.ItemText>{children}</SelectPrimitive.ItemText>
    </SelectPrimitive.Item>)
  );
}

function SelectSeparator({
  className,
  ...props
}) {
  return (
    (<SelectPrimitive.Separator
      data-slot="select-separator"
      className={cn("bg-border pointer-events-none -mx-1 my-1 h-px", className)}
      {...props} />)
  );
}

function SelectScrollUpButton({
  className,
  ...props
}) {
  return (
    (<SelectPrimitive.ScrollUpButton
      data-slot="select-scroll-up-button"
      className={cn("flex cursor-default items-center justify-center py-1", className)}
      {...props}>
      <ChevronUpIcon className="size-4" />
    </SelectPrimitive.ScrollUpButton>)
  );
}

function SelectScrollDownButton({
  className,
  ...props
}) {
  return (
    (<SelectPrimitive.ScrollDownButton
      data-slot="select-scroll-down-button"
      className={cn("flex cursor-default items-center justify-center py-1", className)}
      {...props}>
      <ChevronDownIcon className="size-4" />
    </SelectPrimitive.ScrollDownButton>)
  );
}

export {
  Select,
  SelectContent,
  SelectGroup,
  SelectItem,
  SelectLabel,
  SelectScrollDownButton,
  SelectScrollUpButton,
  SelectSeparator,
  SelectTrigger,
  SelectValue,
}
</file>

<file path="builder/frontend/src/components/ui/tabs.jsx">
"use client"

import * as React from "react"
import * as TabsPrimitive from "@radix-ui/react-tabs"

import { cn } from "@/lib/utils"

function Tabs({
  className,
  ...props
}) {
  return (
    (<TabsPrimitive.Root
      data-slot="tabs"
      className={cn("flex flex-col gap-2", className)}
      {...props} />)
  );
}

function TabsList({
  className,
  ...props
}) {
  return (
    (<TabsPrimitive.List
      data-slot="tabs-list"
      className={cn(
        "bg-muted text-muted-foreground inline-flex h-9 w-fit items-center justify-center rounded-lg p-[3px]",
        className
      )}
      {...props} />)
  );
}

function TabsTrigger({
  className,
  ...props
}) {
  return (
    (<TabsPrimitive.Trigger
      data-slot="tabs-trigger"
      className={cn(
        "data-[state=active]:bg-background dark:data-[state=active]:text-foreground focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:outline-ring dark:data-[state=active]:border-input dark:data-[state=active]:bg-input/30 text-foreground dark:text-muted-foreground inline-flex h-[calc(100%-1px)] flex-1 items-center justify-center gap-1.5 rounded-md border border-transparent px-2 py-1 text-sm font-medium whitespace-nowrap transition-[color,box-shadow] focus-visible:ring-[3px] focus-visible:outline-1 disabled:pointer-events-none disabled:opacity-50 data-[state=active]:shadow-sm [&_svg]:pointer-events-none [&_svg]:shrink-0 [&_svg:not([class*='size-'])]:size-4",
        className
      )}
      {...props} />)
  );
}

function TabsContent({
  className,
  ...props
}) {
  return (
    (<TabsPrimitive.Content
      data-slot="tabs-content"
      className={cn("flex-1 outline-none", className)}
      {...props} />)
  );
}

export { Tabs, TabsList, TabsTrigger, TabsContent }
</file>

<file path="builder/frontend/src/components/ui/textarea.jsx">
import * as React from "react"

import { cn } from "@/lib/utils"

function Textarea({
  className,
  ...props
}) {
  return (
    (<textarea
      data-slot="textarea"
      className={cn(
        "border-input placeholder:text-muted-foreground focus-visible:border-ring focus-visible:ring-ring/50 aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive dark:bg-input/30 flex field-sizing-content min-h-16 w-full rounded-md border bg-transparent px-3 py-2 text-base shadow-xs transition-[color,box-shadow] outline-none focus-visible:ring-[3px] disabled:cursor-not-allowed disabled:opacity-50 md:text-sm",
        className
      )}
      {...props} />)
  );
}

export { Textarea }
</file>

<file path="builder/frontend/src/lib/utils.js">
import { clsx } from "clsx";
import { twMerge } from "tailwind-merge"

export function cn(...inputs) {
  return twMerge(clsx(inputs));
}
</file>

<file path="builder/frontend/src/nodes/BaseNode.jsx">
import React from 'react';
import { Handle, Position } from 'reactflow';

// A simple wrapper for consistent styling if needed, but can apply directly
const BaseNode = ({ children, title }) => {
  return (
    <div className="p-3 rounded-lg shadow-lg bg-gray-800 border border-gray-600 text-gray-200 w-full">
      {title && <div className="text-center font-bold mb-2 border-b border-gray-600 pb-1">{title}</div>}
      <div className="nodrag"> {/* Prevent dragging from content area */}
        {children}
      </div>
    </div>
  );
};

export default BaseNode;

// Helper for creating text inputs/areas easily within nodes
export const NodeInput = ({ label, type = 'text', value, onChange, placeholder, rows, name }) => (
    <div className="mb-2">
        <label htmlFor={name} className="node-label">{label}</label>
        {type === 'textarea' ? (
             <textarea
                 id={name}
                 name={name}
                 rows={rows || 3}
                 className="node-textarea"
                 value={value}
                 onChange={onChange}
                 placeholder={placeholder}
             />
        ) : (
            <input
                id={name}
                name={name}
                type={type}
                className="node-input"
                value={value || ''} // Ensure value is never null/undefined for input
                onChange={onChange}
                placeholder={placeholder}
             />
        )}
    </div>
);
</file>

<file path="builder/frontend/src/nodes/BasicAgentNode.jsx">
import React, { useCallback } from 'react';
import { Handle, Position } from 'reactflow';
import { useStore } from '../store'; // Assuming store.js is in src/
import { NodeInput } from './BaseNode'; // Use the helper

const BasicAgentNode = ({ id, data }) => {
  const updateNodeData = useStore((state) => state.updateNodeData);

  const handleChange = useCallback((evt) => {
    const { name, value, type } = evt.target;
    const val = type === 'number' ? parseInt(value, 10) : value;
    updateNodeData(id, { [name]: val });
  }, [id, updateNodeData]);

  return (
    <div className="p-3 rounded-lg shadow-lg bg-gray-800 border border-gray-600 text-gray-200 w-64"> {/* Fixed width example */}
      <div className="text-center font-bold mb-2 border-b border-gray-600 pb-1">Basic Agent</div>
      <div className="nodrag p-1">
        <NodeInput
          name="prompt"
          label="Prompt:"
          type="textarea"
          value={data.prompt || ''}
          onChange={handleChange}
          placeholder="Enter agent prompt"
          rows={4}
        />
         <NodeInput
          name="maxTokens"
          label="Max Tokens (Optional):"
          type="number"
          value={data.maxTokens}
          onChange={handleChange}
          placeholder="Default"
        />
      </div>
      {/* Single input handle (can have multiple if needed) */}
      <Handle
        type="target"
        position={Position.Left}
        id="prompt_in"
        style={{ top: '50%', background: '#555' }}
        isConnectable={true} // Control connectability later if needed
      />
      {/* Single output handle */}
      <Handle
        type="source"
        position={Position.Right}
        id="output_out"
         style={{ top: '50%', background: '#555' }}
        isConnectable={true}
      />
    </div>
  );
};

export default BasicAgentNode;
</file>

<file path="builder/frontend/src/nodes/ChainOfAgentsNode.jsx">
import React, { useCallback } from 'react';
import { Handle, Position } from 'reactflow';
import { useStore } from '../store';
import { NodeInput } from './BaseNode';

const ChainOfAgentsNode = ({ id, data }) => {
  const updateNodeData = useStore((state) => state.updateNodeData);

  const handleChange = useCallback((evt) => {
    const { name, value, type } = evt.target;
    // Ensure numbers are stored as numbers
    const val = type === 'number' ? (value === '' ? null : parseInt(value, 10)) : value;
    updateNodeData(id, { [name]: val });
  }, [id, updateNodeData]);

  return (
    <div className="p-3 rounded-lg shadow-lg bg-gray-800 border border-gray-600 text-gray-200 w-80">
      <div className="text-center font-bold mb-2 border-b border-gray-600 pb-1">Chain of Agents System</div>
      <div className="nodrag p-1">
         <NodeInput
          name="initialPrompt"
          label="Initial Prompt:"
          type="textarea"
          value={data.initialPrompt || ''}
          onChange={handleChange}
          placeholder="Enter initial prompt for the chain"
          rows={3}
        />
         <NodeInput
          name="longText"
          label="Long Text Input:"
          type="textarea"
          value={data.longText || ''}
          onChange={handleChange}
          placeholder="Paste the long text here"
          rows={6}
        />
        <div className="grid grid-cols-2 gap-2">
          <NodeInput
            name="chunkSize"
            label="Chunk Size:"
            type="number"
            value={data.chunkSize}
            onChange={handleChange}
            placeholder="e.g., 2000"
          />
           <NodeInput
            name="chunkOverlap"
            label="Chunk Overlap:"
            type="number"
            value={data.chunkOverlap}
            onChange={handleChange}
            placeholder="e.g., 200"
          />
        </div>
        <NodeInput
          name="maxTokens"
          label="Max Tokens (Final) (Optional):"
          type="number"
          value={data.maxTokens}
          onChange={handleChange}
          placeholder="Default"
        />
      </div>
      {/* Input handles */}
      <Handle type="target" position={Position.Left} id="prompt_in" style={{ top: '30%', background: '#555' }} isConnectable={true} />
      <Handle type="target" position={Position.Left} id="text_in" style={{ top: '70%', background: '#555' }} isConnectable={true} />
      {/* Output handle */}
      <Handle type="source" position={Position.Right} id="output_out" style={{ top: '50%', background: '#555' }} isConnectable={true} />
    </div>
  );
};

export default ChainOfAgentsNode;
</file>

<file path="builder/frontend/src/nodes/ContextAgentNode.jsx">
import React, { useCallback } from 'react';
import { Handle, Position } from 'reactflow';
import { useStore } from '../store';
import { NodeInput } from './BaseNode';

const ContextAgentNode = ({ id, data }) => {
  const updateNodeData = useStore((state) => state.updateNodeData);

  const handleChange = useCallback((evt) => {
    const { name, value, type } = evt.target;
    const val = type === 'number' ? parseInt(value, 10) : value;
    updateNodeData(id, { [name]: val });
  }, [id, updateNodeData]);

  return (
    <div className="p-3 rounded-lg shadow-lg bg-gray-800 border border-gray-600 text-gray-200 w-72">
      <div className="text-center font-bold mb-2 border-b border-gray-600 pb-1">Context Agent</div>
      <div className="nodrag p-1">
        <NodeInput
          name="context"
          label="Context:"
          type="textarea"
          value={data.context || ''}
          onChange={handleChange}
          placeholder="Enter context text"
          rows={5}
        />
         <NodeInput
          name="prompt"
          label="Prompt:"
          type="textarea"
          value={data.prompt || ''}
          onChange={handleChange}
          placeholder="Enter prompt"
          rows={3}
        />
         <NodeInput
          name="maxTokens"
          label="Max Tokens (Optional):"
          type="number"
          value={data.maxTokens}
          onChange={handleChange}
          placeholder="Default"
        />
      </div>
      {/* Multiple input handles */}
      <Handle type="target" position={Position.Left} id="context_in" style={{ top: '30%', background: '#555' }} isConnectable={true} />
      <Handle type="target" position={Position.Left} id="prompt_in" style={{ top: '70%', background: '#555' }} isConnectable={true} />
      {/* Single output handle */}
      <Handle type="source" position={Position.Right} id="output_out" style={{ top: '50%', background: '#555' }} isConnectable={true} />
    </div>
  );
};

export default ContextAgentNode;
</file>

<file path="builder/frontend/src/nodes/DistributorAgentNode.jsx">
// builder/frontend/src/nodes/DistributorAgentNode.jsx
import React from 'react';
import { Handle, Position } from 'reactflow';
// No internal state updates needed via UI for this node's core function
// import { useStore } from '../store';
// import { NodeInput } from './BaseNode';

const DistributorAgentNode = ({ id, data }) => {
  // No handleChange needed as data primarily flows via edges

  return (
    <div className="p-3 rounded-lg shadow-lg bg-gray-800 border border-gray-600 text-gray-200 w-72"> {/* Standard width */}
      <div className="text-center font-bold mb-2 border-b border-gray-600 pb-1">Software: Distributor</div>
      <div className="nodrag p-1 text-xs text-center text-gray-400">
        (Takes Plan In, Outputs Memory & File Prompts)
      </div>
      {/* Input handle for the plan */}
      <Handle
        type="target"
        position={Position.Left}
        id="plan_in" // Matches agent_definitions handle_id
        style={{ top: '50%', background: '#555' }}
        isConnectable={true}
      />
      {/* Output handle for memory */}
      <Handle
        type="source"
        position={Position.Right}
        id="memory_out" // Matches agent_definitions handle_id
         style={{ top: '35%', background: '#555' }} // Space handles out
        isConnectable={true}
      />
      {/* Output handle for file prompts JSON */}
      <Handle
        type="source"
        position={Position.Right}
        id="file_prompts_out" // Matches agent_definitions handle_id
         style={{ top: '65%', background: '#555' }} // Space handles out
        isConnectable={true}
      />
    </div>
  );
};

export default DistributorAgentNode;
</file>

<file path="builder/frontend/src/nodes/FileGeneratorAgentNode.jsx">
// builder/frontend/src/nodes/FileGeneratorAgentNode.jsx
import React from 'react';
import { Handle, Position } from 'reactflow';
// No internal state updates needed via UI for this node's core function
// import { useStore } from '../store';
// import { NodeInput } from './BaseNode';

const FileGeneratorAgentNode = ({ id, data }) => {
   // No handleChange needed as data primarily flows via edges

  return (
    <div className="p-3 rounded-lg shadow-lg bg-gray-800 border border-gray-600 text-gray-200 w-72"> {/* Standard width */}
      <div className="text-center font-bold mb-2 border-b border-gray-600 pb-1">Software: File Generator</div>
       <div className="nodrag p-1 text-xs text-center text-gray-400">
        (Generates files based on Memory & Prompts)
      </div>
      {/* Input handle for memory */}
      <Handle
        type="target"
        position={Position.Left}
        id="memory_in" // Matches agent_definitions handle_id
        style={{ top: '35%', background: '#555' }} // Space handles out
        isConnectable={true}
      />
       {/* Input handle for file prompts JSON */}
      <Handle
        type="target"
        position={Position.Left}
        id="file_prompts_in" // Matches agent_definitions handle_id
        style={{ top: '65%', background: '#555' }} // Space handles out
        isConnectable={true}
      />
      {/* Output handle for the summary log */}
      <Handle
        type="source"
        position={Position.Right}
        id="summary_out" // Matches agent_definitions handle_id
         style={{ top: '35%', background: '#555' }} // Space handles out
        isConnectable={true}
      />
       {/* Output handle for the preview link */}
      <Handle
        type="source"
        position={Position.Right}
        id="preview_link_out" // Matches agent_definitions handle_id
         style={{ top: '65%', background: '#555' }} // Space handles out
        isConnectable={true}
      />
       {/* Note: run_id is an internal input from the executor, not a visual handle */}
    </div>
  );
};

export default FileGeneratorAgentNode;
</file>

<file path="builder/frontend/src/nodes/MultiCallSystemNode.jsx">
import React, { useCallback } from 'react';
import { Handle, Position } from 'reactflow';
import { useStore } from '../store';
import { NodeInput } from './BaseNode';

const MultiCallSystemNode = ({ id, data }) => {
  const updateNodeData = useStore((state) => state.updateNodeData);

 const handleChange = useCallback((evt) => {
    const { name, value, type } = evt.target;
    const val = type === 'number' ? (value === '' ? null : parseInt(value, 10)) : value;
    updateNodeData(id, { [name]: val });
  }, [id, updateNodeData]);


  return (
    <div className="p-3 rounded-lg shadow-lg bg-gray-800 border border-gray-600 text-gray-200 w-72">
      <div className="text-center font-bold mb-2 border-b border-gray-600 pb-1">Multi Call System</div>
      <div className="nodrag p-1">
         <NodeInput
          name="prompt"
          label="Prompt (for each call):"
          type="textarea"
          value={data.prompt || ''}
          onChange={handleChange}
          placeholder="Enter the prompt"
          rows={4}
        />
        <div className="grid grid-cols-2 gap-2">
            <NodeInput
                name="numCalls"
                label="# Calls:"
                type="number"
                value={data.numCalls}
                onChange={handleChange}
                placeholder="e.g., 5"
            />
             <NodeInput
                name="maxTokens"
                label="Max Tokens (per call):"
                type="number"
                value={data.maxTokens}
                onChange={handleChange}
                placeholder="e.g., 1000"
            />
        </div>
        <NodeInput
          name="baseFilename"
          label="Base Filename:"
          type="text"
          value={data.baseFilename || ''}
          onChange={handleChange}
          placeholder="e.g., output_call"
        />
        <NodeInput
          name="outputDir"
          label="Output Directory (Backend):"
          type="text"
          value={data.outputDir || ''}
          onChange={handleChange}
          placeholder="Default: example_outputs/..."
        />

      </div>
      {/* Input handle */}
      <Handle type="target" position={Position.Left} id="prompt_in" style={{ top: '50%', background: '#555' }} isConnectable={true} />
      {/* Output handle */}
      <Handle type="source" position={Position.Right} id="output_out" style={{ top: '50%', background: '#555' }} isConnectable={true} />
    </div>
  );
};

export default MultiCallSystemNode;
</file>

<file path="builder/frontend/src/nodes/PlannerAgentNode.jsx">
// builder/frontend/src/nodes/PlannerAgentNode.jsx
import React, { useCallback } from 'react';
import { Handle, Position } from 'reactflow';
import { useStore } from '../store';
import { NodeInput } from './BaseNode'; // Use the helper

const PlannerAgentNode = ({ id, data }) => {
  const updateNodeData = useStore((state) => state.updateNodeData);

  // Needed to update the user_request from the textarea
  const handleChange = useCallback((evt) => {
    const { name, value } = evt.target;
    updateNodeData(id, { [name]: value });
  }, [id, updateNodeData]);

  return (
    <div className="p-3 rounded-lg shadow-lg bg-gray-800 border border-gray-600 text-gray-200 w-80"> {/* Wider node */}
      <div className="text-center font-bold mb-2 border-b border-gray-600 pb-1">Software: Planner</div>
      <div className="nodrag p-1">
        <NodeInput
          name="user_request" // Matches the key in node data and agent_definitions input
          label="User Request:"
          type="textarea"
          value={data.user_request || ''}
          onChange={handleChange}
          placeholder="Enter the software request..."
          rows={6} // Give it some space
        />
      </div>
      {/* Input handle for the request */}
      <Handle
        type="target"
        position={Position.Left}
        id="user_request_in" // Matches agent_definitions handle_id
        style={{ top: '50%', background: '#555' }}
        isConnectable={true}
      />
      {/* Output handle for the plan */}
      <Handle
        type="source"
        position={Position.Right}
        id="plan_out" // Matches agent_definitions handle_id
         style={{ top: '50%', background: '#555' }}
        isConnectable={true}
      />
    </div>
  );
};

export default PlannerAgentNode;
</file>

<file path="builder/frontend/src/nodes/SoftwareBuilderNode.jsx">
import React, { useCallback } from 'react';
import { Handle, Position } from 'reactflow';
import { useStore } from '../store';
import { NodeInput } from './BaseNode';

const SoftwareBuilderNode = ({ id, data }) => {
  const updateNodeData = useStore((state) => state.updateNodeData);

  const handleChange = useCallback((evt) => {
    const { name, value } = evt.target;
    updateNodeData(id, { [name]: value });
  }, [id, updateNodeData]);

  return (
    <div className="p-3 rounded-lg shadow-lg bg-gray-800 border border-gray-600 text-gray-200 w-80">
      <div className="text-center font-bold mb-2 border-b border-gray-600 pb-1">Software Builder System</div>
      <div className="nodrag p-1">
         <NodeInput
          name="userRequest"
          label="User Request:"
          type="textarea"
          value={data.userRequest || ''}
          onChange={handleChange}
          placeholder="Describe the software you want to build..."
          rows={6}
        />
      </div>
      {/* Input handle */}
      <Handle type="target" position={Position.Left} id="request_in" style={{ top: '50%', background: '#555' }} isConnectable={true} />
      {/* Output handle */}
      <Handle type="source" position={Position.Right} id="output_out" style={{ top: '50%', background: '#555' }} isConnectable={true} />
    </div>
  );
};

export default SoftwareBuilderNode;
</file>

<file path="pyproject.toml">
# TAF/pyproject.toml

[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[project]
name = "tframex"
version = "0.1.0"  # Start with an initial version
authors = [
  { name="TesslateAI", email="your.email@example.com" }, # Replace with actual details
]
description = "A framework for building agentic systems with large language models."
readme = "README.md"
requires-python = ">=3.8"
license = { text = "MIT" } # Assuming MIT based on README badge
classifiers = [
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.8",
    "Programming Language :: Python :: 3.9",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent",
    "Intended Audience :: Developers",
    "Topic :: Software Development :: Libraries :: Python Modules",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
]
dependencies = [
    "httpx>=0.25.0",
    # Add any other CORE library dependencies here
    # Dependencies ONLY for examples (like python-dotenv) shouldn't strictly be here
    # but can be for simplicity in this project setup.
]

[project.urls]
Homepage = "https://github.com/TesslateAI/TFrameX"
# Repository = "https://github.com/TesslateAI/TFrameX"
# Bug Tracker = "https://github.com/TesslateAI/TFrameX/issues" # Optional

# This tells setuptools where to find your package code
[tool.setuptools.packages.find]
where = ["."]             # Look for packages in the root directory (where pyproject.toml is)
include = ["tframex*"]    # Find the 'tframex' package and any subpackages
exclude = ["examples*"]   # Exclude the examples directory from the package itself
</file>

<file path="repomix.config.json">
{
  "input": {
    "maxFileSize": 52428800
  },
  "output": {
    "filePath": "repomix-output.xml",
    "style": "xml",
    "parsableStyle": false,
    "fileSummary": true,
    "directoryStructure": true,
    "files": true,
    "removeComments": false,
    "removeEmptyLines": false,
    "compress": false,
    "topFilesLength": 5,
    "showLineNumbers": false,
    "copyToClipboard": false,
    "git": {
      "sortByChanges": true,
      "sortByChangesMaxCommits": 100
    }
  },
  "include": [],
  "ignore": {
    "useGitignore": true,
    "useDefaultPatterns": true,
    "customPatterns": []
  },
  "security": {
    "enableSecurityCheck": true
  },
  "tokenCount": {
    "encoding": "o200k_base"
  }
}
</file>

<file path="tframex/agents/__init__.py">
# TAF/tframex/agents/__init__.py

# Import from the specific files within the 'agents' sub-package
from .agent_logic import BaseAgent
from .agents import BasicAgent, ContextAgent

# Optional: Define __all__
__all__ = ['BaseAgent', 'BasicAgent', 'ContextAgent']
</file>

<file path="tframex/agents/agent_logic.py">
# agent_logic.py
import logging
from abc import ABC, abstractmethod
from tframex.model.model_logic import BaseModel # NEW
from typing import Any, List, Dict
# --- END MODIFICATION ---

logger = logging.getLogger(__name__)

class BaseAgent(ABC):
    """Abstract base class for all agents."""
    def __init__(self, agent_id: str, model: BaseModel):
        """
        Initializes the BaseAgent.

        Args:
            agent_id (str): A unique identifier for the agent instance.
            model (BaseModel): The language model instance the agent will use.
        """
        self.agent_id = agent_id
        self.model = model
        logger.info(f"Agent '{self.agent_id}' initialized using model '{self.model.model_id}'")

    @abstractmethod
    async def run(self, *args, **kwargs) -> Any:
        """
        The main execution method for the agent.
        Must be implemented by subclasses.
        Returns:
            Any: The result of the agent's operation.
        """
        raise NotImplementedError

    async def _stream_and_aggregate(self, prompt: str, **kwargs) -> str:
        """
        Helper to call model's stream (using chat format) and collect the full response.
        """
        # --- MODIFICATION: Convert prompt string to messages list ---
        messages: List[Dict[str, str]] = [{"role": "user", "content": prompt}]
        full_response = ""
        # --- MODIFICATION: Pass messages list to model ---
        async for chunk in self.model.call_stream(messages, **kwargs):
            full_response += chunk
        return full_response
        # --- END MODIFICATION ---

# Potentially add shared utility functions here in the future
# e.g., parse_xml_tags, format_common_prompts etc.
</file>

<file path="tframex/agents/agents.py">
# agents.py
import logging
from tframex.agents.agent_logic import BaseAgent # NEW
from tframex.model.model_logic import BaseModel # NEWBaseModel

logger = logging.getLogger(__name__)

class BasicAgent(BaseAgent):
    """
    A simple agent that takes a prompt, calls the LLM, and returns the full response.
    """
    def __init__(self, agent_id: str, model: BaseModel):
        super().__init__(agent_id, model)

    async def run(self, prompt: str, **kwargs) -> str:
        """
        Sends the prompt to the LLM and returns the aggregated streamed response.

        Args:
            prompt (str): The input prompt.
            **kwargs: Additional parameters for the model call (e.g., max_tokens).

        Returns:
            str: The complete response from the LLM.
        """
        logger.info(f"Agent '{self.agent_id}' running with prompt: '{prompt[:50]}...'")
        full_response = await self._stream_and_aggregate(prompt, **kwargs)
        logger.info(f"Agent '{self.agent_id}' finished.")
        return full_response

class ContextAgent(BaseAgent):
    """
    An agent that combines a given context with the prompt before calling the LLM.
    """
    def __init__(self, agent_id: str, model: BaseModel, context: str):
        """
        Initializes the ContextAgent.

        Args:
            agent_id (str): Unique identifier for the agent.
            model (BaseModel): The language model instance.
            context (str): The predefined context to use.
        """
        super().__init__(agent_id, model)
        self.context = context
        logger.info(f"Agent '{self.agent_id}' initialized with context: '{self.context[:100]}...'")

    async def run(self, prompt: str, **kwargs) -> str:
        """
        Combines context and prompt, sends to LLM, and returns the full response.

        Args:
            prompt (str): The input prompt.
            **kwargs: Additional parameters for the model call (e.g., max_tokens).

        Returns:
            str: The complete response from the LLM.
        """
        combined_prompt = f"Context:\n{self.context}\n\n---\n\nPrompt:\n{prompt}"
        logger.info(f"Agent '{self.agent_id}' running with combined prompt.")
        logger.debug(f"Agent '{self.agent_id}' combined prompt: {combined_prompt}") # Log full prompt if needed
        full_response = await self._stream_and_aggregate(combined_prompt, **kwargs)
        logger.info(f"Agent '{self.agent_id}' finished.")
        return full_response
</file>

<file path="tframex/model/__init__.py">
# TAF/tframex/model/__init__.py

# Import the classes you want to expose directly from the 'model' package
from .model_logic import BaseModel, VLLMModel

# Optional: Define __all__ to control 'from tframex.model import *' behaviour
__all__ = ['BaseModel', 'VLLMModel']
</file>

<file path="tframex/model/model_logic.py">
# model_logic.py
import httpx
import json
import asyncio
import logging
from abc import ABC, abstractmethod
# --- MODIFICATION: Added List for messages type hint ---
from typing import AsyncGenerator, Optional, Dict, Any, List

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class BaseModel(ABC):
    """Abstract base class for language models."""
    def __init__(self, model_id: str):
        self.model_id = model_id
        logger.info(f"Initializing base model structure for ID: {model_id}")

    # --- MODIFICATION: Changed signature and docstring for 'messages' ---
    @abstractmethod
    async def call_stream(self, messages: List[Dict[str, str]], **kwargs) -> AsyncGenerator[str, None]:
        """
        Calls the language model (now expecting chat format) and streams response chunks.
        Must be implemented by subclasses.

        Args:
            messages (List[Dict[str, str]]): A list of message dictionaries,
                                             e.g., [{"role": "user", "content": "Hello"}].
        Yields:
            str: Chunks of the generated text content.
        """
        raise NotImplementedError
        yield "" # Required for async generator typing
    # --- END MODIFICATION ---

    @abstractmethod
    async def close_client(self):
        """Closes any underlying network clients."""
        raise NotImplementedError

class VLLMModel(BaseModel):
    """
    Represents a connection to a VLLM OpenAI-compatible endpoint.
    MODIFIED TO USE CHAT COMPLETIONS ENDPOINT AND FORMAT.
    """
    def __init__(self,
                 model_name: str,
                 api_url: str,
                 api_key: str,
                 default_max_tokens: int = 1024,
                 default_temperature: float = 0.7):
        super().__init__(model_id=f"vllm_{model_name.replace('/', '_')}")
        self.model_name = model_name
        base_url = api_url.replace('/v1', '').rstrip('/')
        # --- MODIFICATION: Changed URL name and target endpoint ---
        self.chat_completions_url = f"{base_url}/v1/chat/completions"
        # --- END MODIFICATION ---
        self.api_key = api_key
        self.default_max_tokens = default_max_tokens
        self.default_temperature = default_temperature
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }
        timeouts = httpx.Timeout(None, connect=100.0)
        self._client = httpx.AsyncClient(headers=self.headers, timeout=timeouts)
        # --- MODIFICATION: Updated log message ---
        logger.info(f"VLLMModel '{self.model_id}' initialized for CHAT endpoint {self.chat_completions_url}")
        # --- END MODIFICATION ---

    # --- MODIFICATION: Method signature now expects 'messages' list ---
    async def call_stream(self, messages: List[Dict[str, str]], max_retries: int = 2, **kwargs) -> AsyncGenerator[str, None]:
        """
        Calls the VLLM CHAT completions endpoint with messages and streams the response.
        Includes basic retry logic for specific network errors.

        Args:
            messages (List[Dict[str, str]]): The conversation history/prompt.
            max_retries (int): Maximum number of retries on specific errors.
            **kwargs: Override default parameters like 'max_tokens', 'temperature'.

        Yields:
            str: Chunks of the generated text content, including tags like <think>.
        """
        payload = {
            "model": self.model_name,
            # --- MODIFICATION: Use 'messages' instead of 'prompt' ---
            "messages": messages,
            "max_tokens": kwargs.get('max_tokens', self.default_max_tokens),
            "temperature": kwargs.get('temperature', self.default_temperature),
            "stream": True,
            **{k: v for k, v in kwargs.items() if k not in ['max_tokens', 'temperature', 'max_retries']}
        }
        # --- END MODIFICATION ---

        last_exception = None
        for attempt in range(max_retries + 1):
            try:
                # --- MODIFICATION: Updated log message and URL variable ---
                logger.debug(f"[{self.model_id}] Attempt {attempt+1}/{max_retries+1}: Sending request to {self.chat_completions_url}")
                async with self._client.stream("POST", self.chat_completions_url, json=payload) as response:
                # --- END MODIFICATION ---
                    if response.status_code == 429: # Specific handling for rate limits
                         retry_after = int(response.headers.get("Retry-After", "5")) # Default to 5s
                         logger.warning(f"[{self.model_id}] Rate limit hit (429). Retrying after {retry_after} seconds.")
                         await asyncio.sleep(retry_after)
                         last_exception = httpx.HTTPStatusError("Rate limit hit", request=response.request, response=response)
                         continue # Go to next attempt

                    if response.status_code != 200:
                        error_content = await response.aread()
                        error_msg = f"API Error: Status {response.status_code}, Response: {error_content.decode()}"
                        logger.error(f"[{self.model_id}] {error_msg}")
                        yield f"ERROR: {error_msg}" # Yield non-retryable API errors
                        return # Stop processing this request

                    # --- Stream Processing ---
                    async for line in response.aiter_lines():
                        line = line.strip()
                        if line.startswith("data:"):
                            data_content = line[len("data:"):].strip()
                            if data_content == "[DONE]":
                                logger.debug(f"[{self.model_id}] DONE signal received.")
                                return # Successful completion of the stream

                            try:
                                json_data = json.loads(data_content)
                                text_chunk = "" # Initialize empty
                                # --- MODIFICATION: Parse chat completions stream format ---
                                if 'choices' in json_data and len(json_data['choices']) > 0:
                                    choice = json_data['choices'][0]
                                    if 'delta' in choice and 'content' in choice['delta']:
                                         # Check if content is not None before assigning
                                         content = choice['delta']['content']
                                         if content is not None:
                                             text_chunk = content
                                # --- END MODIFICATION ---

                                # Yield chunk even if empty/whitespace
                                yield text_chunk

                            except json.JSONDecodeError:
                                logger.warning(f"[{self.model_id}] Could not decode JSON chunk: {data_content}")
                            except Exception as e:
                                logger.warning(f"[{self.model_id}] Error processing chunk data {data_content}: {e}")

                    logger.debug(f"[{self.model_id}] Stream finished without explicit [DONE] after loop.")
                    return # Successfully finished processing stream

            except httpx.ReadError as e:
                 last_exception = e
                 logger.warning(f"[{self.model_id}] Attempt {attempt+1} failed with ReadError: {e}. Retrying...")
                 await asyncio.sleep(2 ** attempt)
            except httpx.ConnectError as e:
                 last_exception = e
                 logger.warning(f"[{self.model_id}] Attempt {attempt+1} failed with ConnectError: {e}. Retrying...")
                 await asyncio.sleep(2 ** attempt)
            except httpx.PoolTimeout as e:
                 last_exception = e
                 logger.warning(f"[{self.model_id}] Attempt {attempt+1} failed with PoolTimeout: {e}. Retrying...")
                 await asyncio.sleep(2 ** attempt)
            # --- MODIFICATION: Catch RemoteProtocolError specifically for logging/retry ---
            except httpx.RemoteProtocolError as e:
                 last_exception = e
                 logger.warning(f"[{self.model_id}] Attempt {attempt+1} failed with RemoteProtocolError: {e}. Retrying...")
                 await asyncio.sleep(2 ** attempt)
            # --- END MODIFICATION ---
            except Exception as e:
                 logger.error(f"[{self.model_id}] An unexpected error occurred during streaming attempt {attempt+1}: {e}", exc_info=True)
                 yield f"ERROR: Unexpected error - {e}"
                 return

        logger.error(f"[{self.model_id}] All {max_retries + 1} attempts failed. Last error: {last_exception}")
        yield f"ERROR: Request failed after multiple retries - {last_exception}"


    async def close_client(self):
        """Closes the underlying HTTP client."""
        await self._client.aclose()
        logger.info(f"[{self.model_id}] VLLM HTTP client closed.")
</file>

<file path="builder/backend/requirements.txt">
anyio==4.9.0
asgiref==3.8.1
blinker==1.9.0
certifi==2025.4.26
click==8.1.8
colorama==0.4.6
Flask==3.1.0
flask-cors==5.0.1
h11==0.16.0
httpcore==1.0.9
httpx==0.28.1
idna==3.10
itsdangerous==2.2.0
Jinja2==3.1.6
MarkupSafe==3.0.2
python-dotenv==1.1.0
sniffio==1.3.1
-e git+https://github.com/TesslateAI/TFrameX.git@cfda269e2219ec8ef7416ddccadb0dae3708e75b#egg=tframex
Werkzeug==3.1.3
</file>

<file path="builder/frontend/index.html">
<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <link rel="icon" type="image/svg+xml" href="/Tesslate.svg" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <link href="/src/index.css" rel="stylesheet">
    <title>Tesslate Studio</title>
  </head>
  <body>
    <div id="root"></div>
    <script type="module" src="/src/main.jsx"></script>
  </body>
</html>
</file>

<file path="builder/frontend/package.json">
{
  "name": "frontend",
  "private": true,
  "version": "0.0.0",
  "type": "module",
  "scripts": {
    "dev": "vite",
    "build": "vite build",
    "lint": "eslint .",
    "preview": "vite preview"
  },
  "dependencies": {
    "@radix-ui/react-label": "^2.1.4",
    "@radix-ui/react-scroll-area": "^1.2.6",
    "@radix-ui/react-select": "^2.2.2",
    "@radix-ui/react-slot": "^1.2.0",
    "@radix-ui/react-tabs": "^1.1.9",
    "@reactflow/node-resizer": "^2.2.14",
    "@reactflow/node-toolbar": "^1.3.14",
    "@tailwindcss/vite": "^4.1.5",
    "autoprefixer": "^10.4.21",
    "axios": "^1.9.0",
    "class-variance-authority": "^0.7.1",
    "clsx": "^2.1.1",
    "lucide-react": "^0.507.0",
    "postcss": "^8.5.3",
    "react": "^19.0.0",
    "react-dom": "^19.0.0",
    "reactflow": "^11.11.4",
    "tailwind-merge": "^3.2.0",
    "tailwindcss": "^4.1.5",
    "zustand": "^5.0.4"
  },
  "devDependencies": {
    "@eslint/js": "^9.22.0",
    "@tailwindcss/forms": "^0.5.10",
    "@types/react": "^19.0.10",
    "@types/react-dom": "^19.0.4",
    "@vitejs/plugin-react-swc": "^3.8.0",
    "eslint": "^9.22.0",
    "eslint-plugin-react-hooks": "^5.2.0",
    "eslint-plugin-react-refresh": "^0.4.19",
    "globals": "^16.0.0",
    "tw-animate-css": "^1.2.9",
    "vite": "^6.3.1"
  }
}
</file>

<file path="builder/frontend/src/components/ChatbotPanel.jsx">
// src/components/ChatbotPanel.jsx
import React, { useState, useRef, useEffect } from 'react';
import { useStore } from '../store';
import { Input } from '@/components/ui/input';
import { Button } from '@/components/ui/button';
import { ScrollArea } from '@/components/ui/scroll-area'; // Use shadcn ScrollArea
import { Send, Trash2, Loader2 } from 'lucide-react'; // Icons
import { cn } from '@/lib/utils'; // Import cn utility

const ChatbotPanel = () => {
  const [inputMessage, setInputMessage] = useState('');
  const chatHistory = useStore((state) => state.chatHistory);
  const sendChatMessage = useStore((state) => state.sendChatMessage);
  const isChatbotLoading = useStore((state) => state.isChatbotLoading);
  const clearChatHistory = useStore((state) => state.clearChatHistory);
  const messagesEndRef = useRef(null);
  const scrollAreaViewportRef = useRef(null);

  // Scroll to bottom when new messages arrive or loading state changes
  useEffect(() => {
    const viewport = scrollAreaViewportRef.current;
    if (viewport) {
        // Use setTimeout to allow the DOM to update before scrolling
        setTimeout(() => {
             viewport.scrollTo({ top: viewport.scrollHeight, behavior: 'smooth' });
        }, 50); // Short delay
    }
  }, [chatHistory, isChatbotLoading]); // Trigger on history and loading state

  const handleSendMessage = (e) => {
    e.preventDefault();
    if (inputMessage.trim() && !isChatbotLoading) {
      sendChatMessage(inputMessage.trim());
      setInputMessage('');
    }
  };

  return (
    <div className="flex flex-col h-full p-3"> {/* Add padding to the panel */}
      {/* Chat History */}
      <ScrollArea className="flex-grow mb-3 rounded-md border border-border bg-background">
         <div ref={scrollAreaViewportRef} className="h-full p-3 space-y-4"> {/* Add padding inside scroll area */}
            {chatHistory.map((msg, index) => (
              <div key={index} className={cn('flex', msg.sender === 'user' ? 'justify-end' : 'justify-start')}>
                <div
                  className={cn(
                    'max-w-[80%] p-2.5 rounded-lg text-sm whitespace-pre-wrap break-words shadow-sm', // Added shadow
                    msg.sender === 'user'
                      ? 'bg-primary text-primary-foreground'
                      : msg.type === 'error'
                      ? 'bg-destructive text-destructive-foreground'
                      : 'bg-secondary text-secondary-foreground' // Default bot message
                  )}
                >
                  {msg.message}
                </div>
              </div>
            ))}
             {isChatbotLoading && (
                 <div className="flex justify-start">
                     <div className="max-w-[80%] p-2.5 rounded-lg text-sm bg-secondary text-muted-foreground flex items-center">
                        <Loader2 className="mr-2 h-4 w-4 animate-spin" /> Thinking...
                     </div>
                 </div>
             )}
            <div ref={messagesEndRef} /> {/* Invisible element to scroll to */}
         </div>
      </ScrollArea>

      {/* Input Area */}
      <form onSubmit={handleSendMessage} className="flex-shrink-0 flex items-center space-x-2">
        <Input
          type="text"
          value={inputMessage}
          onChange={(e) => setInputMessage(e.target.value)}
          placeholder="Describe the flow..."
          className="flex-grow" // Removed !mt-0 as margin handled by space-x
          disabled={isChatbotLoading}
          aria-label="Chat input"
        />
        <Button
          type="submit"
          size="icon"
          disabled={isChatbotLoading || !inputMessage.trim()}
          title="Send Message"
        >
          <Send className="h-4 w-4" />
          <span className="sr-only">Send</span>
        </Button>
         <Button
            type="button"
            variant="outline"
            size="icon"
            onClick={clearChatHistory}
            disabled={isChatbotLoading || chatHistory.length === 0}
            title="Clear Chat"
        >
            <Trash2 className="h-4 w-4" />
            <span className="sr-only">Clear Chat</span>
        </Button>
      </form>
    </div>
  );
};

export default ChatbotPanel;
</file>

<file path="builder/frontend/src/components/NodesPanel.jsx">
// src/components/NodesPanel.jsx
import React from 'react';
import { Card, CardHeader, CardTitle, CardDescription } from "@/components/ui/card"; // Use shadcn Card
import { Alert, AlertDescription, AlertTitle } from "@/components/ui/alert"; // Use shadcn Alert
import { Loader2, Terminal } from 'lucide-react'; // Icons

const DraggableNode = ({ type, label, description }) => {
  const onDragStart = (event, nodeType, nodeLabel) => {
    const nodeInfo = { type: nodeType, label: nodeLabel };
    event.dataTransfer.setData('application/reactflow', JSON.stringify(nodeInfo));
    event.dataTransfer.effectAllowed = 'move';
  };

  return (
    <Card
      className="mb-3 cursor-grab hover:border-primary transition-colors duration-150 ease-in-out"
      onDragStart={(event) => onDragStart(event, type, label)}
      draggable
      title={description || label} // Tooltip
    >
      <CardHeader className="p-3">
        <CardTitle className="text-sm font-semibold">{label}</CardTitle>
        {description && <CardDescription className="text-xs mt-1">{description}</CardDescription>}
      </CardHeader>
    </Card>
  );
};


const NodesPanel = ({ agentDefs, isLoading, error }) => {
    // Container div with padding is now handled in Sidebar for TabsContent
    return (
        <>
            {isLoading && (
                <div className="flex items-center justify-center text-muted-foreground py-4">
                    <Loader2 className="mr-2 h-4 w-4 animate-spin" /> Loading nodes...
                </div>
            )}
            {error && (
                 <Alert variant="destructive" className="mx-1">
                    <Terminal className="h-4 w-4" />
                    <AlertTitle>Error</AlertTitle>
                    <AlertDescription>{error}</AlertDescription>
                </Alert>
            )}
            {!isLoading && !error && agentDefs.length === 0 && (
                <div className="text-center text-muted-foreground py-4 text-sm">No nodes available.</div>
            )}
            {!isLoading && !error && agentDefs.map((def) => (
                <DraggableNode
                key={def.id}
                type={def.id}
                label={def.name}
                description={def.description}
                />
            ))}
        </>
    );
};

export default NodesPanel;
</file>

<file path="builder/frontend/src/index.css">
@import "tailwindcss";
@import "tw-animate-css";

@custom-variant dark (&:is(.dark *));
/* Basic dark theme styles */
body {
  @apply bg-gray-900 text-gray-200 font-sans;
}

/* Style React Flow */
.react-flow__pane {
  background-color: #1a202c; /* Slightly lighter dark */
}

.react-flow__controls button {
  @apply bg-gray-700 text-gray-200 border-gray-600 hover:bg-gray-600;
}

.react-flow__minimap {
   @apply bg-gray-800 border border-gray-600;
}
.react-flow__minimap-node {
   @apply fill-blue-500 stroke-blue-300;
}

.react-flow__node {
  /* Add more specific node styles in node components or here */
  @apply bg-gray-800 border border-gray-600 rounded-lg shadow-md text-gray-200;
  min-width: 200px; /* Ensure nodes have some minimum width */
}

.react-flow__handle {
    @apply bg-blue-500 border-2 border-gray-900;
    width: 10px;
    height: 10px;
}

.react-flow__handle-connecting {
    @apply bg-green-500;
}

.react-flow__edge-path {
  @apply stroke-blue-400;
   stroke-width: 2;
}

/* Custom Scrollbar for dark theme */
::-webkit-scrollbar {
  width: 8px;
  height: 8px;
}
::-webkit-scrollbar-track {
  @apply bg-gray-700 rounded;
}
::-webkit-scrollbar-thumb {
  @apply bg-gray-500 rounded;
}
::-webkit-scrollbar-thumb:hover {
  @apply bg-gray-400;
}

/* Input/Textarea Styling */
.node-input, .node-textarea {
    @apply block w-full bg-gray-700 border border-gray-600 rounded-md shadow-sm py-1 px-2 text-gray-200 placeholder-gray-400 focus:outline-none focus:ring-blue-500 focus:border-blue-500 sm:text-sm;
    margin-top: 4px; /* Add some space */
}
.node-textarea {
    min-height: 60px; /* Give textareas some default height */
}
.node-label {
    @apply block text-sm font-medium text-gray-300 mb-1;
}

@theme inline {
  --radius-sm: calc(var(--radius) - 4px);
  --radius-md: calc(var(--radius) - 2px);
  --radius-lg: var(--radius);
  --radius-xl: calc(var(--radius) + 4px);
  --color-background: var(--background);
  --color-foreground: var(--foreground);
  --color-card: var(--card);
  --color-card-foreground: var(--card-foreground);
  --color-popover: var(--popover);
  --color-popover-foreground: var(--popover-foreground);
  --color-primary: var(--primary);
  --color-primary-foreground: var(--primary-foreground);
  --color-secondary: var(--secondary);
  --color-secondary-foreground: var(--secondary-foreground);
  --color-muted: var(--muted);
  --color-muted-foreground: var(--muted-foreground);
  --color-accent: var(--accent);
  --color-accent-foreground: var(--accent-foreground);
  --color-destructive: var(--destructive);
  --color-border: var(--border);
  --color-input: var(--input);
  --color-ring: var(--ring);
  --color-chart-1: var(--chart-1);
  --color-chart-2: var(--chart-2);
  --color-chart-3: var(--chart-3);
  --color-chart-4: var(--chart-4);
  --color-chart-5: var(--chart-5);
  --color-sidebar: var(--sidebar);
  --color-sidebar-foreground: var(--sidebar-foreground);
  --color-sidebar-primary: var(--sidebar-primary);
  --color-sidebar-primary-foreground: var(--sidebar-primary-foreground);
  --color-sidebar-accent: var(--sidebar-accent);
  --color-sidebar-accent-foreground: var(--sidebar-accent-foreground);
  --color-sidebar-border: var(--sidebar-border);
  --color-sidebar-ring: var(--sidebar-ring);
}

:root {
  --radius: 0.625rem;
  --background: oklch(1 0 0);
  --foreground: oklch(0.145 0 0);
  --card: oklch(1 0 0);
  --card-foreground: oklch(0.145 0 0);
  --popover: oklch(1 0 0);
  --popover-foreground: oklch(0.145 0 0);
  --primary: oklch(0.205 0 0);
  --primary-foreground: oklch(0.985 0 0);
  --secondary: oklch(0.97 0 0);
  --secondary-foreground: oklch(0.205 0 0);
  --muted: oklch(0.97 0 0);
  --muted-foreground: oklch(0.556 0 0);
  --accent: oklch(0.97 0 0);
  --accent-foreground: oklch(0.205 0 0);
  --destructive: oklch(0.577 0.245 27.325);
  --border: oklch(0.922 0 0);
  --input: oklch(0.922 0 0);
  --ring: oklch(0.708 0 0);
  --chart-1: oklch(0.646 0.222 41.116);
  --chart-2: oklch(0.6 0.118 184.704);
  --chart-3: oklch(0.398 0.07 227.392);
  --chart-4: oklch(0.828 0.189 84.429);
  --chart-5: oklch(0.769 0.188 70.08);
  --sidebar: oklch(0.985 0 0);
  --sidebar-foreground: oklch(0.145 0 0);
  --sidebar-primary: oklch(0.205 0 0);
  --sidebar-primary-foreground: oklch(0.985 0 0);
  --sidebar-accent: oklch(0.97 0 0);
  --sidebar-accent-foreground: oklch(0.205 0 0);
  --sidebar-border: oklch(0.922 0 0);
  --sidebar-ring: oklch(0.708 0 0);
}

.dark {
  --background: oklch(0.145 0 0);
  --foreground: oklch(0.985 0 0);
  --card: oklch(0.205 0 0);
  --card-foreground: oklch(0.985 0 0);
  --popover: oklch(0.205 0 0);
  --popover-foreground: oklch(0.985 0 0);
  --primary: oklch(0.922 0 0);
  --primary-foreground: oklch(0.205 0 0);
  --secondary: oklch(0.269 0 0);
  --secondary-foreground: oklch(0.985 0 0);
  --muted: oklch(0.269 0 0);
  --muted-foreground: oklch(0.708 0 0);
  --accent: oklch(0.269 0 0);
  --accent-foreground: oklch(0.985 0 0);
  --destructive: oklch(0.704 0.191 22.216);
  --border: oklch(1 0 0 / 10%);
  --input: oklch(1 0 0 / 15%);
  --ring: oklch(0.556 0 0);
  --chart-1: oklch(0.488 0.243 264.376);
  --chart-2: oklch(0.696 0.17 162.48);
  --chart-3: oklch(0.769 0.188 70.08);
  --chart-4: oklch(0.627 0.265 303.9);
  --chart-5: oklch(0.645 0.246 16.439);
  --sidebar: oklch(0.205 0 0);
  --sidebar-foreground: oklch(0.985 0 0);
  --sidebar-primary: oklch(0.488 0.243 264.376);
  --sidebar-primary-foreground: oklch(0.985 0 0);
  --sidebar-accent: oklch(0.269 0 0);
  --sidebar-accent-foreground: oklch(0.985 0 0);
  --sidebar-border: oklch(1 0 0 / 10%);
  --sidebar-ring: oklch(0.556 0 0);
}

@layer base {
  * {
    @apply border-border outline-ring/50;
  }
  body {
    @apply bg-background text-foreground;
  }
}
</file>

<file path="builder/frontend/src/main.jsx">
// src/main.jsx
import React from 'react';
import ReactDOM from 'react-dom/client';
import App from './App.jsx';
import './index.css'; // Ensure Tailwind is imported

// Add the 'dark' class to the root element for shadcn dark theme
document.documentElement.classList.add('dark');

ReactDOM.createRoot(document.getElementById('root')).render(
  <React.StrictMode>
    <App />
  </React.StrictMode>,
);
</file>

<file path="builder/frontend/src/store.js">
// src/store.js
import { create } from 'zustand';
import {
  applyNodeChanges,
  applyEdgeChanges,
  // addEdge, // Keep addEdge if needed elsewhere (not used directly in store logic provided)
} from 'reactflow';
import { nanoid } from 'nanoid'; // For unique IDs
import axios from 'axios'; // Import axios

// --- Persistence Helpers ---
// Helper function to load state from localStorage
const loadState = (key) => {
  try {
    const serializedState = localStorage.getItem(key);
    if (serializedState === null) {
      return undefined; // No state saved
    }
    return JSON.parse(serializedState);
  } catch (err) {
    console.error("Could not load state from localStorage", err);
    return undefined;
  }
};

// Helper function to save state to localStorage
const saveState = (key, state) => {
  try {
    const serializedState = JSON.stringify(state);
    localStorage.setItem(key, serializedState);
  } catch (err) {
    console.error("Could not save state to localStorage", err);
  }
};


// --- Initial Project Data ---
// Placeholders for example content - load or define appropriately
const exampleContextContent = `The user is interested in Python programming best practices, especially regarding code cleanliness, performance, and concurrency. They are working with potentially large codebases and interacting with external APIs.`; // Replace with actual context.txt content or fetch mechanism

const exampleLongTextContent = `Python is dynamically typed, which offers flexibility but can lead to runtime errors if not carefully managed. This means variables can change type during execution. Static analysis tools like MyPy help mitigate this by adding optional type hints (PEP 484) and checking them before runtime, catching potential type errors early in the development cycle. This improves code reliability and maintainability, especially in larger projects.

Another key aspect is its extensive standard library, often called "batteries included". It covers areas from web protocols (HTTP, email) to GUI development (Tkinter), data processing (CSV, JSON, XML), operating system interfaces, and more. This reduces the need for external packages for common tasks.

The Global Interpreter Lock (GIL) in CPython, the most common Python implementation, is a mutex that protects access to Python objects, preventing multiple threads from executing Python bytecode simultaneously within a single process. While it simplifies memory management (making C extensions easier to write), it can limit the performance of CPU-bound multithreaded programs on multi-core processors, as only one thread runs Python code at a time. However, it doesn't significantly impact I/O-bound tasks, where threads spend most of their time waiting for external operations (like network requests or disk reads).

Asynchronous programming with the \`asyncio\` library provides concurrency for I/O-bound tasks without needing multiple OS threads. It uses an event loop and \`async\`/\`await\` syntax to manage coroutines, allowing the program to switch between tasks when one is waiting for I/O, leading to efficient handling of many concurrent connections or operations. This is particularly useful for web servers, network clients, and applications involving significant waiting time. Libraries like \`aiohttp\` build upon asyncio for asynchronous web development.`; // Replace with actual longtext.txt content or fetch mechanism


const initialProjects = {
  'example1': {
    name: "Example 1: Basic Agent",
    nodes: [
      { id: 'basic-1', type: 'basicAgent', position: { x: 100, y: 100 }, data: { label: 'Basic Agent', prompt: 'Explain the difference between synchronous and asynchronous programming using a simple analogy.', maxTokens: 300 } },
    ],
    edges: [],
  },
  'example2': {
    name: "Example 2: Context Agent",
    nodes: [
      { id: 'context-1', type: 'contextAgent', position: { x: 100, y: 100 }, data: { label: 'Context Agent', context: exampleContextContent, prompt: 'What are 3 key recommendations for writing clean Python code based on the context?' } },
    ],
    edges: [],
  },
  'example3': {
    name: "Example 3: Chain of Agents",
    nodes: [
      { id: 'chain-1', type: 'chainOfAgents', position: { x: 100, y: 100 }, data: { label: 'Chain Summarizer', initialPrompt: "Based on the provided text, explain the implications of Python's dynamic typing and the GIL.", longText: exampleLongTextContent, chunkSize: 2000, chunkOverlap: 200, maxTokens: 500 } },
    ],
    edges: [],
  },
   'example4': {
    name: "Example 4: Multi Call System",
    nodes: [
      { id: 'multi-1', type: 'multiCallSystem', position: { x: 100, y: 100 }, data: { label: 'Website Generator', prompt: 'Make the best looking website for a html css js tailwind coffee shop landing page.', numCalls: 5, baseFilename: 'website_call', maxTokens: 35000, outputDir: 'example_outputs/ex4_multi_call_outputs' } },
    ],
    edges: [],
  },
  // 'example5': Removed Software Builder as per first file's comments
  'new_project': {
      name: "New Project",
      nodes: [],
      edges: [],
  }
};

const savedProjects = loadState('reactFlowProjects') || initialProjects;
const initialProjectId = loadState('reactFlowCurrentProject') || 'example1'; // Default to first example

export const useStore = create((set, get) => ({
  // === React Flow State ===
  nodes: savedProjects[initialProjectId]?.nodes || [],
  edges: savedProjects[initialProjectId]?.edges || [],
  onNodesChange: (changes) => {
    set((state) => ({ nodes: applyNodeChanges(changes, state.nodes) }));
  },
  onEdgesChange: (changes) => {
    set((state) => ({ edges: applyEdgeChanges(changes, state.edges) }));
  },
  addNode: (newNode) => {
    set((state) => ({ nodes: [...state.nodes, newNode] }));
  },
  setNodes: (nodes) => {
    set({ nodes });
  },
  setEdges: (edges) => {
    set({ edges });
  },
  // Update data within a specific node
  updateNodeData: (nodeId, data) => {
      set((state) => ({
          nodes: state.nodes.map((node) =>
              node.id === nodeId ? { ...node, data: { ...node.data, ...data } } : node
          ),
      }));
  },

  // === Project Management State ===
  projects: savedProjects,
  currentProjectId: initialProjectId,

  saveCurrentProject: () => {
    const { nodes, edges, currentProjectId, projects } = get();
    const currentProject = projects[currentProjectId];
    if (currentProject) {
        const updatedProjects = {
            ...projects,
            [currentProjectId]: { ...currentProject, nodes, edges }
        };
        set({ projects: updatedProjects });
        // saveState('reactFlowProjects', updatedProjects); // Persisted via subscribe
        console.log(`Project '${currentProject.name}' saved.`);
    }
  },

  loadProject: (projectId) => {
    const { projects, saveCurrentProject } = get();
    const projectToLoad = projects[projectId];

    if (projectToLoad) {
      // Save the current state before switching
      saveCurrentProject();
      set({
        nodes: projectToLoad.nodes || [],
        edges: projectToLoad.edges || [],
        currentProjectId: projectId,
        output: "Output will appear here...", // Clear output on load
        chatHistory: [], // Clear chat history on load
      });
      // saveState('reactFlowCurrentProject', projectId); // Persisted via subscribe
      console.log(`Project '${projectToLoad.name}' loaded.`);
    } else {
        console.warn(`Project with ID ${projectId} not found.`);
    }
  },

  createProject: (name) => {
    const { projects, saveCurrentProject } = get();
    // Save the current state before creating
    saveCurrentProject();

    const newProjectId = `project_${nanoid(8)}`;
    const newProject = {
        name: name || `New Project ${Object.keys(projects).length + 1}`,
        nodes: [],
        edges: []
    };
    const updatedProjects = { ...projects, [newProjectId]: newProject };
    set({
        projects: updatedProjects,
        nodes: [], // Clear canvas for new project
        edges: [],
        currentProjectId: newProjectId,
        output: "Output will appear here...", // Clear output
        chatHistory: [], // Clear chat history
    });
    // saveState('reactFlowProjects', updatedProjects); // Persisted via subscribe
    // saveState('reactFlowCurrentProject', newProjectId); // Persisted via subscribe
    console.log(`Project '${newProject.name}' created.`);
  },

  deleteProject: (projectId) => {
      const { projects, currentProjectId, loadProject } = get();
      if (!projects[projectId]) return;
      if (Object.keys(projects).length <= 1) {
          alert("Cannot delete the last project.");
          return; // Don't delete the last project
      }
       if (!confirm(`Are you sure you want to delete project "${projects[projectId].name}"? This cannot be undone.`)) {
           return;
       }

      const updatedProjects = { ...projects };
      delete updatedProjects[projectId];

      // If deleting the current project, load another one (e.g., the first available)
      let nextProjectId = currentProjectId;
      if (currentProjectId === projectId) {
          nextProjectId = Object.keys(updatedProjects)[0];
      }

      set({ projects: updatedProjects });
      // saveState('reactFlowProjects', updatedProjects); // Persisted via subscribe

      // Load the next project AFTER state is updated
       if (currentProjectId === projectId) {
           loadProject(nextProjectId); // loadProject will handle setting currentProjectId and persisting
       } else {
           // If we deleted a non-current project, just save the updated projects list
           saveState('reactFlowProjects', updatedProjects);
       }

      console.log(`Project "${projects[projectId].name}" deleted.`);
  },


  // === Execution State ===
  output: "Output will appear here...",
  isRunning: false,
  runFlow: async () => {
    const { nodes, edges, saveCurrentProject } = get();
     // Save before running
    saveCurrentProject();

    set({ isRunning: true, output: "Executing flow..." });
    console.log("Sending to backend:", { nodes, edges });

    try {
      // Use the correct backend URL (ensure backend runs on port 5001 or change here)
      const response = await axios.post('http://localhost:5001/api/run', { nodes, edges });
      console.log("Received from backend:", response.data);
      set({ output: response.data.output || "Execution finished, but no output received." });
    } catch (error) {
      console.error("Error running flow:", error);
      let errorMessage = "Failed to run flow.";
      if (error.response) {
        // The request was made and the server responded with a status code
        // that falls out of the range of 2xx
        console.error("Backend Error Data:", error.response.data);
        console.error("Backend Error Status:", error.response.status);
        errorMessage = `Backend Error (${error.response.status}): ${error.response.data?.error || 'Unknown error'}`;
      } else if (error.request) {
        // The request was made but no response was received
        console.error("No response received:", error.request);
        errorMessage = "Network Error: Could not connect to the backend. Is it running?";
      } else {
        // Something happened in setting up the request that triggered an Error
        console.error('Error', error.message);
        errorMessage = `Request Error: ${error.message}`;
      }
      set({ output: errorMessage });
    } finally {
      set({ isRunning: false });
    }
  },
  clearOutput: () => {
      set({ output: "" });
  },

  // === Agent Definitions State (NEW from first file) ===
  agentDefinitions: [], // Store fetched definitions
  isDefinitionLoading: false,
  definitionError: null,
  fetchAgentDefinitions: async () => {
      if (get().isDefinitionLoading) return; // Prevent concurrent fetches
      set({ isDefinitionLoading: true, definitionError: null });
      try {
          // Ensure backend is running on 5001 or update URL
          const response = await axios.get('http://localhost:5001/api/agents');
          if (response.data && Array.isArray(response.data)) {
              console.log("Fetched agent definitions:", response.data);
              set({ agentDefinitions: response.data, isDefinitionLoading: false });
          } else {
              throw new Error("Invalid response format from server.");
          }
      } catch (err) {
          console.error("Failed to fetch agent definitions:", err);
          set({
              definitionError: "Could not load nodes. Is the backend running on port 5001?",
              agentDefinitions: [],
              isDefinitionLoading: false
          });
      }
  },

  // === Chatbot State (NEW from first file) ===
  chatHistory: [], // Array of { sender: 'user' | 'bot', message: string, type?: 'error' | 'normal' }
  isChatbotLoading: false,
  addChatMessage: (sender, message, type = 'normal') => {
    set((state) => ({
      // Limit history length if desired
      chatHistory: [...state.chatHistory, { sender, message, type }] //.slice(-50)
    }));
  },
  clearChatHistory: () => set({ chatHistory: [] }),
  sendChatMessage: async (userMessage) => {
      const { nodes, edges, agentDefinitions, addChatMessage } = get();

      if (!userMessage.trim()) return; // Don't send empty messages

      // Add user message immediately
      addChatMessage('user', userMessage);
      set({ isChatbotLoading: true });

      try {
          const backendUrl = 'http://localhost:5001/api/chatbot'; // Ensure correct port
          const payload = {
              message: userMessage,
              nodes: nodes,
              edges: edges,
              // Send definitions the backend expects (check backend endpoint)
              // Assuming it needs the same format as /api/agents returns
              definitions: agentDefinitions
          };
          console.log("Sending to chatbot:", payload);

          const response = await axios.post(backendUrl, payload);
          console.log("Received from chatbot:", response.data);

          const reply = response.data?.reply || "Received no reply from chatbot.";
          const flowUpdate = response.data?.flow; // Expected: { nodes: [], edges: [] } or null/undefined

          addChatMessage('bot', reply); // Add the main reply first

          // Apply flow update ONLY if it's valid
          if (flowUpdate && Array.isArray(flowUpdate.nodes) && Array.isArray(flowUpdate.edges)) {
              console.log("Chatbot applying flow update:", flowUpdate);
              // Here you might want to add more sophisticated validation
              // e.g., check if node types in flowUpdate.nodes exist in agentDefinitions
              set({ nodes: flowUpdate.nodes, edges: flowUpdate.edges });
              // Optionally add a confirmation message after the main reply
              // addChatMessage('bot', 'Flow updated successfully.', 'info');
          } else if (response.data?.hasOwnProperty('flow') && flowUpdate !== null) {
              // If 'flow' key exists but isn't valid (and not explicitly null)
              console.warn("Chatbot returned an invalid flow update structure:", flowUpdate);
              addChatMessage('bot', "(Received an invalid flow structure from the chatbot)", 'error');
          }
           // If flowUpdate is null or undefined, or invalid, just show the main reply.

      } catch (error) {
          console.error("Error sending chat message:", error);
          let errorMessage = "Failed to get response from chatbot.";
          if (error.response) {
              errorMessage = `Chatbot Error (${error.response.status}): ${error.response.data?.error || error.response.data?.reply || 'Unknown backend error'}`;
          } else if (error.request) {
              errorMessage = "Network Error: Could not connect to the chatbot backend.";
          } else {
              errorMessage = `Request Error: ${error.message}`;
          }
          addChatMessage('bot', errorMessage, 'error'); // Add error message to chat
      } finally {
          set({ isChatbotLoading: false });
      }
  },

}));

// --- Persistence Subscription ---
// Persist changes to projects and the current project ID
useStore.subscribe(
  (state) => ({ projects: state.projects, currentProjectId: state.currentProjectId }),
  (currentState) => {
      if (currentState.projects && currentState.currentProjectId) {
          saveState('reactFlowProjects', currentState.projects);
          saveState('reactFlowCurrentProject', currentState.currentProjectId);
      }
  },
  { fireImmediately: false } // Only save on actual changes after initial load
);

// --- Initial Fetch ---
// Fetch agent definitions when the store is initialized
useStore.getState().fetchAgentDefinitions();
</file>

<file path="builder/frontend/vite.config.js">
import path from "path"
import tailwindcss from "@tailwindcss/vite"
import react from "@vitejs/plugin-react-swc"
import { defineConfig } from "vite"

// https://vite.dev/config/
export default defineConfig({
  plugins: [react(), tailwindcss()],
  resolve: {
    alias: {
      "@": path.resolve(__dirname, "./src"),
    },
  },
  server: {
    host: true,
    strictPort: true,
    cors: true,
    hmr: {
      host: "studio.tesslate.com",
    },
    allowedHosts: ["studio.tesslate.com"],
  },
})
</file>

<file path="requirements.txt">
anyio==4.9.0
asgiref==3.8.1
blinker==1.9.0
certifi==2025.4.26
click==8.1.8
colorama==0.4.6
Flask==3.1.0
flask-cors==5.0.1
h11==0.16.0
httpcore==1.0.9
httpx==0.28.1
idna==3.10
itsdangerous==2.2.0
Jinja2==3.1.6
MarkupSafe==3.0.2
python-dotenv==1.1.0
sniffio==1.3.1
-e git+https://github.com/TesslateAI/TFrameX.git@cfda269e2219ec8ef7416ddccadb0dae3708e75b#egg=tframex
Werkzeug==3.1.3
</file>

<file path="tframex/systems/__init__.py">
# TAF/tframex/systems/__init__.py

# Import from the specific files within the 'systems' sub-package
from .systems import ChainOfAgents, MultiCallSystem, FrontendAgentSystem

# Optional: Define __all__
__all__ = ['ChainOfAgents', 'MultiCallSystem', 'FrontendAgentSystem']
</file>

<file path="tframex/systems/systems.py">
# systems.py
import asyncio
import logging
import os
import math
import re # Added
import time # Added
from tframex.model import BaseModel
from tframex.agents import BasicAgent # Using BasicAgent for summarization/final answer
# --- MODIFICATION: Added more specific types ---
from typing import List, Dict, Any, Tuple, Optional
# --- END MODIFICATION ---


logger = logging.getLogger(__name__)

# --- Text Chunking Helper (Existing) ---
def chunk_text(text: str, chunk_size: int, chunk_overlap: int = 50) -> List[str]:
    """Splits text into overlapping chunks."""
    if chunk_overlap >= chunk_size:
        raise ValueError("Overlap must be smaller than chunk size")
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunks.append(text[start:end])
        start += chunk_size - chunk_overlap
        if end >= len(text):
             break
    if len(chunks) > 1 and chunks[-1] == chunks[-2][chunk_overlap:]:
         pass
    final_chunks = [c for c in chunks if c]
    logger.info(f"Chunked text into {len(final_chunks)} chunks (size={chunk_size}, overlap={chunk_overlap})")
    return final_chunks

# --- ChainOfAgents (Existing - No Changes Needed) ---
class ChainOfAgents:
    """
    A system that processes long text by summarizing chunks sequentially.
    """
    def __init__(self, system_id: str, model: BaseModel, chunk_size: int = 1000, chunk_overlap: int = 100):
        self.system_id = system_id
        self.model = model
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.processing_agent = BasicAgent(agent_id=f"{system_id}_processor", model=model)
        logger.info(f"System '{self.system_id}' initialized (Chain of Agents).")

    async def run(self, initial_prompt: str, long_text: str, **kwargs) -> str:
        """Processes the long text based on the initial prompt using a chain of summaries."""
        logger.info(f"System '{self.system_id}' starting run for prompt: '{initial_prompt[:50]}...'")
        chunks = chunk_text(long_text, self.chunk_size, self.chunk_overlap)
        if not chunks:
            logger.warning(f"System '{self.system_id}': No text chunks generated from input.")
            return "Error: Input text was empty or too short to chunk."

        current_summary = ""
        num_chunks = len(chunks)

        for i, chunk in enumerate(chunks):
            chunk_prompt = (
                f"Overall Goal: {initial_prompt}\n\n"
                f"Previous Summary (if any):\n{current_summary}\n\n"
                f"---\n\n"
                f"Current Text Chunk ({i+1}/{num_chunks}):\n{chunk}\n\n"
                f"---\n\n"
                f"Task: Summarize the 'Current Text Chunk' focusing on information relevant to the 'Overall Goal'. "
                f"Integrate relevant details from the 'Previous Summary' if applicable, but keep the summary concise. "
                f"Output *only* the refined summary."
            )
            logger.info(f"System '{self.system_id}': Processing chunk {i+1}/{num_chunks}...")
            current_summary = await self.processing_agent.run(chunk_prompt, **kwargs)
            logger.debug(f"System '{self.system_id}': Intermediate summary after chunk {i+1}: '{current_summary[:100]}...'")

        final_prompt = (
            f"Context (summary derived from the full text):\n{current_summary}\n\n"
            f"---\n\n"
            f"Prompt:\n{initial_prompt}\n\n"
            f"---\n\n"
            f"Task: Using the provided context (summary), answer the prompt accurately and completely."
        )
        logger.info(f"System '{self.system_id}': Generating final answer...")
        final_answer = await self.processing_agent.run(final_prompt, **kwargs)

        logger.info(f"System '{self.system_id}' finished run.")
        return final_answer

# --- MultiCallSystem (Existing - No Changes Needed) ---
class MultiCallSystem:
    """
    A system that makes multiple simultaneous calls to the LLM with the same prompt.
    """
    def __init__(self, system_id: str, model: BaseModel):
        self.system_id = system_id
        self.model = model
        logger.info(f"System '{self.system_id}' initialized (Multi Call).")

    async def _call_and_save_task(self, prompt: str, output_filename: str, **kwargs) -> str:
        """Internal task to call LLM stream (using chat format) and save to a file."""
        full_response = ""
        messages: List[Dict[str, str]] = [{"role": "user", "content": prompt}]
        try:
            with open(output_filename, 'w', encoding='utf-8') as f:
                async for chunk in self.model.call_stream(messages, **kwargs):
                    f.write(chunk)
                    full_response += chunk
                    f.flush()
            logger.info(f"System '{self.system_id}': Saved response to {output_filename}")
            return output_filename
        except Exception as e:
            logger.error(f"System '{self.system_id}': Error saving to {output_filename}: {e}")
            try:
                 with open(output_filename, 'w', encoding='utf-8') as f:
                      f.write(f"ERROR processing/saving response: {e}\n\nPartial response if any:\n{full_response}")
            except Exception:
                 pass
            return f"ERROR: Failed to write to {output_filename}"

    async def run(self, prompt: str, num_calls: int, output_dir: str = "multi_call_outputs", base_filename: str = "output", **kwargs) -> Dict[str, str]:
        """Makes `num_calls` simultaneous requests to the model with the given prompt."""
        logger.info(f"System '{self.system_id}' starting run for {num_calls} simultaneous calls.")
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
            logger.info(f"Created output directory: {output_dir}")

        tasks = []
        output_files = {}

        for i in range(1, num_calls + 1):
            output_filename = os.path.join(output_dir, f"{base_filename}_{i}.txt")
            task_id = f"call_{i}"
            task = self._call_and_save_task(prompt, output_filename, **kwargs)
            tasks.append(task)
            output_files[task_id] = output_filename

        logger.info(f"System '{self.system_id}': Launching {num_calls} concurrent tasks...")
        results = await asyncio.gather(*tasks, return_exceptions=True)

        final_results = {}
        success_count = 0
        error_count = 0
        for i, result in enumerate(results):
            task_id = f"call_{i+1}"
            original_filename = output_files[task_id]
            if isinstance(result, Exception):
                logger.error(f"System '{self.system_id}': Task {task_id} raised an exception: {result}")
                final_results[task_id] = f"ERROR: Task Exception - {result}"
                error_count += 1
                try:
                    with open(original_filename, 'w', encoding='utf-8') as f:
                         f.write(f"ERROR: Task Exception - {result}")
                except Exception:
                    pass
            elif isinstance(result, str) and result.startswith("ERROR:"):
                 logger.error(f"System '{self.system_id}': Task {task_id} failed: {result}")
                 final_results[task_id] = result
                 error_count += 1
            else:
                 final_results[task_id] = result
                 success_count +=1

        logger.info(f"System '{self.system_id}' finished run. Success: {success_count}, Errors: {error_count}")
        return final_results

# --- NEW: FrontendAgentSystem Class ---
class FrontendAgentSystem:
    """
    A system that orchestrates the generation of a multi-page frontend website
    based on a user request, using planning, task distribution, and parallel
    file generation agents.
    """
    def __init__(self,
                 system_id: str,
                 model: BaseModel,
                 artifacts_dir: str = "build_artifacts",
                 website_output_dir: str = "generated_website",
                 max_tokens_plan: int = 4096,
                 max_tokens_file_gen: int = 34000,
                 temperature: float = 0.5):
        """
        Initializes the FrontendAgentSystem.

        Args:
            system_id (str): Identifier for this system instance.
            model (BaseModel): The language model instance to use.
            artifacts_dir (str): Directory to save intermediate build artifacts (plan, etc.).
            website_output_dir (str): Directory to save the final generated website files.
            max_tokens_plan (int): Max tokens for planning/distribution LLM calls.
            max_tokens_file_gen (int): Max tokens for file generation LLM calls.
            temperature (float): Default temperature for LLM calls.
        """
        self.system_id = system_id
        self.model = model
        self.artifacts_dir = artifacts_dir
        self.website_output_dir = website_output_dir
        self.max_tokens_plan = max_tokens_plan
        self.max_tokens_file_gen = max_tokens_file_gen
        self.temperature = temperature
        self._logger = logging.getLogger(f"FrontendAgentSystem.{system_id}") # Specific logger instance

        # Ensure output directories exist
        os.makedirs(self.artifacts_dir, exist_ok=True)
        os.makedirs(self.website_output_dir, exist_ok=True)
        self._logger.info(f"Initialized. Artifacts Dir: '{self.artifacts_dir}', Website Output Dir: '{self.website_output_dir}'")

    # --- Helper Methods as Private Class Methods ---

    def _save_file(self, filename: str, content: str, is_artifact: bool = False) -> bool:
        """Saves content to a file in the appropriate directory (artifact or website)."""
        base_dir = self.artifacts_dir if is_artifact else self.website_output_dir
        # Basic security check
        if os.path.isabs(filename) or ".." in filename:
            self._logger.error(f"Invalid filepath detected (absolute or traversal): {filename}")
            return False

        full_path = os.path.join(base_dir, filename)
        try:
            os.makedirs(os.path.dirname(full_path), exist_ok=True)
            with open(full_path, 'w', encoding='utf-8') as f:
                f.write(content)
            self._logger.info(f"Successfully saved file: {full_path}")
            return True
        except OSError as e:
            self._logger.error(f"Failed to save file {full_path}: {e}")
            return False
        except Exception as e:
            self._logger.error(f"An unexpected error occurred saving file {full_path}: {e}")
            return False

    def _extract_code(self, llm_output: str) -> Optional[str]:
        """Extracts the first code block (```...```) from LLM output."""
        match = re.search(r"```(?:[a-zA-Z0-9]*\n)?(.*?)```", llm_output, re.DOTALL | re.IGNORECASE)
        if match:
            code = match.group(1).strip()
            if code:
                self._logger.debug(f"Extracted code block (length: {len(code)}).")
                return code
            else:
                self._logger.warning("Found code block delimiters but content inside was empty.")
                return None
        else:
            self._logger.warning("No markdown code block found in the LLM output.")
            if llm_output.strip().startswith('<') or llm_output.strip().startswith(('function', 'const', 'let', 'var', 'import', 'public class', '@', '.', '#')):
                 self._logger.warning("No code block found, but output resembles code. Returning full output.")
                 return llm_output.strip()
            return None

    def _parse_task_distribution(self, dist_output: str) -> Tuple[Optional[str], List[Dict[str, str]]]:
        """Parses the Task Distributor output to extract memory and file prompts."""
        memory = None
        prompts = []
        memory_match = re.search(r"<memory>(.*?)</memory>", dist_output, re.DOTALL | re.IGNORECASE)
        if memory_match:
            memory = memory_match.group(1).strip()
            self._logger.info("Extracted memory block.")
        else:
            self._logger.warning("Could not find <memory> block in Task Distributor output.")

        prompt_pattern = re.compile(
            r'<prompt\s+filename="(?P<filename>[^"]+)"(?:\s+url="(?P<url>[^"]+)?"\s*)?>(?P<prompt_content>.*?)</prompt>',
            re.DOTALL | re.IGNORECASE
        )
        for match in prompt_pattern.finditer(dist_output):
            data = match.groupdict()
            filename = data['filename'].strip()
            prompt_content = data['prompt_content'].strip()
            url = data.get('url', '').strip()

            if filename and prompt_content:
                prompts.append({
                    "filename": filename,
                    "url": url if url else filename,
                    "prompt": prompt_content
                })
                self._logger.debug(f"Extracted prompt for file: {filename}")
            else:
                self._logger.warning(f"Found prompt block but filename or content was empty: {match.group(0)}")

        if not prompts:
             self._logger.warning("Could not find any valid <prompt ...> blocks in Task Distributor output.")
        return memory, prompts

    def _strip_think_tags(self, text: str) -> str:
        """Removes content up to and including the first </think> tag if present."""
        think_end_tag = "</think>"
        tag_pos = text.find(think_end_tag)
        if tag_pos != -1:
            self._logger.debug("Found </think> tag, stripping preceding content.")
            return text[tag_pos + len(think_end_tag):].strip()
        else:
            return text

    # --- Main Run Method ---
    async def run(self, user_request: str):
        """
        Orchestrates the frontend generation process for the given user request.
        """
        start_time_build = time.time()
        self._logger.info(f"--- Starting Frontend Build Process for Request: '{user_request}' ---")

        # Create agent instances needed for this run
        # Using separate IDs for clarity within this system run
        orchestration_agent = BasicAgent(agent_id=f"{self.system_id}_orchestrator", model=self.model)
        generation_agent = BasicAgent(agent_id=f"{self.system_id}_generator", model=self.model)

        # === STEP 1: Planner Agent ===
        self._logger.info("--- Step 1: Planning ---")
        plan = None
        planner_prompt = f"""
You are an expert software architect and planner. Your goal is to create a comprehensive plan to build the software requested by the user.

User Request: "{user_request}"

Instructions:
1.  Analyze the user request thoroughly.
2.  Think through the project structure: Define a clear and logical directory and file structure. List all necessary files (HTML, CSS, JavaScript, images, etc.).
3.  Think through styling and UI/UX: Describe the desired look and feel, color palette, typography, and any key UI components. Consider responsiveness.
4.  Think through images and media: Identify the types of images or media needed and suggest placeholders or sources if applicable.
5.  Think through formatting and content: Outline the content required for each page or component.
6.  Think through frameworks and libraries: Recommend appropriate technologies (e.g., Tailwind CSS was mentioned in context, stick to basic HTML/CSS/JS if not specified, but plan for it if requested). If using libraries, specify how they should be included (CDN, local).
7.  Think through caveats and best practices: Mention any potential challenges, limitations, or important development practices (like accessibility, SEO basics for web).
8.  Output *only* the detailed plan in a clear, structured format (e.g., using markdown). Do not include any conversational text before or after the plan itself. Ensure the plan is detailed enough for another agent to break it down into specific file-generation tasks.
"""
        try:
            self._logger.info("Calling Planner Agent...")
            raw_plan_response = await orchestration_agent.run(planner_prompt, max_tokens=self.max_tokens_plan, temperature=self.temperature)
            plan = self._strip_think_tags(raw_plan_response) # Strip think tags
            if not plan or plan.startswith("ERROR:") :
                 self._logger.error(f"Planner Agent failed or returned an error: {plan}")
                 raise ValueError("Planner Agent failed.")
            self._logger.info("Planner Agent finished. Plan received (first 500 chars):\n" + plan[:500] + "...")
            # Save the plan artifact
            self._save_file("plan.md", f"User Request:\n{user_request}\n\n---\n\nGenerated Plan:\n{plan}", is_artifact=True)

        except Exception as e:
            self._logger.error(f"Error during Planning step: {e}")
            # Note: We don't close the client here, caller is responsible
            return # Stop the process

        # === STEP 2: Task Distribution Agent ===
        self._logger.info("--- Step 2: Task Distribution ---")
        memory = None
        file_prompts = []
        distributor_prompt = f"""
You are a task distribution agent. Your input is a software development plan. Your goal is to break down this plan into:
1.  A shared `<memory>` block containing essential context, design guidelines, framework choices, and overall architecture described in the plan that *all* subsequent file-generation agents need to know to ensure consistency.
2.  Individual `<prompt>` blocks, one for *each file* identified in the plan's file structure. Each prompt block must specify the target `filename` and contain a highly detailed and specific prompt instructing another agent on *exactly* what code to generate for that single file, referencing the shared memory/plan as needed.

Development Plan:
--- START PLAN ---
{plan}
--- END PLAN ---

Instructions:
1.  Carefully read the entire Development Plan.
2.  Extract the core principles, design language, chosen frameworks/libraries, color schemes, typography, file structure overview, and any global requirements into a concise `<memory>` block. This memory block should *not* contain specific file contents but rather the shared context.
3.  For *each* file mentioned in the plan's file structure (e.g., index.html, style.css, script.js, about.html, assets/logo.png):
    *   If it's a code file (HTML, CSS, JS, etc.): Create a `<prompt filename="path/to/file.ext">` block. Inside this block, write a very specific prompt detailing exactly what code needs to be in this file. Include details about structure, content (referencing the plan), functionality, required HTML elements, CSS classes (mentioning framework if used), JS functions, links to other files (using relative paths based on the plan's structure), and references to the shared `<memory>` context if necessary. Instruct the agent receiving this prompt to output *only* the raw code for the file, enclosed in appropriate markdown ``` code blocks.
    *   If it's a non-code asset (like an image placeholder path mentioned in the plan): You can optionally create a prompt block instructing to note this placeholder or skip creating a prompt block for it. Focus on generating code files.
4.  Ensure filenames and relative paths in the prompts are consistent with the file structure defined in the plan.
5.  Output *only* the `<memory>` block followed immediately by all the `<prompt>` blocks. Do not include any other conversational text, introductions, or summaries.
"""
        try:
            self._logger.info("Calling Task Distributor Agent...")
            raw_dist_response = await orchestration_agent.run(distributor_prompt, max_tokens=self.max_tokens_plan, temperature=self.temperature)
            dist_output = self._strip_think_tags(raw_dist_response) # Strip think tags
            if not dist_output or dist_output.startswith("ERROR:") :
                 self._logger.error(f"Task Distributor Agent failed or returned an error: {dist_output}")
                 raise ValueError("Task Distributor Agent failed.")

            self._logger.info("Task Distributor Agent finished. Parsing output...")
            memory, file_prompts = self._parse_task_distribution(dist_output)

            if not memory or not file_prompts:
                self._logger.error("Failed to parse memory or file prompts from Task Distributor output. Check the output format.")
                self._save_file("task_distribution_raw_output.txt", f"Distributor Prompt:\n{distributor_prompt}\n\n---\n\nRaw Response:\n{raw_dist_response}", is_artifact=True)
                raise ValueError("Failed to parse Task Distributor output.")

            self._logger.info(f"Successfully parsed memory and {len(file_prompts)} file prompts.")
            # Save parsed data artifact
            parsed_prompts_log = f"<memory>\n{memory}\n</memory>\n\n" + "\n\n".join([f"<prompt filename=\"{p['filename']}\">\n{p['prompt']}\n</prompt>" for p in file_prompts])
            self._save_file("task_distribution_parsed.txt", parsed_prompts_log, is_artifact=True)

        except Exception as e:
            self._logger.error(f"Error during Task Distribution step: {e}")
            return # Stop the process

        # === STEP 3: File Generation Agents (Parallel Execution) ===
        self._logger.info("--- Step 3: Generating Files in Parallel ---")
        if not file_prompts:
            self._logger.warning("No file prompts were generated. Skipping file generation step.")
        else:
            tasks = []
            file_mapping = {}

            self._logger.info(f"Preparing {len(file_prompts)} file generation tasks...")
            for i, task_info in enumerate(file_prompts):
                filename = task_info["filename"]
                specific_prompt = task_info["prompt"]
                generation_prompt = f"""
<memory>
{memory}
</memory>

<prompt filename="{filename}">
{specific_prompt}
</prompt>

Based *only* on the specific prompt for `{filename}` above and the shared `<memory>` context, generate the complete, raw code content for the file `{filename}`.
Output *only* the raw code content for the file, enclosed in the appropriate markdown code block (e.g., ```html ... ```, ```css ... ```, ```javascript ... ```).
Do not include any other text, explanations, introductions, or summaries outside the code block.
"""
                task = generation_agent.run(generation_prompt, max_tokens=self.max_tokens_file_gen, temperature=self.temperature)
                tasks.append(task)
                file_mapping[i] = filename
                self._logger.debug(f"Created generation task for: {filename}")

            self._logger.info(f"Launching {len(tasks)} file generation tasks concurrently...")
            start_time_gen = time.time()
            results = await asyncio.gather(*tasks, return_exceptions=True)
            end_time_gen = time.time()
            self._logger.info(f"File generation tasks completed in {end_time_gen - start_time_gen:.2f} seconds.")

            self._logger.info("Processing generation results and saving files...")
            files_saved = 0
            files_failed = 0
            for i, result in enumerate(results):
                filename = file_mapping[i]
                self._logger.debug(f"Processing result for: {filename}")

                if isinstance(result, Exception):
                    self._logger.error(f"Task for {filename} failed with exception: {result}")
                    files_failed += 1
                    self._save_file(f"{filename}.error.txt", f"Task failed with exception:\n{result}", is_artifact=False) # Save error in website dir
                elif isinstance(result, str) and result.startswith("ERROR:"):
                     self._logger.error(f"Task for {filename} returned an error: {result}")
                     files_failed += 1
                     self._save_file(f"{filename}.error.txt", f"Task returned error:\n{result}", is_artifact=False)
                elif isinstance(result, str):
                    generation_output = self._strip_think_tags(result) # Strip think tags
                    code_content = self._extract_code(generation_output)
                    if code_content:
                        if self._save_file(filename, code_content, is_artifact=False): # Save to website dir
                            files_saved += 1
                        else:
                            files_failed += 1
                    else:
                        self._logger.error(f"Failed to extract code block for {filename}. Saving raw output.")
                        files_failed += 1
                        self._save_file(f"{filename}.raw_output.txt", generation_output, is_artifact=False) # Save raw output to website dir
                else:
                     self._logger.error(f"Task for {filename} returned unexpected result type: {type(result)}")
                     files_failed += 1
                     self._save_file(f"{filename}.error.txt", f"Task returned unexpected result type: {type(result)}\n{result}", is_artifact=False)

            self._logger.info(f"File generation finished. Saved: {files_saved}, Failed/Skipped: {files_failed}")

        end_time_build = time.time()
        self._logger.info(f"--- Frontend build process finished in {end_time_build - start_time_build:.2f} seconds ---")
        self._logger.info(f"Generated website files should be in '{self.website_output_dir}'")
        self._logger.info(f"Build artifacts (plan, etc.) are in '{self.artifacts_dir}'")
</file>

<file path=".gitignore">
# ... existing ignores ...
.env
/example_outputs
/__pycache__

# Python build artifacts
*.egg-info/
dist/
build/
*.pyc
*.pyo
*.pyd
.pytest_cache/
.mypy_cache/
.venv/
venv/
generated/
</file>

<file path="builder/backend/agent_definitions.py">
# builder/backend/agent_definitions.py
import os
import logging
from dotenv import load_dotenv

# Import tframex components needed for instantiation
from tframex.model import VLLMModel
from tframex.agents import BasicAgent, ContextAgent # BasicAgent is needed for FlowBuilder and others
from tframex.systems import ChainOfAgents, MultiCallSystem

# Import the execution functions
from agents.basic import execute_basic_agent
from agents.context import execute_context_agent
from agents.chain import execute_chain_system
from agents.multi_call import execute_multi_call_system
from agents.flow_builder import execute_flow_builder_agent
# --- NEW AGENT IMPORTS ---
from agents.planner_agent import execute_planner_agent
from agents.distributor_agent import execute_distributor_agent
from agents.file_generator_agent import execute_file_generator_agent
# --- END NEW AGENT IMPORTS ---


load_dotenv()
logger = logging.getLogger(__name__)

# --- Configs (remain the same) ---
API_URL = os.getenv("API_URL")
API_KEY = os.getenv("API_KEY")
MODEL_NAME = os.getenv("MODEL_NAME")
DEFAULT_MAX_TOKENS = int(os.getenv("MAX_TOKENS", 32000))
DEFAULT_TEMPERATURE = float(os.getenv("TEMPERATURE", 0.7))
MULTI_CALL_OUTPUT_DIR = os.getenv("MULTI_CALL_OUTPUT_DIR", "example_outputs/ex4_multi_call_outputs")
CHAIN_CHUNK_SIZE = int(os.getenv("CHAIN_CHUNK_SIZE", 2000))
CHAIN_CHUNK_OVERLAP = int(os.getenv("CHAIN_CHUNK_OVERLAP", 200))
# Note: Add specific configs for new agents if needed (e.g., output directories)
# Example: SOFTWARE_OUTPUT_DIR = os.getenv("SOFTWARE_OUTPUT_DIR", "generated_software")

# --- Agent/System Definition Registry ---
AGENT_REGISTRY = {}

def register_agent(agent_id, name, description, agent_type, inputs, outputs, constructor, execute_function):
    """Helper to register agent/system definitions."""
    if agent_id in AGENT_REGISTRY:
        logger.warning(f"Agent/System with ID '{agent_id}' is being re-registered.")

    # Basic validation (Improved version allowing description-only inputs/outputs)
    for logical_name, details in inputs.items():
         if not isinstance(details, dict) or ('handle_id' not in details and 'description' not in details): # Allow description only
             raise ValueError(f"Input '{logical_name}' for agent '{agent_id}' lacks required 'description' key (and potentially 'handle_id').")
         # Add more specific validation if needed (e.g., type, required)
         if not details.get("description"):
             raise ValueError(f"Input '{logical_name}' for agent '{agent_id}' must have a 'description'.")

    for logical_name, details in outputs.items():
        if not isinstance(details, dict) or ('handle_id' not in details and 'description' not in details): # Allow description only
             raise ValueError(f"Output '{logical_name}' for agent '{agent_id}' lacks required 'description' key (and potentially 'handle_id').")
        if not details.get("description"):
             raise ValueError(f"Output '{logical_name}' for agent '{agent_id}' must have a 'description'.")

    AGENT_REGISTRY[agent_id] = {
        "id": agent_id,
        "name": name,
        "description": description,
        "type": agent_type,
        "inputs": inputs,
        "outputs": outputs,
        "constructor": constructor,
        "execute_function": execute_function
    }
    logger.debug(f"Registered agent/system: {agent_id}")

# --- Define Existing Agents/Systems with Explicit Handles ---

# Basic Agent
register_agent(
    agent_id="basicAgent",
    name="Basic Agent",
    description="A simple agent that takes a prompt and returns a response.",
    agent_type="agent",
    inputs={
        "prompt": {
            "handle_id": "prompt_in",
            "description": "The main instruction or question.",
            "required": True,
            "type": "string"
        },
        "max_tokens": {
            "handle_id": "max_tokens_in",
            "description": "(Optional) Max tokens for the response.",
            "required": False,
            "type": "integer"
        }
    },
    outputs={
        "output": {
            "handle_id": "output_out",
            "description": "The generated response text.",
            "type": "string"
        }
    },
    constructor=lambda model: BasicAgent(agent_id="dynamic_basic", model=model),
    execute_function=execute_basic_agent
)

# Context Agent
register_agent(
    agent_id="contextAgent",
    name="Context Agent",
    description="An agent that considers provided context along with the prompt.",
    agent_type="agent",
    inputs={
        "prompt": {
            "handle_id": "prompt_in",
            "description": "The main instruction or question.",
            "required": True,
            "type": "string"
        },
        "context": {
            "handle_id": "context_in",
            "description": "Background text or information.",
            "required": True, # Make context required if passed via edge usually
            "type": "string"
        },
        "max_tokens": {
            "handle_id": "max_tokens_in",
            "description": "(Optional) Max tokens for the response.",
            "required": False,
            "type": "integer"
        }
    },
    outputs={
        "output": {
            "handle_id": "output_out",
            "description": "The generated response text.",
            "type": "string"
        }
    },
    constructor=lambda model: ContextAgent(agent_id="dynamic_context", model=model, context=""), # Default context empty
    execute_function=execute_context_agent
)

# Chain of Agents System
register_agent(
    agent_id="chainOfAgents",
    name="Chain of Agents",
    description="Summarizes or answers questions about long text by processing it in chunks.",
    agent_type="system",
    inputs={
        "initial_prompt": {
            "handle_id": "prompt_in",
            "description": "The prompt guiding the overall task.",
            "required": True,
            "type": "string"
        },
        "long_text": {
            "handle_id": "text_in",
            "description": "The long document to process.",
            "required": True,
            "type": "string"
        },
        "max_tokens": {
            "handle_id": "max_tokens_in",
            "description": "(Optional) Max tokens for the final combined response.",
            "required": False,
            "type": "integer"
        }
        # Note: chunk_size, chunk_overlap are configured via constructor, not input handles
    },
    outputs={
        "output": {
            "handle_id": "output_out",
            "description": "The final combined response.",
            "type": "string"
        }
    },
    constructor=lambda model: ChainOfAgents(
        system_id="dynamic_chain",
        model=model,
        chunk_size=CHAIN_CHUNK_SIZE,
        chunk_overlap=CHAIN_CHUNK_OVERLAP
    ),
    execute_function=execute_chain_system
)

# Multi Call System
register_agent(
    agent_id="multiCallSystem",
    name="Multi Call System",
    description="Runs the same prompt multiple times concurrently, saving outputs.",
    agent_type="system",
    inputs={
        "prompt": {
            "handle_id": "prompt_in",
            "description": "The prompt to run multiple times.",
            "required": True,
            "type": "string"
        },
        "num_calls": {
            "handle_id": "num_calls_in",
            "description": "Number of concurrent calls (default: 3).",
            "required": False,
            "type": "integer"
        },
        "base_filename": {
            "handle_id": "filename_in",
            "description": "Base name for output files (default: multi_output).",
            "required": False,
            "type": "string"
        },
        "max_tokens": {
            "handle_id": "max_tokens_in",
            "description": "(Optional) Max tokens for each individual call's response.",
            "required": False,
            "type": "integer"
        }
        # Note: output_dir is configured via constructor
    },
    outputs={
        "output": { # The summary log is the primary output here
            "handle_id": "output_out",
            "description": "A summary log of the file paths or errors.",
            "type": "string"
        }
    },
    constructor=lambda model: MultiCallSystem(
        system_id="dynamic_multi",
        model=model,
        default_output_dir=MULTI_CALL_OUTPUT_DIR
    ),
    execute_function=execute_multi_call_system
)

# Flow Builder Agent (Internal)
register_agent(
    agent_id="flowBuilderAgent",
    name="Flow Builder Agent",
    description="Internal agent used by the chatbot sidebar to generate/modify flows.",
    agent_type="agent", # It acts like an agent taking instructions
    inputs={
        # These inputs are not visual handles, just logical data for the execution function
        "user_message": {"description": "The natural language request from the user.", "required": True, "type": "string"},
        "available_nodes_context": {"description": "String describing available node types.", "required": True, "type": "string"},
        "current_flow_context": {"description": "JSON string of the current nodes and edges.", "required": True, "type": "string"}
    },
    outputs={
        # This output is not a visual handle, it's the raw result for the API handler
        "raw_llm_output": {"description": "The raw output from the LLM, potentially including <think> tags and JSON.", "type": "string"}
    },
    # Use a BasicAgent instance as the underlying executor for the complex prompt
    constructor=lambda model: BasicAgent(agent_id="dynamic_flowbuilder", model=model),
    execute_function=execute_flow_builder_agent
)


# --- NEW: Software Builder Agents Registration ---

# 1. Planner Agent
register_agent(
    agent_id="plannerAgent",
    name="Software: Planner",
    description="Takes a user request and generates a detailed development plan.",
    agent_type="agent",
    inputs={
        "user_request": {
            "handle_id": "user_request_in",
            "description": "The high-level user request for the software.",
            "required": True,
            "type": "string"
        }
        # Optional max_tokens can be added if needed
    },
    outputs={
        "plan": {
            "handle_id": "plan_out",
            "description": "The detailed development plan (markdown).",
            "type": "string"
        }
    },
    # Use BasicAgent instance as executor, could also be custom class inheriting BaseAgent
    constructor=lambda model: BasicAgent(agent_id="dynamic_planner", model=model),
    execute_function=execute_planner_agent
)

# 2. Distributor Agent
register_agent(
    agent_id="distributorAgent",
    name="Software: Distributor",
    description="Breaks a development plan into shared memory and specific file prompts.",
    agent_type="agent",
    inputs={
        "plan": {
            "handle_id": "plan_in",
            "description": "The development plan generated by the Planner.",
            "required": True,
            "type": "string"
        }
    },
    outputs={
        "memory": {
            "handle_id": "memory_out",
            "description": "Shared context for file generation.",
            "type": "string"
        },
        "file_prompts_json": {
            "handle_id": "file_prompts_out",
            "description": "JSON string containing a list of file generation prompts.",
            "type": "string"
        }
    },
    constructor=lambda model: BasicAgent(agent_id="dynamic_distributor", model=model),
    execute_function=execute_distributor_agent
)

# 3. File Generator Agent
register_agent(
    agent_id="fileGeneratorAgent",
    name="Software: File Generator",
    description="Generates code files based on prompts and memory, saves them.",
    agent_type="agent", # Acts as one step, despite internal concurrency
    inputs={
        "memory": {
            "handle_id": "memory_in",
            "description": "Shared context from the Distributor.",
            "required": True,
            "type": "string"
        },
        "file_prompts_json": {
            "handle_id": "file_prompts_in",
            "description": "JSON string of file prompts from the Distributor.",
            "required": True,
            "type": "string"
        },
        # This input is special, provided by the executor, not an edge usually
        "run_id": {
            "description": "Unique ID for the current run (set by executor).",
            "required": True, # Technically required by the execution function
            "type": "string",
            # No handle_id as it's not meant for visual connection
        }
    },
    outputs={
        "generation_summary": {
            "handle_id": "summary_out",
            "description": "A log summarizing file generation success/failure.",
            "type": "string"
        },
        "preview_link": {
            "handle_id": "preview_link_out",
            "description": "Relative URL path to preview the generated site (e.g., /api/preview/run_xyz/index.html).",
            "type": "string"
        }
    },
    # Although it uses multiple calls internally, the constructor can still be simple
    # The complexity is handled within its execute_function
    constructor=lambda model: BasicAgent(agent_id="dynamic_generator", model=model),
    execute_function=execute_file_generator_agent
)

# --- END NEW AGENT REGISTRATION ---


# --- Functions to Access Definitions ---
def get_definitions_for_frontend():
    """Returns a list of definitions suitable for the frontend node list."""
    frontend_definitions = []
    # Exclude internal agents like flowBuilderAgent from the node list
    excluded_ids = {"flowBuilderAgent"}
    for agent_id, definition in AGENT_REGISTRY.items():
        if agent_id in excluded_ids:
            continue

        # Filter inputs/outputs to only include those with a visual handle_id
        inputs_for_frontend = {
            lname: details.get("description", "")
            for lname, details in definition.get("inputs", {}).items()
            if details.get("handle_id")
        }
        outputs_for_frontend = {
            lname: details.get("description", "")
            for lname, details in definition.get("outputs", {}).items()
            if details.get("handle_id")
        }
        input_handles = {
            lname: details["handle_id"]
            for lname, details in definition.get("inputs", {}).items()
            if details.get("handle_id")
        }
        output_handles = {
            lname: details["handle_id"]
            for lname, details in definition.get("outputs", {}).items()
            if details.get("handle_id")
        }

        frontend_definitions.append({
            "id": definition["id"],
            "name": definition["name"],
            "description": definition.get("description", ""),
            "type": definition.get("type", "unknown"),
            "inputs": inputs_for_frontend,
            "outputs": outputs_for_frontend,
            "input_handles": input_handles,
            "output_handles": output_handles,
        })
    return frontend_definitions

def get_definition(agent_id):
    """Returns the full internal definition for an agent ID."""
    return AGENT_REGISTRY.get(agent_id)

logger.info(f"Agent Registry loaded with {len(AGENT_REGISTRY)} definitions.")
</file>

<file path="builder/backend/agents/flow_builder.py">
# backend/agents/flow_builder.py
import logging
import json
from tframex.agents import BasicAgent # Use BasicAgent for the underlying call

logger = logging.getLogger(__name__)

# --- Helper to create the prompt ---
def create_flow_builder_prompt(user_request: str, available_nodes_str: str, current_flow_str: str) -> str:
    """Creates the detailed prompt for the LLM flow builder agent."""

    # Define the expected JSON output format explicitly in the prompt
    json_format_description = """
    /no_think
Your goal is to understand the user's request and generate an updated flow configuration consisting of nodes and edges.
You MUST output a valid JSON object *after* your thinking process (outside the <think> tags).
The JSON object MUST have the following structure:
{
  "nodes": [
    {
      "id": "unique_node_id_string",
      "type": "node_type_string", // Must be ONLY one of the available node types
      "position": {"x": number, "y": number}, // Approximate position is fine
      "data": { ... } // An object containing node-specific data (like prompt, context, userRequest etc.)
    }
    // ... more node objects
  ],
  "edges": [
    {
      "id": "unique_edge_id_string", // e.g., "reactflow__edge-node1output_out-node2input_in"
      "source": "source_node_id_string",
      "target": "target_node_id_string",
      "sourceHandle": "source_handle_id_string", // e.g., "output_out"
      "targetHandle": "target_handle_id_string" // e.g., "context_in"
      // Optional: "type": "smoothstep", "animated": true
    }
    // ... more edge objects
  ]
}

Example Node data content based on type:
- basicAgent: {"label": "Agent Name", "prompt": "User prompt here", "max_tokens": null}
- contextAgent: {"label": "Agent Name", "prompt": "User prompt", "context": "Context text", "max_tokens": null}
- chainOfAgents: {"label": "System Name", "initialPrompt": "...", "longText": "...", "maxTokens": null, "chunkSize": 2000, "chunkOverlap": 200}
- multiCallSystem: {"label": "System Name", "prompt": "...", "numCalls": 5, "baseFilename": "output", "maxTokens": 1000}

Example JSON for a website:
{
  "nodes": [
    {
      "id": "planner-1", // ID can vary
      "type": "plannerAgent", // Must match agent_definitions.py
      "position": { "x": 100, "y": 150 }, // Example position
      "data": {
        "label": "Plan Software", // Optional label
        "user_request": "" // Planner node needs this field, initially empty
      }
    },
    {
      "id": "distributor-1", // ID can vary
      "type": "distributorAgent", // Must match agent_definitions.py
      "position": { "x": 400, "y": 150 }, // Example position
      "data": {
        "label": "Distribute Tasks" // Optional label
      }
    },
    {
      "id": "generator-1", // ID can vary
      "type": "fileGeneratorAgent", // Must match agent_definitions.py
      "position": { "x": 700, "y": 150 }, // Example position
      "data": {
        "label": "Generate Files" // Optional label
        // Note: run_id is NOT set here, it's added by the executor
      }
    }
  ],
  "edges": [
    {
      "id": "reactflow__edge-planner-1plan_out-distributor-1plan_in", // ID format convention
      "source": "planner-1",
      "target": "distributor-1",
      "sourceHandle": "plan_out", // Matches plannerAgent output handle
      "targetHandle": "plan_in", // Matches distributorAgent input handle
      "type": "smoothstep", // Optional styling
      "animated": true      // Optional styling
    },
    // Edge for Memory output -> input
    {
      "id": "reactflow__edge-distributor-1memory_out-generator-1memory_in",
      "source": "distributor-1",
      "target": "generator-1",
      "sourceHandle": "memory_out", // Matches distributorAgent output handle
      "targetHandle": "memory_in", // Matches fileGeneratorAgent input handle
      "type": "smoothstep",
      "animated": true
    },
    // Edge for File Prompts output -> input
    {
      "id": "reactflow__edge-distributor-1file_prompts_out-generator-1file_prompts_in",
      "source": "distributor-1",
      "target": "generator-1",
      "sourceHandle": "file_prompts_out", // Matches distributorAgent output handle
      "targetHandle": "file_prompts_in", // Matches fileGeneratorAgent input handle
      "type": "smoothstep",
      "animated": true
    }
  ]
}

IMPORTANT:
- Base your response ENTIRELY on the user's request and the provided context (available nodes, current flow).
- If the user asks to modify the flow, output the *complete* new JSON structure for the *entire* flow, not just the changes.
- If the user asks to create a new flow, generate appropriate node IDs and edge IDs.
- Ensure `type` in nodes matches one of the available node types.
- Ensure `sourceHandle` and `targetHandle` in edges correspond logically to the node types involved (use the available node info for guidance).
- Place nodes at reasonable default positions (e.g., incrementing x for sequential nodes).
- Output *only* the JSON object after the </think> tag. Do not include explanations before or after the JSON.
"""

    prompt = f"""
You are an expert assistant helping a user build automation flows using a visual editor.
Your task is to generate or modify a flow based on the user's request.

<CONTEXT>
Available Node Types:
--- AVAILABLE NODES START ---
{available_nodes_str}
--- AVAILABLE NODES END ---

Current Flow State (Nodes and Edges):
--- CURRENT FLOW START ---
{current_flow_str}
--- CURRENT FLOW END ---
</CONTEXT>

<USER_REQUEST>
{user_request}
</USER_REQUEST>

<INSTRUCTIONS>
{json_format_description}
</INSTRUCTIONS>

<TASK>
First, think step-by-step within <think></think> tags about how to fulfill the user request based on the context and instructions. Analyze the request, determine the necessary nodes, their connections (edges), and required data. Consider the existing flow if modifications are requested.
After your thinking process, output the *complete* JSON object representing the desired final flow state (nodes and edges).
</TASK>
"""
    return prompt

# --- Execution Function ---
async def execute_flow_builder_agent(agent_instance: BasicAgent, input_data: dict) -> dict:
    """
    Executes the Flow Builder Agent logic.
    Input data expected: {
        'user_message': str,
        'available_nodes_context': str,
        'current_flow_context': str
    }
    Output data: {'raw_llm_output': str} # Return the raw output including <think> tags
    """
    user_message = input_data.get('user_message')
    available_nodes_context = input_data.get('available_nodes_context', 'No node definitions provided.')
    current_flow_context = input_data.get('current_flow_context', '{"nodes": [], "edges": []}') # Default to empty flow

    if not user_message:
        logger.error("FlowBuilderAgent execution failed: 'user_message' input is missing.")
        # Return raw output indicating error, the API handler will parse/handle this
        return {"raw_llm_output": "<think>Error: User message was empty.</think>"}

    # Construct the detailed prompt
    prompt = create_flow_builder_prompt(user_message, available_nodes_context, current_flow_context)

    logger.info(f"Running FlowBuilderAgent for user message: '{user_message[:100]}...'")
    try:
        # Use the underlying BasicAgent's run method
        # Use a high max_tokens as the JSON + context can be large
        response = await agent_instance.run(prompt=prompt, max_tokens=8000, temperature=0.3) # Lower temp for more predictable JSON
        # Return the raw response directly
        return {"raw_llm_output": response}
    except Exception as e:
        logger.error(f"FlowBuilderAgent execution encountered an error: {e}", exc_info=True)
        # Return raw output indicating error
        return {"raw_llm_output": f"<think>Error during LLM call: {e}</think>"}
</file>

<file path="builder/frontend/src/App.jsx">
import React, { useCallback, useRef } from 'react';
import ReactFlow, {
  ReactFlowProvider,
  Controls,
  Background,
  addEdge,
  MiniMap,
  useReactFlow, // Import useReactFlow for projecting coordinates
} from 'reactflow';
import 'reactflow/dist/style.css';
import { nanoid } from 'nanoid';

import { useStore } from './store';
import Sidebar from './components/Sidebar';
import TopBar from './components/TopBar';
import OutputPanel from './components/OutputPanel';

// Import Custom Nodes (still needed for rendering)
import BasicAgentNode from './nodes/BasicAgentNode';
import ContextAgentNode from './nodes/ContextAgentNode';
import ChainOfAgentsNode from './nodes/ChainOfAgentsNode';
import MultiCallSystemNode from './nodes/MultiCallSystemNode';
// SoftwareBuilderNode is removed
import PlannerAgentNode from './nodes/PlannerAgentNode'; // Create this file
import DistributorAgentNode from './nodes/DistributorAgentNode'; // Create this file
import FileGeneratorAgentNode from './nodes/FileGeneratorAgentNode'; // Create this file

const nodeTypes = {
  basicAgent: BasicAgentNode,
  contextAgent: ContextAgentNode,
  chainOfAgents: ChainOfAgentsNode,
  multiCallSystem: MultiCallSystemNode,
  // Add new software builder nodes
  plannerAgent: PlannerAgentNode,
  distributorAgent: DistributorAgentNode,
  fileGeneratorAgent: FileGeneratorAgentNode,
};
// ----------------------------------------------------

const FlowEditor = () => {
  const reactFlowWrapper = useRef(null);
  const { project } = useReactFlow(); // Get project function

  const nodes = useStore((state) => state.nodes);
  const edges = useStore((state) => state.edges);
  const onNodesChange = useStore((state) => state.onNodesChange);
  const onEdgesChange = useStore((state) => state.onEdgesChange);
  const addNode = useStore((state) => state.addNode);
  const setEdges = useStore((state) => state.setEdges);

  const onConnect = useCallback(
    (params) => setEdges(addEdge({ ...params, type: 'smoothstep', animated: true }, edges)), // Example: Add edge type
    [edges, setEdges],
  );

  const onDragOver = useCallback((event) => {
    event.preventDefault();
    event.dataTransfer.dropEffect = 'move';
  }, []);

  const onDrop = useCallback(
    (event) => {
      event.preventDefault();

      const currentRef = reactFlowWrapper.current;
       if (!currentRef) {
          console.error("React Flow wrapper ref not available");
          return;
      }

      const reactFlowBounds = currentRef.getBoundingClientRect();
      const nodeInfoString = event.dataTransfer.getData('application/reactflow');

       if (!nodeInfoString) {
           console.warn("No node info found in dataTransfer");
           return;
       }

      try {
          // type is the agent_id from the backend, label is the name
          const {type, label} = JSON.parse(nodeInfoString);

          if (typeof type === 'undefined' || !type) {
            return;
          }

          // Correctly project screen coordinates to flow coordinates
          const position = project({
             x: event.clientX - reactFlowBounds.left,
             y: event.clientY - reactFlowBounds.top,
          });

          // Create initial data. Specific nodes might initialize more fields
          // based on their internal logic or props. The backend definition
          // dictates the *required* inputs for execution via edges.
          const initialNodeData = {
            label: label || `${type} Node`,
            // Add default values for inputs shown in the node UI if applicable
            // e.g., for basicAgent:
            prompt: "",
            max_tokens: null,
            // e.g., for contextAgent:
            context: "",
            // e.g., for chainOfAgents:
            initialPrompt: "",
            longText: "",
            chunkSize: 2000, // Default config shown in UI
            chunkOverlap: 200, // Default config shown in UI
            // e.g., for multiCallSystem:
            numCalls: 3,
            baseFilename: "output",
            // ... other defaults based on what your node component expects ...
          };

          const newNode = {
            id: `${type}-${nanoid(6)}`,
            type, // This MUST match the ID from agent_definitions.py
            position,
            data: initialNodeData,
          };

          console.log("Adding node:", newNode);
          addNode(newNode);
      } catch (e) {
          console.error("Failed to parse dropped node data:", e);
      }

    },
    [addNode, project], // Add project dependency
  );

  return (
    <div className="flex h-screen w-screen bg-gray-900" ref={reactFlowWrapper}>
      <Sidebar />
      <div className="flex-grow flex flex-col h-full">
        <TopBar />
        <div className="flex-grow relative">
          <ReactFlow
            nodes={nodes}
            edges={edges}
            onNodesChange={onNodesChange}
            onEdgesChange={onEdgesChange}
            onConnect={onConnect}
            onDrop={onDrop}
            onDragOver={onDragOver}
            nodeTypes={nodeTypes} // Use the constant defined outside
            fitView
            className="bg-gray-900"
            // Default edge options (example)
            defaultEdgeOptions={{ type: 'smoothstep', animated: true, style: { strokeWidth: 2 } }}
            connectionLineStyle={{ stroke: '#4f46e5', strokeWidth: 2 }} // Indigo color
            connectionLineType="smoothstep"
          >
            <Controls className="react-flow__controls" />
            <Background variant="dots" gap={16} size={1} color="#4A5568" />
             <MiniMap nodeStrokeWidth={3} nodeColor={(n) => {
                 switch (n.type) { // n.type now matches agent_id
                     case 'basicAgent': return '#3b82f6';
                     case 'contextAgent': return '#10b981';
                     case 'chainOfAgents': return '#f97316';
                     case 'multiCallSystem': return '#a855f7';
                    // Add other types if needed
                     default: return '#6b7280';
                 }
             }} />
          </ReactFlow>
        </div>
      </div>
      <OutputPanel />
    </div>
  );
};

// Wrap with ReactFlowProvider
function App() {
  return (
    <ReactFlowProvider>
      <FlowEditor />
    </ReactFlowProvider>
  );
}

export default App;
</file>

<file path="builder/frontend/src/components/TopBar.jsx">
// src/components/TopBar.jsx
import React, { useState, useCallback } from 'react';
import { useStore } from '../store';
import { Button } from '@/components/ui/button'; // Use shadcn Button
import { Input } from '@/components/ui/input'; // Use shadcn Input
import {
    Select,
    SelectContent,
    SelectItem,
    SelectTrigger,
    SelectValue,
} from "@/components/ui/select"; // Use shadcn Select
import { Save, Play, Trash2, PlusCircle } from 'lucide-react'; // Icons

const TopBar = () => {
  const projects = useStore((state) => state.projects);
  const currentProjectId = useStore((state) => state.currentProjectId);
  const loadProject = useStore((state) => state.loadProject);
  const createProject = useStore((state) => state.createProject);
  const deleteProject = useStore((state) => state.deleteProject);
  const saveCurrentProject = useStore((state) => state.saveCurrentProject);
  const runFlow = useStore((state) => state.runFlow);
  const isRunning = useStore((state) => state.isRunning);

  const [newProjectName, setNewProjectName] = useState('');

  const handleCreateProject = useCallback(() => {
    createProject(newProjectName.trim() || undefined); // Pass undefined for default name
    setNewProjectName('');
  }, [createProject, newProjectName]);

  const handleProjectChange = (value) => {
    // Shadcn Select's onValueChange provides the value directly
    if (value) {
      loadProject(value);
    }
  };

   const handleDeleteClick = () => {
        if (currentProjectId) {
            // Optional: Add a confirmation dialog here
            deleteProject(currentProjectId);
        }
    };

  return (
    <div className="h-16 bg-card border-b border-border flex items-center justify-between px-4 shadow-sm flex-shrink-0">
      {/* Left Side: Logo & Project Controls */}
      <div className="flex items-center space-x-4">
         {/* Logo and Title Group */}
         <div className="flex items-center flex-shrink-0"> {/* Grouping element */}
             <img
                src="/Tesslate.svg" // Path relative to public folder
                alt="Tesslate Logo"
                className="h-6 w-auto mr-2" // Adjust height as needed, add margin between logo and text
             />
             <span className="text-lg font-semibold text-foreground whitespace-nowrap">
                Tesslate Studio
             </span>
         </div>

        {/* Project Selector */}
        <Select
            value={currentProjectId || ''}
            onValueChange={handleProjectChange}
            disabled={isRunning}
        >
            <SelectTrigger className="w-[180px] text-sm">
                <SelectValue placeholder="Select Project" />
            </SelectTrigger>
            <SelectContent>
                {Object.entries(projects).map(([id, project]) => (
                    <SelectItem key={id} value={id}>
                        {project.name}
                    </SelectItem>
                ))}
            </SelectContent>
        </Select>

         {/* Create New Project */}
        <div className="flex items-center space-x-2">
            <Input
                type="text"
                value={newProjectName}
                onChange={(e) => setNewProjectName(e.target.value)}
                placeholder="New Project Name..."
                className="w-40 h-9 text-sm" // Adjusted height and width
                disabled={isRunning}
            />
            <Button
                onClick={handleCreateProject}
                variant="secondary"
                size="sm" // Smaller button
                disabled={isRunning}
                title="Create New Project"
            >
                <PlusCircle className="h-4 w-4 mr-1" /> Create
            </Button>
            <Button
                onClick={handleDeleteClick}
                variant="destructive"
                size="icon" // Icon button
                title="Delete Current Project"
                disabled={isRunning || !currentProjectId || Object.keys(projects).length <= 1}
            >
               <Trash2 className="h-4 w-4" />
               <span className="sr-only">Delete Project</span> {/* Keep for accessibility */}
            </Button>
        </div>
      </div>

      {/* Right Side: Action Buttons */}
      <div className="flex items-center space-x-3">
         <Button
            onClick={saveCurrentProject}
            variant="outline"
            size="sm"
            disabled={isRunning}
        >
            <Save className="h-4 w-4 mr-2" /> Save Project
        </Button>
        <Button
          onClick={runFlow}
          size="sm"
          disabled={isRunning}
          className={`font-semibold transition-colors duration-150 ease-in-out ${
            isRunning ? 'bg-muted text-muted-foreground cursor-not-allowed' : 'bg-primary text-primary-foreground hover:bg-primary/90'
          }`}
        >
          {isRunning ? (
             <>
                <span className="animate-spin rounded-full h-4 w-4 border-b-2 border-current mr-2"></span>
                Running...
             </>
          ) : (
             <>
                <Play className="h-4 w-4 mr-2 fill-current" /> Run Flow
             </>
          )}
        </Button>
      </div>
    </div>
  );
};

export default TopBar;
</file>

<file path="README.md">
# TFrameX Agents Framework

[![Python Version](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT) <!-- Adjust license if needed -->

## Overview

This project provides a flexible Python framework for interacting with VLLM (or other OpenAI-compatible) Large Language Model (LLM) endpoints. It features:

*   **Modular Design:** Separates concerns into Models, Agents (Primitives), and Systems within a standard Python package structure.
*   **Simplified Imports:** Core classes are easily accessible directly from their respective modules (e.g., `from tframex.model import VLLMModel`).
*   **Streaming Support:** Handles LLM responses as streams to avoid timeouts with long generations and provide real-time output.
*   **Concurrency:** Supports making multiple simultaneous API calls efficiently using `asyncio`.
*   **Extensibility:** Designed to be easily extended with new models, agents, or complex interaction systems.
*   **Chat & Completions:** Supports the OpenAI Chat Completions API format (`/v1/chat/completions`), ensuring compatibility with features like system prompts and reasoning tags (e.g., `<think>`).
*   **Error Handling & Retries:** Includes basic retry mechanisms for common transient network errors.

## Features

*   Define specific LLM endpoint configurations (e.g., `VLLMModel`).
*   Create primitive agents with specific tasks (e.g., `BasicAgent`, `ContextAgent`).
*   Build complex systems orchestrating multiple agents or calls (e.g., `ChainOfAgents` for summarization, `MultiCallSystem` for parallel generation).
*   Stream responses directly to files or aggregate them.
*   Handle API errors and network issues gracefully.
*   Configure model parameters (temperature, max tokens) per call.

## Project Structure

```
.
 tframex/                  # Core library package
    __init__.py           # Makes 'tframex' importable
    agents/
       __init__.py       # Exposes agent classes (e.g., BasicAgent)
       agent_logic.py    # BaseAgent and shared logic
       agents.py         # Concrete agent implementations
    model/
       __init__.py       # Exposes model classes (e.g., VLLMModel)
       model_logic.py    # BaseModel, VLLMModel implementation
    systems/
        __init__.py       # Exposes system classes (e.g., ChainOfAgents)
        systems.py        # Concrete system implementations

 examples/                 # Example usage scripts (separate from the library)
    website_builder/
       html.py
    context.txt           # Sample input file
    example.py            # Main example script
    longtext.txt          # Sample input file

 .env copy                 # Example environment file template
 .gitignore
 README.md                 # This file
 requirements.txt          # Core library dependencies
 pyproject.toml            # Build system and package configuration
```

*   **`tframex/`**: The main directory containing the library source code.
*   **`tframex/model/`**: Contains `BaseModel` and implementations like `VLLMModel`. Handles API communication, streaming, and response parsing (configured for `/v1/chat/completions`).
*   **`tframex/agents/`**: Contains `BaseAgent` and implementations like `BasicAgent`, `ContextAgent`. Represents individual processing units.
*   **`tframex/systems/`**: Contains orchestrators like `ChainOfAgents` and `MultiCallSystem` for complex workflows.
*   **`__init__.py` files**: These files make the directories Python packages and are used to expose the main classes for easier imports (e.g., allowing `from tframex.model import VLLMModel` instead of `from tframex.model.model_logic import VLLMModel`).
*   **`examples/`**: Contains scripts demonstrating library usage. These scripts import the `tframex` package.
*   **`pyproject.toml`**: Defines how to build and install the `tframex` package using standard Python tools like `pip`.
*   **`requirements.txt`**: Lists core dependencies needed by the `tframex` library itself. Dependencies needed *only* for examples (like `python-dotenv`) should ideally be installed separately by the user running the examples.

## Setup & Installation

### 1. Prerequisites

*   Python 3.8 or higher.
*   Access to a VLLM or other OpenAI-compatible LLM endpoint URL and API key.
*   `pip` and `setuptools` (usually included with Python).

### 2. Clone or Download

Get the project files onto your local machine.

```bash
git clone <your-repository-url> # Or download and extract the ZIP
cd TFrameX # Navigate into the project root directory (containing pyproject.toml)
```

### 3. Install Dependencies & Library

Create a virtual environment (recommended) and install the library. Installing in editable mode (`-e`) is useful during development, as changes to the library code are immediately reflected without reinstalling.

```bash
# Create and activate a virtual environment (optional but recommended)
python -m venv venv
source venv/bin/activate  # On Windows use `venv\Scripts\activate`

# Install the tframex library and its core dependencies
pip install -e .  # Installs in editable mode from the current directory

# OR, for a standard installation:
# pip install .

# If running examples that use .env files, you might also need:
# pip install python-dotenv
```

### 4. Configuration (for Examples)

The example scripts (`examples/example.py`, `examples/website_builder/html.py`) typically read configuration from environment variables (using `python-dotenv` and a `.env` file) or have placeholders.

**Crucial:** To run the examples, ensure your API credentials and endpoint details are accessible. You can:
    *   Create a `.env` file in the `TAF/` root directory by copying `.env copy` and filling in your details.
    *   Set environment variables directly in your shell.
    *   Modify the example scripts to hardcode values (not recommended for API keys).

Example `.env` file content:
```dotenv
API_URL="https://your-vllm-or-openai-compatible-url/v1"
API_KEY="your_actual_api_key"
MODEL_NAME="Qwen/Qwen3-30B-A3B-FP8" # Or your target model
MAX_TOKENS=32000
TEMPERATURE=0.7
```

**Security Warning:** **DO NOT** commit your actual API keys to version control. Use environment variables or secure configuration management for production or shared environments.

### 5. Create Input Files (Optional)

The `examples/example.py` script uses `examples/context.txt` and `examples/longtext.txt`. If they don't exist, basic placeholders might be used or created by the script. You can create your own with relevant content:

*   **`examples/context.txt`**: Text for the `ContextAgent`.
*   **`examples/longtext.txt`**: Longer text for the `ChainOfAgents`.

## Core Concepts

*   **Models (`tframex.model`)**: Represent the connection to an LLM API endpoint. Handle request formatting, API calls, streaming, and response parsing (using Chat Completions format). Access like: `from tframex.model import VLLMModel`.
*   **Agents (`tframex.agents`)**: Represent individual actors using a Model. Inherit from `BaseAgent`. Access like: `from tframex.agents import BasicAgent`.
*   **Systems (`tframex.systems`)**: Higher-level orchestrators managing complex workflows, potentially using multiple Agents or direct Model calls. Access like: `from tframex.systems import ChainOfAgents`.
*   **Streaming**: LLM responses are processed chunk-by-chunk via `AsyncGenerator`s to handle long responses and provide real-time data.
*   **Concurrency (`asyncio`)**: Used for efficient handling of multiple network operations (like in `MultiCallSystem`).
*   **Chat Completions API (`/v1/chat/completions`)**: The framework standardizes on this input/output format for compatibility with modern LLMs and features like reasoning tags.

## Usage

1.  Ensure you have completed the **Setup & Installation** steps, including installing the library (`pip install -e .` or `pip install .`) and configuring API access for the examples.
2.  Navigate to the project root directory (`TAF/`) in your terminal (and activate your virtual environment).
3.  Run an example script:

    ```bash
    # Run the main example suite
    python examples/example.py

    # Run the website builder example
    python examples/website_builder/html.py
    ```

4.  **Output:**
    *   Status messages and response previews will appear in the console.
    *   Detailed outputs are typically saved to files (e.g., within `examples/example_outputs/` or `examples/website_builder/generated_website/`, depending on the script).

## Code Documentation (Key Components & Imports)

*   **`tframex.model.VLLMModel(model_name, api_url, api_key, ...)`**
    *   Initializes connection to a VLLM/OpenAI-compatible chat endpoint.
    *   `call_stream(messages: List[Dict[str, str]], **kwargs) -> AsyncGenerator[str, None]`: Makes the streaming API call, yields content chunks.
    *   `close_client()`: Closes the HTTP client.
*   **`tframex.agents.BaseAgent(agent_id, model)`**
    *   Abstract base class for all agents.
    *   Provides `_stream_and_aggregate` helper.
*   **`tframex.agents.BasicAgent(agent_id, model)`**
    *   `run(prompt: str, **kwargs) -> str`: Simple agent; takes prompt, returns aggregated model response.
*   **`tframex.agents.ContextAgent(agent_id, model, context)`**
    *   `run(prompt: str, **kwargs) -> str`: Prepends context to the prompt before calling the model.
*   **`tframex.systems.ChainOfAgents(system_id, model, ...)`**
    *   `run(initial_prompt: str, long_text: str, **kwargs) -> str`: Processes long text via chunking and sequential summarization using an internal agent.
*   **`tframex.systems.MultiCallSystem(system_id, model)`**
    *   `run(prompt: str, num_calls: int, ..., **kwargs) -> Dict[str, str]`: Makes multiple concurrent model calls with the same prompt, saving results.

**Example Import Style:**

```python
from tframex.model import VLLMModel
from tframex.agents import BasicAgent, ContextAgent
from tframex.systems import ChainOfAgents, MultiCallSystem

# Initialize the model
model = VLLMModel(model_name="...", api_url="...", api_key="...")

# Initialize agents/systems
agent = BasicAgent(agent_id="my_agent", model=model)
# ... rest of your code
```

## Troubleshooting

*   **`httpx.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read)` or similar Timeouts:**
    *   **Cause:** Often proxy timeouts (e.g., Cloudflare Tunnel ~100s), VLLM server overload/timeouts.
    *   **VLLM Logs:** Check VLLM logs for errors (OOM), high load, `Waiting` requests, `Aborted request`.
    *   **Solution 1: Reduce Concurrency:** Lower `num_calls` in `MultiCallSystem` examples.
    *   **Solution 2: Check Proxy/Tunnel:** Verify/increase timeouts if possible.
    *   **Solution 3: Optimize/Scale VLLM:** Ensure adequate server resources.
*   **Missing `<think>` Tags or Initial Content:**
    *   **Cause:** Using wrong API endpoint format or incorrect response parsing. The library uses `/v1/chat/completions` and parses `delta.content`.
    *   **Solution:** Ensure your model endpoint is compatible and outputs reasoning tags within the standard chat completion structure.
*   **Repetitive Output:**
    *   **Cause:** LLM looping, often due to `max_tokens`, unstable decoding, or specific sampling parameters.
    *   **Solution:** Check `max_tokens`, adjust `temperature`/`repetition_penalty`, reduce concurrency/load.
*   **Configuration Errors (401 Unauthorized, 404 Not Found):**
    *   **Cause:** Incorrect `API_KEY`, `API_URL`, or `MODEL_NAME`.
    *   **Solution:** Double-check configuration values against your endpoint details.
*   **Import Errors (`ModuleNotFoundError: No module named 'tframex'`):**
    *   **Cause:** The library hasn't been installed in the current Python environment.
    *   **Solution:** Ensure you have run `pip install .` or `pip install -e .` in the project root directory (`TAF/`) within your activated virtual environment.
*   **Other `httpx` Errors (`ConnectError`, `ReadError`):**
    *   **Cause:** Network issues, server down.
    *   **Solution:** Basic retry logic helps. Check network, firewall, server status.

## Customization & Extension

*   **Add New Models:** Create a new class in `tframex/model/` inheriting from `BaseModel`, implement `call_stream`, and expose it in `tframex/model/__init__.py`.
*   **Add New Agents:** Create new classes in `tframex/agents/` inheriting from `BaseAgent` and expose them in `tframex/agents/__init__.py`.
*   **Add New Systems:** Create new classes in `tframex/systems/` and expose them in `tframex/systems/__init__.py`.
*   **Configuration Management:** Use a more robust system like environment variables or dedicated config files instead of hardcoding in examples.
*   **Input/Output:** Adapt agents/systems to interact with databases, web APIs, etc.
*   **Enhanced Retries:** Use libraries like `tenacity` for more sophisticated retry strategies.

## License

This project is licensed under the MIT License.
</file>

<file path="builder/backend/app.py">
# backend/app.py
import os
import asyncio
import json
import logging
from flask import Flask, request, jsonify, send_from_directory # Added send_from_directory
from flask_cors import CORS
from dotenv import load_dotenv

# (Keep existing imports: run_flow, get_model, strip_think_tags, get_definitions...)
from flow_executor import run_flow, get_model, strip_think_tags
from agent_definitions import get_definitions_for_frontend, get_definition

load_dotenv()
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("FlaskServer")

app = Flask(__name__)
CORS(app, resources={r"/api/*": {"origins": "http://localhost:5173"}})

# Ensure base output directory for runs exists
GENERATED_DIR = "generated"
# (Keep existing directory creation logic, add GENERATED_DIR)
required_dirs = [
    "example_outputs",
    os.path.join("example_outputs", "ex4_multi_call_outputs"),
    GENERATED_DIR # Add the new base directory
]
for dir_path in required_dirs:
    if not os.path.exists(dir_path):
        try:
            os.makedirs(dir_path, exist_ok=True)
            logger.info(f"Created directory: {dir_path}")
        except OSError as e:
            logger.error(f"Failed to create directory {dir_path}: {e}")

# --- API Endpoints ---

@app.route('/')
def index():
    return "Backend is running. Use API endpoints."

# (Keep existing /api/agents, /api/run, /api/chatbot endpoints...)
@app.route('/api/agents', methods=['GET'])
def list_agents():
    # Keep existing logic...
    logger.info("Request received on /api/agents")
    try:
        definitions = get_definitions_for_frontend()
        return jsonify(definitions)
    except Exception as e:
        logger.error(f"Error getting agent definitions: {e}", exc_info=True)
        return jsonify({"error": "Failed to load agent definitions"}), 500

@app.route('/api/run', methods=['POST'])
async def handle_run_flow():
    # Keep existing logic...
    logger.info("Received request on /api/run")
    if not request.is_json:
        logger.warning("Request is not JSON")
        return jsonify({"error": "Request must be JSON"}), 400
    data = request.get_json()
    nodes = data.get('nodes')
    edges = data.get('edges')
    if nodes is None or edges is None:
        logger.warning("Missing 'nodes' or 'edges' in request data")
        return jsonify({"error": "Missing 'nodes' or 'edges' in request data"}), 400

    logger.info(f"Received {len(nodes)} nodes and {len(edges)} edges for dynamic execution.")
    try:
        result_log = await run_flow(nodes, edges)
        logger.info("Dynamic flow execution completed.")
        return jsonify({"output": result_log})
    except RuntimeError as e:
         logger.error(f"Runtime error during flow execution: {e}", exc_info=True)
         return jsonify({"error": f"Runtime Error: {e}"}), 500
    except ImportError as e:
        logger.error(f"Import error, check agent definitions/imports: {e}", exc_info=True)
        return jsonify({"error": f"Import Error: {e}. Check backend agent setup."}), 500
    except Exception as e:
        logger.error(f"An unexpected error occurred during flow execution: {e}", exc_info=True)
        return jsonify({"error": f"An unexpected error occurred: {e}"}), 500


@app.route('/api/chatbot', methods=['POST'])
async def handle_chatbot():
    # Keep existing logic...
    logger.info("Received request on /api/chatbot")
    if not request.is_json: return jsonify({"error": "Request must be JSON"}), 400
    data = request.get_json()
    user_message = data.get('message')
    current_nodes = data.get('nodes', [])
    current_edges = data.get('edges', [])
    available_defs_frontend = data.get('definitions', [])
    if not user_message: return jsonify({"error": "Missing 'message'"}), 400

    try:
        available_nodes_str = "\n".join([f"- ID: {d['id']}, Name: {d['name']}, Type: {d['type']}" for d in available_defs_frontend])
        current_flow_str = json.dumps({"nodes": current_nodes, "edges": current_edges}, indent=2)
        agent_id = "flowBuilderAgent"
        definition = get_definition(agent_id)
        if not definition or not definition.get('constructor') or not definition.get('execute_function'):
            logger.error(f"Flow Builder Agent ('{agent_id}') not defined correctly.")
            return jsonify({"reply": "Error: Chatbot agent is not configured.", "flow": None}), 500
        model = await get_model()
        agent_instance = definition['constructor'](model)
        input_payload = {
            "user_message": user_message,
            "available_nodes_context": available_nodes_str,
            "current_flow_context": current_flow_str
        }
        agent_result = await definition['execute_function'](agent_instance, input_payload)
        raw_output = agent_result.get('raw_llm_output', '')
        logger.debug(f"Raw Chatbot LLM Output:\n{raw_output}")
        json_string = strip_think_tags(raw_output)
        parsed_flow = None
        reply_message = "Sorry, I couldn't process your request properly."
        if json_string:
            try:
                parsed_data = json.loads(json_string)
                if isinstance(parsed_data, dict) and \
                   'nodes' in parsed_data and isinstance(parsed_data.get('nodes'), list) and \
                   'edges' in parsed_data and isinstance(parsed_data.get('edges'), list):
                    parsed_flow = parsed_data
                    reply_message = "Okay, I've updated the flow."
                    logger.info(f"Chatbot successfully generated a valid flow update.")
                else:
                    reply_message = "Sorry, the response wasn't in the expected flow format."
                    logger.warning(f"Chatbot output failed validation. Parsed type: {type(parsed_data)}")
            except json.JSONDecodeError as json_err:
                reply_message = "Sorry, my internal response was not valid JSON."
                logger.error(f"Chatbot output failed JSON parsing: {json_err}. String: {json_string}")
            except Exception as parse_err:
                 reply_message = f"Sorry, an error occurred processing the response: {parse_err}"
                 logger.error(f"Chatbot unexpected parsing error: {parse_err}", exc_info=True)
        else:
             reply_message = "Sorry, I received an empty response. Please try again."
             logger.warning("Chatbot returned empty string after stripping tags.")

        return jsonify({"reply": reply_message, "flow": parsed_flow})
    except RuntimeError as e:
         logger.error(f"Runtime error during chatbot request: {e}", exc_info=True)
         return jsonify({"reply": f"Server Runtime Error: {e}", "flow": None}), 500
    except Exception as e:
        logger.error(f"Unexpected error during chatbot request: {e}", exc_info=True)
        return jsonify({"reply": f"Unexpected server error: {e}", "flow": None}), 500


# --- NEW: Preview Route ---
@app.route('/api/preview/<run_id>/<path:filepath>')
def serve_generated_file(run_id, filepath):
    """Serves files from a specific run's generated folder."""
    logger.info(f"Request received for preview: run_id={run_id}, filepath={filepath}")
    # Security: Ensure run_id and filepath are safe path components
    # Basic check: ensure no '..' traversal
    if '..' in run_id or '..' in filepath:
        logger.warning(f"Potential path traversal detected in preview request: {run_id}/{filepath}")
        return "Invalid path", 404

    directory = os.path.abspath(os.path.join(GENERATED_DIR, run_id))

    # Security: Double-check the final directory path is within GENERATED_DIR
    if not directory.startswith(os.path.abspath(GENERATED_DIR)):
         logger.error(f"Attempt to access directory outside generated folder: {directory}")
         return "Access denied", 403

    # Check if the directory for the run_id exists
    if not os.path.isdir(directory):
        logger.error(f"Preview directory not found: {directory}")
        return "Run ID not found", 404

    logger.debug(f"Attempting to send file: {filepath} from directory: {directory}")
    try:
        return send_from_directory(directory, filepath)
    except FileNotFoundError:
        logger.error(f"File not found in preview request: {directory}/{filepath}")
        return "File not found", 404
    except Exception as e:
         logger.error(f"Error serving file {directory}/{filepath}: {e}", exc_info=True)
         return "Error serving file", 500
# --- END NEW ---


# --- Application Entry Point (Keep existing) ---
if __name__ == '__main__':
    # (Keep checks for agent definitions)
    try:
        defs = get_definitions_for_frontend()
        if not defs: logger.warning("Agent definitions list for frontend is empty.")
        else: logger.info(f"Loaded {len(defs)} agent/system definitions for frontend.")
        # Add check for the new agents if desired
        req_agents = {'plannerAgent', 'distributorAgent', 'fileGeneratorAgent'}
        loaded_agents = {d['id'] for d in defs}
        if not req_agents.issubset(loaded_agents):
             logger.warning(f"Missing one or more Software Builder agents: {req_agents - loaded_agents}")
    except Exception as e:
         logger.error(f"CRITICAL: Failed to load agent definitions: {e}", exc_info=True)

    host = os.getenv('FLASK_RUN_HOST', '127.0.0.1')
    port = int(os.getenv('FLASK_RUN_PORT', 5001))
    debug = os.getenv('FLASK_ENV') == 'development'

    logger.info(f"Starting Flask server on {host}:{port} (Debug: {debug})")
    app.run(host=host, port=port, debug=debug)
</file>

<file path="builder/frontend/src/components/OutputPanel.jsx">
// src/components/OutputPanel.jsx
import React from 'react';
import { useStore } from '../store';
import { Button } from '@/components/ui/button';
import { Card, CardContent, CardHeader, CardTitle } from "@/components/ui/card";
import { ScrollArea } from '@/components/ui/scroll-area';
import { Trash2, ExternalLink } from 'lucide-react'; // Import ExternalLink

const OutputPanel = () => {
  const output = useStore((state) => state.output);
  const clearOutput = useStore((state) => state.clearOutput);
  const isRunning = useStore((state) => state.isRunning);

  const hasContent = output && output !== "Output will appear here..." && output.trim() !== "";

  // --- NEW: Detect Preview Link ---
  let previewLink = null;
  let cleanedOutput = output; // Output without the preview marker line
  if (hasContent) {
      const linkMarker = "PREVIEW_LINK::";
      const linkIndex = output.indexOf(linkMarker);
      if (linkIndex !== -1) {
          const linkLine = output.substring(linkIndex + linkMarker.length);
          // Extract the link (assuming it's the rest of the line)
          const linkMatch = linkLine.match(/(\/api\/preview\/.*)/);
          if (linkMatch && linkMatch[1]) {
               const relativePreviewLink = linkMatch[1].trim();
               previewLink = `http://localhost:5001${relativePreviewLink}`; // Prepend the base URL
               // Remove the marker line and potentially the user-friendly message line below it from the displayed output
               const lines = output.split('\n');
               cleanedOutput = lines.filter(line => !line.startsWith(linkMarker) && !line.includes("(Link to preview generated content:")).join('\n');
          } else {
              // If marker exists but link extraction fails, keep original output
               cleanedOutput = output;
          }
      } else {
          cleanedOutput = output;
      }
  }
  // --- END NEW ---

  return (
    <Card className="w-[450px] flex flex-col rounded-none border-l border-t-0 border-b-0 border-r-0 border-border h-full shadow-none">
       <CardHeader className="flex flex-row justify-between items-center p-3 border-b border-border flex-shrink-0 h-16">
            <CardTitle className="text-lg font-semibold">Output</CardTitle>
            {/* --- NEW: Add Preview Button --- */}
            {previewLink && (
                 <Button
                    variant="secondary" // Or another variant
                    size="sm"
                    onClick={() => window.open(previewLink, '_blank')}
                    title="Open Preview in New Tab"
                  >
                    <ExternalLink className="h-4 w-4 mr-1" /> Preview
                 </Button>
            )}
            {/* --- END NEW --- */}
            <Button
                variant="outline"
                size="sm"
                onClick={clearOutput}
                disabled={isRunning || !hasContent}
                className="ml-auto" // Push clear button to the right if preview exists
            >
                <Trash2 className="h-4 w-4 mr-1" /> Clear
            </Button>
       </CardHeader>

      <CardContent className="flex-grow p-0 overflow-hidden">
        <ScrollArea className="h-full w-full">
            <pre className="text-sm text-muted-foreground whitespace-pre-wrap break-words font-mono p-4">
                 {/* Display cleanedOutput */}
                {hasContent ? cleanedOutput : <span className="text-muted-foreground/70 italic">Output will appear here...</span>}
            </pre>
        </ScrollArea>
      </CardContent>
    </Card>
  );
};

export default OutputPanel;
</file>

<file path="builder/frontend/src/components/Sidebar.jsx">
// src/components/Sidebar.jsx
import React, { useState, useEffect } from 'react';
import NodesPanel from './NodesPanel';
import ChatbotPanel from './ChatbotPanel';
import { useStore } from '../store';
import { Tabs, TabsContent, TabsList, TabsTrigger } from "@/components/ui/tabs"; // Use shadcn Tabs

const Sidebar = () => {
  // Shadcn Tabs manages its own active state via the defaultValue/value prop
  const agentDefinitions = useStore((state) => state.agentDefinitions);
  const fetchAgentDefinitions = useStore((state) => state.fetchAgentDefinitions);
  const isLoading = useStore((state) => state.isDefinitionLoading);
  const error = useStore((state) => state.definitionError);

  useEffect(() => {
      if (agentDefinitions.length === 0 && !isLoading && !error) {
          fetchAgentDefinitions();
      }
  }, [fetchAgentDefinitions, agentDefinitions.length, isLoading, error]);

  return (
    // Use background defined by shadcn's card or background
    <aside className="w-72 flex flex-col bg-card border-r border-border h-full">
      <Tabs defaultValue="nodes" className="flex flex-col flex-grow h-full">
        {/* Tabs Header */}
        <TabsList className="grid w-full grid-cols-2 rounded-none border-b border-border">
          <TabsTrigger value="nodes" className="rounded-none data-[state=active]:border-b-2 data-[state=active]:border-primary data-[state=active]:shadow-none">
             Nodes
          </TabsTrigger>
          <TabsTrigger value="chatbot" className="rounded-none data-[state=active]:border-b-2 data-[state=active]:border-primary data-[state=active]:shadow-none">
             Chatbot
          </TabsTrigger>
        </TabsList>

        {/* Content Area */}
        <TabsContent value="nodes" className="flex-grow overflow-hidden mt-0 data-[state=inactive]:hidden">
            {/* NodesPanel content needs padding */}
            <div className="h-full overflow-y-auto p-3">
                <NodesPanel agentDefs={agentDefinitions} isLoading={isLoading} error={error} />
            </div>
        </TabsContent>
        <TabsContent value="chatbot" className="flex-grow overflow-hidden mt-0 data-[state=inactive]:hidden">
           {/* ChatbotPanel already has internal padding */}
           <ChatbotPanel />
        </TabsContent>
      </Tabs>
    </aside>
  );
};

export default Sidebar;
</file>

<file path="builder/backend/flow_executor.py">
# builder/backend/flow_executor.py
import asyncio
import os
import logging
import time
import re # Import re for stripping tags
from typing import List, Dict, Any, Set, Optional, Tuple
from collections import deque, defaultdict
from dotenv import load_dotenv

# Import agent definitions and model setup
from agent_definitions import get_definition # Import only the getter
from tframex.model import VLLMModel

load_dotenv()

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("FlowExecutor")

# --- Model Setup (Keep existing get_model logic) ---
API_URL = os.getenv("API_URL")
API_KEY = os.getenv("API_KEY")
MODEL_NAME = os.getenv("MODEL_NAME")
DEFAULT_MAX_TOKENS = int(os.getenv("MAX_TOKENS", 32000))
DEFAULT_TEMPERATURE = float(os.getenv("TEMPERATURE", 0.7))

vllm_model_instance: Optional[VLLMModel] = None
model_lock = asyncio.Lock()

async def get_model() -> VLLMModel:
    # Keep existing get_model logic...
    global vllm_model_instance
    async with model_lock:
        if vllm_model_instance is None:
            logger.info("Initializing VLLM Model instance...")
            try:
                # --- Add helper method if not using BasicAgent ---
                async def _stream_and_aggregate_helper(model, messages, **kwargs):
                    full_response = ""
                    async for chunk in model.call_stream(messages, **kwargs):
                        full_response += chunk
                    return full_response
                # Add it to the class prototype dynamically if needed
                # This is a bit hacky, better if BaseAgent provided it or if VLLMModel had it
                if not hasattr(VLLMModel, 'call_stream_and_aggregate'):
                     VLLMModel.call_stream_and_aggregate = _stream_and_aggregate_helper
                # --- End helper method addition ---

                vllm_model_instance = VLLMModel(
                    model_name=MODEL_NAME,
                    api_url=API_URL,
                    api_key=API_KEY,
                    default_max_tokens=DEFAULT_MAX_TOKENS,
                    default_temperature=DEFAULT_TEMPERATURE
                )
                logger.info("VLLM Model instance created.")
            except Exception as e:
                logger.error(f"Fatal Error: Failed to initialize VLLM Model: {e}", exc_info=True)
                raise RuntimeError(f"Could not initialize VLLM Model: {e}")
        return vllm_model_instance
# --- End Model Setup ---


# --- Helper Functions (Keep existing strip_think_tags, find_logical_name_for_handle) ---
def strip_think_tags(text: str) -> str:
     # Keep existing logic...
    if not isinstance(text, str):
        return text
    think_end_tag = "</think>"
    tag_pos = text.find(think_end_tag)
    if tag_pos != -1:
        logger.debug("Found </think> tag, stripping preceding content.")
        content_after = text[tag_pos + len(think_end_tag):].strip()
        return content_after
    else:
        # logger.debug("No </think> tag found, using full response.") # Reduce noise
        return text.strip()

def find_logical_name_for_handle(definition: Dict, handle_id: str, io_type: str) -> Optional[str]:
     # Keep existing logic...
    if not definition or io_type not in definition:
        return None
    io_map = definition[io_type]
    for logical_name, details in io_map.items():
        if isinstance(details, dict) and details.get('handle_id') == handle_id:
            return logical_name
    if handle_id in io_map:
        # logger.debug(f"Falling back to using handle_id '{handle_id}' as logical name for {io_type}.")
        return handle_id
    return None
# --- End Helper Functions ---


# --- Dynamic Flow Execution Logic ---
async def run_flow(nodes: List[Dict[str, Any]], edges: List[Dict[str, Any]]) -> str:
    """
    Executes the flow topologically based on nodes and edges.
    Passes data between nodes according to edge connections and handle mapping.
    Strips <think> tags from string outputs before logging or passing downstream.
    Injects run_id for specific agents.
    Appends preview link to the final log if generated.
    """
    # --- NEW: Generate Run ID ---
    run_id = f"run_{int(time.time())}_{os.urandom(4).hex()}"
    logger.info(f"--- Starting Dynamic Flow Execution ({run_id}) ---")
    output_log = [f"--- Flow Execution Start ({run_id}) ---"]
    start_time = time.time()
    final_preview_link = None # Store the link if generated

    # (Keep graph building and dependency logic...)
    if not nodes:
        output_log.append("No nodes found in the flow.")
        output_log.append(f"--- Flow Execution End ({run_id}) ---")
        return "\n".join(output_log)

    adj: Dict[str, List[Tuple[str, str, str]]] = defaultdict(list)
    in_degree: Dict[str, int] = defaultdict(int)
    node_map: Dict[str, Dict] = {node['id']: node for node in nodes}
    all_node_ids: Set[str] = set(node_map.keys())
    connected_logical_inputs: Dict[str, Dict[str, bool]] = defaultdict(dict)

    for edge in edges:
        source_id = edge.get('source')
        target_id = edge.get('target')
        source_handle = edge.get('sourceHandle')
        target_handle = edge.get('targetHandle')
        if source_id in all_node_ids and target_id in all_node_ids and source_handle and target_handle:
            adj[source_id].append((target_id, source_handle, target_handle))
            in_degree[target_id] += 1
            target_definition = get_definition(node_map.get(target_id, {}).get('type'))
            logical_input_name = find_logical_name_for_handle(target_definition, target_handle, 'inputs')
            if logical_input_name:
                connected_logical_inputs[target_id][logical_input_name] = True
                # logger.debug(f"[{run_id}] Edge maps: {source_id}[{source_handle}] -> {target_id}[{target_handle}] (logical input: '{logical_input_name}')")
            else:
                logger.warning(f"[{run_id}] Edge target handle '{target_handle}' on node {target_id} does not map to a defined logical input.")
        else:
            logger.warning(f"[{run_id}] Skipping invalid edge: {edge}")


    # 2. Initialize Execution State (Keep existing)
    queue = deque([node_id for node_id in all_node_ids if in_degree[node_id] == 0])
    execution_results: Dict[str, Dict[str, Any]] = {} # node_id -> {logical_output_name: stripped_value}
    node_input_data: Dict[str, Dict[str, Any]] = defaultdict(dict)
    executed_nodes: Set[str] = set()
    agent_instances: Dict[str, Any] = {}

    # Get model instance once (Keep existing)
    try:
        model = await get_model()
    except RuntimeError as e:
        output_log.append(f"\n## FATAL ERROR: Could not initialize model: {e} ##")
        output_log.append(f"--- Flow Execution End ({run_id}) ---")
        return "\n".join(output_log)

    # Instantiate agents/systems (Keep existing)
    for node_id, node in node_map.items():
        node_type = node.get('type')
        definition = get_definition(node_type)
        if definition and definition.get('constructor'):
            try:
                agent_instances[node_id] = definition['constructor'](model)
                logger.info(f"[{run_id}] Instantiated agent/system for node {node_id} ({node_type})")
            except Exception as e:
                logger.error(f"[{run_id}] Failed to instantiate node {node_id} ({node_type}): {e}", exc_info=True)
                output_log.append(f"\n## ERROR: Failed to instantiate node {node_id} ({node_type}): {e} ##")


    # 3. Execute Nodes Topologically
    exec_count = 0
    while queue:
        # (Keep cycle detection...)
        exec_count += 1
        if exec_count > len(all_node_ids) * 2:
             logger.error(f"[{run_id}] Potential cycle detected or excessive execution. Stopping.")
             output_log.append("\n## ERROR: Potential cycle detected or flow stuck. Execution halted. ##")
             break

        node_id = queue.popleft()

        # (Keep checks for node existence, instantiation, definition...)
        if node_id not in node_map: continue
        if node_id not in agent_instances:
            node_type_for_log = node_map.get(node_id, {}).get('type', 'Unknown Type')
            logger.warning(f"[{run_id}] Node {node_id} ({node_type_for_log}) was not instantiated successfully. Skipping execution.")
            output_log.append(f"\n--- Node: {node_id} ({node_type_for_log}) ---")
            output_log.append(f"Status: SKIPPED (Instantiation Failed)")
            continue

        node = node_map[node_id]
        node_type = node.get('type')
        definition = get_definition(node_type)
        agent_instance = agent_instances[node_id]

        if not definition or not definition.get('execute_function'):
             logger.warning(f"[{run_id}] No definition or execute_function for node {node_id} ({node_type}). Skipping.")
             output_log.append(f"\n--- Node: {node_id} ({node_type}) ---")
             output_log.append(f"Status: SKIPPED (No execute function defined)")
             continue

        # Gather input data (Keep existing logic)
        current_node_inputs = node.get('data', {}).copy()
        defined_input_names = set(definition.get('inputs', {}).keys())
        # --- SPECIAL CASE: Ensure 'run_id' from definition is included even if not in node data ---
        if 'run_id' in defined_input_names:
             defined_input_names.add('run_id')
        # --- END SPECIAL CASE ---
        current_node_inputs = {k: v for k, v in current_node_inputs.items() if k in defined_input_names}
        current_node_inputs.update(node_input_data[node_id])

        # --- NEW: Inject run_id if the agent expects it ---
        if 'run_id' in definition.get('inputs', {}):
            current_node_inputs['run_id'] = run_id
            logger.debug(f"[{run_id}] Injected run_id into inputs for node {node_id}")
        # --- END NEW ---

        # Check for missing required inputs (Keep existing logic)
        missing_required = False
        for logical_name, details in definition.get('inputs', {}).items():
            # Skip run_id check here, as it's injected
            if logical_name == 'run_id': continue
            is_required = details.get('required', False)
            # Check if required and EITHER visually connected but data missing OR not connected at all
            is_connected = connected_logical_inputs.get(node_id, {}).get(logical_name, False)
            has_data = logical_name in current_node_inputs

            if is_required and ((is_connected and not has_data) or (not is_connected and not has_data)):
                 logger.error(f"[{run_id}] Node {node_id}: Required input '{logical_name}' is missing.")
                 output_log.append(f"\n--- Node: {node_id} ({node_type}) ---")
                 output_log.append(f"Status: FAILED")
                 output_log.append(f"Error: Missing required input data for '{logical_name}'.")
                 missing_required = True
                 break

        if missing_required:
             continue

        logger.info(f"[{run_id}] Executing Node: {node_id} ({node_type}) ...") # Simplified log
        output_log.append(f"\n--- Node: {node_id} ({node_type}) ---")

        try:
            # Execute the node's logic (Keep existing)
            raw_node_outputs = await definition['execute_function'](agent_instance, current_node_inputs)

            # (Keep output processing and validation...)
            if not isinstance(raw_node_outputs, dict):
                 logger.warning(f"[{run_id}] Node {node_id} execution function did not return a dictionary. Result: {raw_node_outputs}")
                 output_keys = list(definition.get('outputs', {}).keys())
                 if len(output_keys) == 1:
                      raw_node_outputs = {output_keys[0]: raw_node_outputs}
                 else:
                      raw_node_outputs = {"output": str(raw_node_outputs)}

            # Strip <think> tags from string outputs (Keep existing)
            processed_node_outputs = {}
            for logical_out_name, out_value in raw_node_outputs.items():
                if isinstance(out_value, str):
                    stripped_value = strip_think_tags(out_value)
                    # if stripped_value != out_value: logger.info(f"[{run_id}] Node {node_id}: Stripped <think> tags from output '{logical_out_name}'.") # Reduce noise
                    processed_node_outputs[logical_out_name] = stripped_value
                else:
                    processed_node_outputs[logical_out_name] = out_value

            execution_results[node_id] = processed_node_outputs
            executed_nodes.add(node_id)

            # Log success and *stripped* output (Keep existing)
            output_log.append(f"Status: Success")
            for logical_out_name, stripped_value in processed_node_outputs.items():
                 # --- NEW: Capture Preview Link ---
                 if node_type == 'fileGeneratorAgent' and logical_out_name == 'preview_link' and stripped_value:
                     final_preview_link = stripped_value
                     logger.info(f"[{run_id}] Captured preview link: {final_preview_link}")
                     output_log.append(f"Output '{logical_out_name}': Preview link generated (see end of log).") # Don't log the link itself here
                 elif logical_out_name == 'generation_summary': # Just log the summary directly
                     output_log.append(f"Output '{logical_out_name}':\n{str(stripped_value)}")
                 elif logical_out_name != 'preview_link': # Avoid logging the link value twice
                     # Truncate long outputs for readability in main log
                     log_value = str(stripped_value)
                     if len(log_value) > 500:
                         log_value = log_value[:500] + "... (truncated)"
                     output_log.append(f"Output '{logical_out_name}':\n{log_value}")

            logger.info(f"[{run_id}] Node {node_id} execution successful.")

            # Process downstream nodes (Keep existing logic for passing data and checking readiness)
            for target_id, source_handle, target_handle in adj[node_id]:
                 target_node = node_map.get(target_id)
                 target_definition = get_definition(target_node.get('type')) if target_node else None

                 if target_node and target_definition:
                    logical_output_name = find_logical_name_for_handle(definition, source_handle, 'outputs')
                    logical_input_name = find_logical_name_for_handle(target_definition, target_handle, 'inputs')

                    if logical_output_name and logical_input_name:
                        output_value_to_pass = processed_node_outputs.get(logical_output_name)
                        if output_value_to_pass is not None:
                            node_input_data[target_id][logical_input_name] = output_value_to_pass
                            # logger.debug(f"[{run_id}] Passing '{logical_output_name}' from {node_id} to '{logical_input_name}' of {target_id}")
                        else:
                             logger.warning(f"[{run_id}] Logical output '{logical_output_name}' not found in processed results of {node_id}. Cannot pass.")
                    else:
                        logger.warning(f"[{run_id}] Could not map handles for edge {node_id}[{source_handle}] -> {target_id}[{target_handle}].")

                    in_degree[target_id] -= 1
                    if in_degree[target_id] == 0:
                        # Keep readiness check logic...
                        is_ready = True
                        target_reqs = target_definition.get('inputs', {})
                        for req_logical_name, details in target_reqs.items():
                             if req_logical_name == 'run_id': continue # Skip run_id check
                             is_req = details.get('required', False)
                             was_connected = connected_logical_inputs.get(target_id, {}).get(req_logical_name, False)
                             has_data = req_logical_name in node_input_data[target_id]
                             if is_req and ((was_connected and not has_data) or (not was_connected and not has_data)):
                                  is_ready = False
                                  # logger.debug(f"[{run_id}] Node {target_id} still waiting for required input '{req_logical_name}'.")
                                  break
                        if is_ready and target_id not in executed_nodes and target_id not in queue:
                           logger.info(f"[{run_id}] Node {target_id} is ready. Adding to queue.")
                           queue.append(target_id)
                        # elif not is_ready and target_id not in executed_nodes:
                        #      logger.debug(f"[{run_id}] Node {target_id} has in_degree 0 but still waiting.")

        except Exception as e:
             # (Keep existing exception handling)
            logger.error(f"[{run_id}] Error during execution or downstream processing for node {node_id} ({node_type}): {e}", exc_info=True)
            output_log.append(f"Status: FAILED")
            output_log.append(f"Error:\n{e}")

    # 4. Compile Final Output Log
    output_log.append(f"\n--- Execution Summary ({run_id}) ---")
    # (Keep existing summary logic)
    executed_count = len(executed_nodes)
    total_nodes = len(all_node_ids)
    if executed_count == total_nodes:
         output_log.append("All nodes executed successfully.")
    else:
         output_log.append(f"Executed {executed_count}/{total_nodes} nodes.")
         failed_or_skipped = all_node_ids - executed_nodes
         if failed_or_skipped:
             output_log.append(f"Failed/Skipped/Waiting nodes: {', '.join(failed_or_skipped)}")

    end_time = time.time()
    output_log.append(f"Total execution time: {end_time - start_time:.2f} seconds")

    # --- NEW: Append Preview Link if Available ---
    if final_preview_link:
        output_log.append("\n--- Preview Link ---")
        # Use a clear marker for the frontend to detect
        output_log.append(f"PREVIEW_LINK::{final_preview_link}")
        # Also add a user-friendly message
        output_log.append(f"(Link to preview generated content: {final_preview_link} )")
    # --- END NEW ---

    output_log.append(f"\n--- Flow Execution End ({run_id}) ---")
    logger.info(f"--- Dynamic Flow Execution Finished ({run_id}) ---")

    return "\n".join(output_log)
</file>

</files>
